{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from MileStone_1_Training_Validation_data_Setup.load_data import load_data_from_ImageFolder\n",
    "\n",
    "SOURCE_DIR = \"../Dataset_Willys_2020/ORGINAL/\"\n",
    "dataset = load_data_from_ImageFolder(root=SOURCE_DIR)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "valid_procentage = 0.15\n",
    "test_procentage = 0.1\n",
    "batch_size = 400\n",
    "\n",
    "n_jobs = 32\n",
    "n_epochs =10\n",
    "\n",
    "\n",
    "num_train = len(dataset)\n",
    "val_indices = list(range(num_train))\n",
    "valid_split = int(valid_procentage*num_train)\n",
    "\n",
    "\n",
    "\n",
    "valid_idx = np.random.choice(val_indices, size=valid_split, replace=False)\n",
    "train_idx = list(set(val_indices)-set(valid_idx))\n",
    "\n",
    "test_split = int(test_procentage*len(train_idx))\n",
    "test_indices = list(range(len(train_idx)))\n",
    "test_idx = np.random.choice(test_indices, size=test_split,replace=False)\n",
    "\n",
    "train_idx = list(set(test_indices) - set(test_idx))\n",
    "\n",
    "print(f\"total number of images {num_train}, which {len(train_idx)} of them is used for training\")\n",
    "print(f\"And {len(valid_idx)} for validation and {len(test_idx)} for testing\")\n",
    "\n",
    "assert len(train_idx)+ len(valid_idx) +len(test_idx) == num_train "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total number of images 9730, which 7444 of them is used for training\n",
      "And 1459 for validation and 827 for testing\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size,\n",
    "                          num_workers=n_jobs, pin_memory=True, drop_last=True)\n",
    "\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "valid_loader = DataLoader(dataset, sampler=valid_sampler, batch_size=batch_size,\n",
    "                          num_workers=n_jobs, pin_memory=True, drop_last=True)\n",
    "\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "test_loader = DataLoader(dataset, sampler=test_sampler, batch_size=batch_size,\n",
    "                          num_workers=n_jobs, pin_memory=True, drop_last=True\n",
    "                          )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch.optim as optim\n",
    "def train(model, device, train_loaderIn, args):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args[\"lr\"], momentum = args[\"momentum\"])\n",
    "    # optimizer = optim.AdamW(model.parameters(), weight_decay=1e-6)\n",
    "    for epoch in range(1, args[\"epochs\"]+1):\n",
    "        train_epoch(epoch=epoch, args=args, model=model,\n",
    "         device=device, data_loader=train_loaderIn, optimizer=optimizer\n",
    "         )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from torch.nn import functional as F\n",
    "def train_epoch(epoch:int, args:dict, model, device, data_loader, optimizer):\n",
    "    model.train(True)\n",
    "    pid = os.getpid()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        # output_pred = F.log_softmax(output,dim=1)\n",
    "        loss = F.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args[\"log_interval\"] == 0:\n",
    "            print(\"{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                pid, epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.item()\n",
    "                ) \n",
    "            )\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def test(model, device, test_loaderIn):\n",
    "    test_epoch(model=model, device=device,data_loader=test_loaderIn)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def test_epoch(model, device, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = model(data.to(device))\n",
    "            # output_pred = F.log_softmax(output,dim=1)\n",
    "            test_loss += F.nll_loss(output, target.to(device),reduction='sum').item()\n",
    "            pred = output.max(1)[1]\n",
    "            correct += pred.eq(target.to(device)).sum().item()\n",
    "\n",
    "    test_loss /=  len(data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset))\n",
    "        )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#import the model\n",
    "from MileStone_3_Create_Neural_Network.WillyNet import WillyNet\n",
    "import torch\n",
    "assert torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "willy = WillyNet()\n",
    "from torchinfo import summary\n",
    "summary(willy)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "WillyNet                                 --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       87,300\n",
       "│    └─BatchNorm2d: 2-2                  360\n",
       "│    └─Mish: 2-3                         --\n",
       "│    └─Dropout2d: 2-4                    --\n",
       "├─Sequential: 1-2                        --\n",
       "│    └─Conv2d: 2-5                       1,458,100\n",
       "│    └─BatchNorm2d: 2-6                  200\n",
       "│    └─Mish: 2-7                         --\n",
       "│    └─Dropout2d: 2-8                    --\n",
       "├─Sequential: 1-3                        --\n",
       "│    └─Conv2d: 2-9                       1,280,100\n",
       "│    └─BatchNorm2d: 2-10                 200\n",
       "│    └─Mish: 2-11                        --\n",
       "│    └─Dropout2d: 2-12                   --\n",
       "├─Sequential: 1-4                        --\n",
       "│    └─Conv2d: 2-13                      864,060\n",
       "│    └─BatchNorm2d: 2-14                 120\n",
       "│    └─Mish: 2-15                        --\n",
       "│    └─Dropout2d: 2-16                   --\n",
       "├─Sequential: 1-5                        --\n",
       "│    └─Conv2d: 2-17                      76,820\n",
       "│    └─BatchNorm2d: 2-18                 40\n",
       "│    └─Mish: 2-19                        --\n",
       "│    └─Dropout2d: 2-20                   --\n",
       "├─Flatten: 1-6                           --\n",
       "├─Sequential: 1-7                        --\n",
       "│    └─Linear: 2-21                      450,100\n",
       "│    └─BatchNorm1d: 2-22                 200\n",
       "│    └─Mish: 2-23                        --\n",
       "├─Sequential: 1-8                        --\n",
       "│    └─Linear: 2-24                      1,818\n",
       "│    └─BatchNorm1d: 2-25                 36\n",
       "=================================================================\n",
       "Total params: 4,219,454\n",
       "Trainable params: 4,219,454\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\":0.5, \"epochs\": 5, \"log_interval\": 3 }\n",
    "device = torch.device('cuda:0')\n",
    "willy.to(device)\n",
    "willy.share_memory()\n",
    "train(model=willy, device=device, train_loader=train_loader,args=args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "464944\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: -0.214329\n",
      "464944\tTrain Epoch: 1 [256/9730 (3%)]\tLoss: -0.207512\n",
      "464944\tTrain Epoch: 1 [512/9730 (7%)]\tLoss: -0.222591\n",
      "464944\tTrain Epoch: 1 [768/9730 (10%)]\tLoss: -0.220490\n",
      "464944\tTrain Epoch: 1 [1024/9730 (14%)]\tLoss: -0.347552\n",
      "464944\tTrain Epoch: 1 [1280/9730 (17%)]\tLoss: -0.329333\n",
      "464944\tTrain Epoch: 1 [1536/9730 (21%)]\tLoss: -0.348770\n",
      "464944\tTrain Epoch: 1 [1792/9730 (24%)]\tLoss: -0.347140\n",
      "464944\tTrain Epoch: 1 [2048/9730 (28%)]\tLoss: -0.324026\n",
      "464944\tTrain Epoch: 1 [2304/9730 (31%)]\tLoss: -0.356711\n",
      "464944\tTrain Epoch: 1 [2560/9730 (34%)]\tLoss: -0.376132\n",
      "464944\tTrain Epoch: 1 [2816/9730 (38%)]\tLoss: -0.368968\n",
      "464944\tTrain Epoch: 1 [3072/9730 (41%)]\tLoss: -0.393984\n",
      "464944\tTrain Epoch: 1 [3328/9730 (45%)]\tLoss: -0.432054\n",
      "464944\tTrain Epoch: 1 [3584/9730 (48%)]\tLoss: -0.411580\n",
      "464944\tTrain Epoch: 1 [3840/9730 (52%)]\tLoss: -0.448485\n",
      "464944\tTrain Epoch: 1 [4096/9730 (55%)]\tLoss: -0.500170\n",
      "464944\tTrain Epoch: 1 [4352/9730 (59%)]\tLoss: -0.487960\n",
      "464944\tTrain Epoch: 1 [4608/9730 (62%)]\tLoss: -0.461770\n",
      "464944\tTrain Epoch: 1 [4864/9730 (66%)]\tLoss: -0.519137\n",
      "464944\tTrain Epoch: 1 [5120/9730 (69%)]\tLoss: -0.541068\n",
      "464944\tTrain Epoch: 1 [5376/9730 (72%)]\tLoss: -0.464733\n",
      "464944\tTrain Epoch: 1 [5632/9730 (76%)]\tLoss: -0.503201\n",
      "464944\tTrain Epoch: 1 [5888/9730 (79%)]\tLoss: -0.497970\n",
      "464944\tTrain Epoch: 1 [6144/9730 (83%)]\tLoss: -0.503771\n",
      "464944\tTrain Epoch: 1 [6400/9730 (86%)]\tLoss: -0.464699\n",
      "464944\tTrain Epoch: 1 [6656/9730 (90%)]\tLoss: -0.580834\n",
      "464944\tTrain Epoch: 1 [6912/9730 (93%)]\tLoss: -0.593262\n",
      "464944\tTrain Epoch: 1 [7168/9730 (97%)]\tLoss: -0.552489\n",
      "464944\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: -0.558345\n",
      "464944\tTrain Epoch: 2 [256/9730 (3%)]\tLoss: -0.585864\n",
      "464944\tTrain Epoch: 2 [512/9730 (7%)]\tLoss: -0.552925\n",
      "464944\tTrain Epoch: 2 [768/9730 (10%)]\tLoss: -0.514918\n",
      "464944\tTrain Epoch: 2 [1024/9730 (14%)]\tLoss: -0.542008\n",
      "464944\tTrain Epoch: 2 [1280/9730 (17%)]\tLoss: -0.532429\n",
      "464944\tTrain Epoch: 2 [1536/9730 (21%)]\tLoss: -0.621235\n",
      "464944\tTrain Epoch: 2 [1792/9730 (24%)]\tLoss: -0.576230\n",
      "464944\tTrain Epoch: 2 [2048/9730 (28%)]\tLoss: -0.569385\n",
      "464944\tTrain Epoch: 2 [2304/9730 (31%)]\tLoss: -0.472973\n",
      "464944\tTrain Epoch: 2 [2560/9730 (34%)]\tLoss: -0.588717\n",
      "464944\tTrain Epoch: 2 [2816/9730 (38%)]\tLoss: -0.645552\n",
      "464944\tTrain Epoch: 2 [3072/9730 (41%)]\tLoss: -0.591351\n",
      "464944\tTrain Epoch: 2 [3328/9730 (45%)]\tLoss: -0.546079\n",
      "464944\tTrain Epoch: 2 [3584/9730 (48%)]\tLoss: -0.657045\n",
      "464944\tTrain Epoch: 2 [3840/9730 (52%)]\tLoss: -0.673496\n",
      "464944\tTrain Epoch: 2 [4096/9730 (55%)]\tLoss: -0.500799\n",
      "464944\tTrain Epoch: 2 [4352/9730 (59%)]\tLoss: -0.598686\n",
      "464944\tTrain Epoch: 2 [4608/9730 (62%)]\tLoss: -0.568121\n",
      "464944\tTrain Epoch: 2 [4864/9730 (66%)]\tLoss: -0.625018\n",
      "464944\tTrain Epoch: 2 [5120/9730 (69%)]\tLoss: -0.595162\n",
      "464944\tTrain Epoch: 2 [5376/9730 (72%)]\tLoss: -0.569978\n",
      "464944\tTrain Epoch: 2 [5632/9730 (76%)]\tLoss: -0.599901\n",
      "464944\tTrain Epoch: 2 [5888/9730 (79%)]\tLoss: -0.677867\n",
      "464944\tTrain Epoch: 2 [6144/9730 (83%)]\tLoss: -0.655040\n",
      "464944\tTrain Epoch: 2 [6400/9730 (86%)]\tLoss: -0.705562\n",
      "464944\tTrain Epoch: 2 [6656/9730 (90%)]\tLoss: -0.552245\n",
      "464944\tTrain Epoch: 2 [6912/9730 (93%)]\tLoss: -0.695393\n",
      "464944\tTrain Epoch: 2 [7168/9730 (97%)]\tLoss: -0.635980\n",
      "464944\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: -0.586679\n",
      "464944\tTrain Epoch: 3 [256/9730 (3%)]\tLoss: -0.708267\n",
      "464944\tTrain Epoch: 3 [512/9730 (7%)]\tLoss: -0.617430\n",
      "464944\tTrain Epoch: 3 [768/9730 (10%)]\tLoss: -0.572932\n",
      "464944\tTrain Epoch: 3 [1024/9730 (14%)]\tLoss: -0.668227\n",
      "464944\tTrain Epoch: 3 [1280/9730 (17%)]\tLoss: -0.749329\n",
      "464944\tTrain Epoch: 3 [1536/9730 (21%)]\tLoss: -0.665671\n",
      "464944\tTrain Epoch: 3 [1792/9730 (24%)]\tLoss: -0.619152\n",
      "464944\tTrain Epoch: 3 [2048/9730 (28%)]\tLoss: -0.793710\n",
      "464944\tTrain Epoch: 3 [2304/9730 (31%)]\tLoss: -0.793386\n",
      "464944\tTrain Epoch: 3 [2560/9730 (34%)]\tLoss: -0.747932\n",
      "464944\tTrain Epoch: 3 [2816/9730 (38%)]\tLoss: -0.662002\n",
      "464944\tTrain Epoch: 3 [3072/9730 (41%)]\tLoss: -0.692685\n",
      "464944\tTrain Epoch: 3 [3328/9730 (45%)]\tLoss: -0.657660\n",
      "464944\tTrain Epoch: 3 [3584/9730 (48%)]\tLoss: -0.730004\n",
      "464944\tTrain Epoch: 3 [3840/9730 (52%)]\tLoss: -0.629310\n",
      "464944\tTrain Epoch: 3 [4096/9730 (55%)]\tLoss: -0.692205\n",
      "464944\tTrain Epoch: 3 [4352/9730 (59%)]\tLoss: -0.727699\n",
      "464944\tTrain Epoch: 3 [4608/9730 (62%)]\tLoss: -0.691476\n",
      "464944\tTrain Epoch: 3 [4864/9730 (66%)]\tLoss: -0.630734\n",
      "464944\tTrain Epoch: 3 [5120/9730 (69%)]\tLoss: -0.761858\n",
      "464944\tTrain Epoch: 3 [5376/9730 (72%)]\tLoss: -0.754696\n",
      "464944\tTrain Epoch: 3 [5632/9730 (76%)]\tLoss: -0.651443\n",
      "464944\tTrain Epoch: 3 [5888/9730 (79%)]\tLoss: -0.723280\n",
      "464944\tTrain Epoch: 3 [6144/9730 (83%)]\tLoss: -0.719629\n",
      "464944\tTrain Epoch: 3 [6400/9730 (86%)]\tLoss: -0.738266\n",
      "464944\tTrain Epoch: 3 [6656/9730 (90%)]\tLoss: -0.662242\n",
      "464944\tTrain Epoch: 3 [6912/9730 (93%)]\tLoss: -0.798135\n",
      "464944\tTrain Epoch: 3 [7168/9730 (97%)]\tLoss: -0.787185\n",
      "464944\tTrain Epoch: 4 [0/9730 (0%)]\tLoss: -0.863259\n",
      "464944\tTrain Epoch: 4 [256/9730 (3%)]\tLoss: -0.704975\n",
      "464944\tTrain Epoch: 4 [512/9730 (7%)]\tLoss: -0.777909\n",
      "464944\tTrain Epoch: 4 [768/9730 (10%)]\tLoss: -0.676736\n",
      "464944\tTrain Epoch: 4 [1024/9730 (14%)]\tLoss: -0.784392\n",
      "464944\tTrain Epoch: 4 [1280/9730 (17%)]\tLoss: -0.822324\n",
      "464944\tTrain Epoch: 4 [1536/9730 (21%)]\tLoss: -0.751920\n",
      "464944\tTrain Epoch: 4 [1792/9730 (24%)]\tLoss: -0.866949\n",
      "464944\tTrain Epoch: 4 [2048/9730 (28%)]\tLoss: -0.814179\n",
      "464944\tTrain Epoch: 4 [2304/9730 (31%)]\tLoss: -0.762092\n",
      "464944\tTrain Epoch: 4 [2560/9730 (34%)]\tLoss: -0.807163\n",
      "464944\tTrain Epoch: 4 [2816/9730 (38%)]\tLoss: -0.646948\n",
      "464944\tTrain Epoch: 4 [3072/9730 (41%)]\tLoss: -0.787776\n",
      "464944\tTrain Epoch: 4 [3328/9730 (45%)]\tLoss: -0.861957\n",
      "464944\tTrain Epoch: 4 [3584/9730 (48%)]\tLoss: -0.755333\n",
      "464944\tTrain Epoch: 4 [3840/9730 (52%)]\tLoss: -0.822981\n",
      "464944\tTrain Epoch: 4 [4096/9730 (55%)]\tLoss: -0.887969\n",
      "464944\tTrain Epoch: 4 [4352/9730 (59%)]\tLoss: -0.829387\n",
      "464944\tTrain Epoch: 4 [4608/9730 (62%)]\tLoss: -0.784809\n",
      "464944\tTrain Epoch: 4 [4864/9730 (66%)]\tLoss: -0.774681\n",
      "464944\tTrain Epoch: 4 [5120/9730 (69%)]\tLoss: -0.818282\n",
      "464944\tTrain Epoch: 4 [5376/9730 (72%)]\tLoss: -0.781915\n",
      "464944\tTrain Epoch: 4 [5632/9730 (76%)]\tLoss: -0.742090\n",
      "464944\tTrain Epoch: 4 [5888/9730 (79%)]\tLoss: -0.770429\n",
      "464944\tTrain Epoch: 4 [6144/9730 (83%)]\tLoss: -0.863884\n",
      "464944\tTrain Epoch: 4 [6400/9730 (86%)]\tLoss: -0.747898\n",
      "464944\tTrain Epoch: 4 [6656/9730 (90%)]\tLoss: -0.689527\n",
      "464944\tTrain Epoch: 4 [6912/9730 (93%)]\tLoss: -0.818349\n",
      "464944\tTrain Epoch: 4 [7168/9730 (97%)]\tLoss: -0.864271\n",
      "464944\tTrain Epoch: 5 [0/9730 (0%)]\tLoss: -0.851114\n",
      "464944\tTrain Epoch: 5 [256/9730 (3%)]\tLoss: -0.779795\n",
      "464944\tTrain Epoch: 5 [512/9730 (7%)]\tLoss: -0.747381\n",
      "464944\tTrain Epoch: 5 [768/9730 (10%)]\tLoss: -0.721651\n",
      "464944\tTrain Epoch: 5 [1024/9730 (14%)]\tLoss: -0.889797\n",
      "464944\tTrain Epoch: 5 [1280/9730 (17%)]\tLoss: -0.828859\n",
      "464944\tTrain Epoch: 5 [1536/9730 (21%)]\tLoss: -0.880945\n",
      "464944\tTrain Epoch: 5 [1792/9730 (24%)]\tLoss: -0.879265\n",
      "464944\tTrain Epoch: 5 [2048/9730 (28%)]\tLoss: -0.876412\n",
      "464944\tTrain Epoch: 5 [2304/9730 (31%)]\tLoss: -0.808943\n",
      "464944\tTrain Epoch: 5 [2560/9730 (34%)]\tLoss: -0.909370\n",
      "464944\tTrain Epoch: 5 [2816/9730 (38%)]\tLoss: -0.885865\n",
      "464944\tTrain Epoch: 5 [3072/9730 (41%)]\tLoss: -0.883734\n",
      "464944\tTrain Epoch: 5 [3328/9730 (45%)]\tLoss: -0.821149\n",
      "464944\tTrain Epoch: 5 [3584/9730 (48%)]\tLoss: -0.965921\n",
      "464944\tTrain Epoch: 5 [3840/9730 (52%)]\tLoss: -0.770762\n",
      "464944\tTrain Epoch: 5 [4096/9730 (55%)]\tLoss: -0.830924\n",
      "464944\tTrain Epoch: 5 [4352/9730 (59%)]\tLoss: -0.858256\n",
      "464944\tTrain Epoch: 5 [4608/9730 (62%)]\tLoss: -1.011328\n",
      "464944\tTrain Epoch: 5 [4864/9730 (66%)]\tLoss: -0.970591\n",
      "464944\tTrain Epoch: 5 [5120/9730 (69%)]\tLoss: -0.930335\n",
      "464944\tTrain Epoch: 5 [5376/9730 (72%)]\tLoss: -0.819715\n",
      "464944\tTrain Epoch: 5 [5632/9730 (76%)]\tLoss: -0.835265\n",
      "464944\tTrain Epoch: 5 [5888/9730 (79%)]\tLoss: -0.915883\n",
      "464944\tTrain Epoch: 5 [6144/9730 (83%)]\tLoss: -0.999073\n",
      "464944\tTrain Epoch: 5 [6400/9730 (86%)]\tLoss: -0.920204\n",
      "464944\tTrain Epoch: 5 [6656/9730 (90%)]\tLoss: -0.979229\n",
      "464944\tTrain Epoch: 5 [6912/9730 (93%)]\tLoss: -0.869860\n",
      "464944\tTrain Epoch: 5 [7168/9730 (97%)]\tLoss: -0.952890\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "test(model=willy, device=device, test_loader=train_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: -0.7862, Accuracy: 1450/9730 (15%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 5, \"log_interval\": 3}\n",
    "device = torch.device('cuda:0')\n",
    "willy.to(device)\n",
    "willy.share_memory()\n",
    "train(model=willy, device=device, train_loader=train_loader, args=args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "487114\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: 3.085736\n",
      "487114\tTrain Epoch: 1 [1200/9730 (17%)]\tLoss: 3.062003\n",
      "487114\tTrain Epoch: 1 [2400/9730 (33%)]\tLoss: 2.928991\n",
      "487114\tTrain Epoch: 1 [3600/9730 (50%)]\tLoss: 2.824156\n",
      "487114\tTrain Epoch: 1 [4800/9730 (67%)]\tLoss: 2.825988\n",
      "487114\tTrain Epoch: 1 [6000/9730 (83%)]\tLoss: 2.807476\n",
      "487114\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: 2.797379\n",
      "487114\tTrain Epoch: 2 [1200/9730 (17%)]\tLoss: 2.833917\n",
      "487114\tTrain Epoch: 2 [2400/9730 (33%)]\tLoss: 2.821617\n",
      "487114\tTrain Epoch: 2 [3600/9730 (50%)]\tLoss: 2.795099\n",
      "487114\tTrain Epoch: 2 [4800/9730 (67%)]\tLoss: 2.778999\n",
      "487114\tTrain Epoch: 2 [6000/9730 (83%)]\tLoss: 2.774713\n",
      "487114\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: 2.767551\n",
      "487114\tTrain Epoch: 3 [1200/9730 (17%)]\tLoss: 2.746999\n",
      "487114\tTrain Epoch: 3 [2400/9730 (33%)]\tLoss: 2.752237\n",
      "487114\tTrain Epoch: 3 [3600/9730 (50%)]\tLoss: 2.728916\n",
      "487114\tTrain Epoch: 3 [4800/9730 (67%)]\tLoss: 2.768703\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test(model=willy, device=device, test_loader=train_loader)\n",
    "test(model=willy, device=device,test_loader=valid_loader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 5, \"log_interval\": 3}\n",
    "device = torch.device('cuda:0')\n",
    "willy.to(device)\n",
    "willy.share_memory()\n",
    "train(model=willy, device=device, train_loader=train_loader, args=args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "496275\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: 3.167415\n",
      "496275\tTrain Epoch: 1 [1200/9730 (17%)]\tLoss: 3.015418\n",
      "496275\tTrain Epoch: 1 [2400/9730 (33%)]\tLoss: 2.904104\n",
      "496275\tTrain Epoch: 1 [3600/9730 (50%)]\tLoss: 2.813510\n",
      "496275\tTrain Epoch: 1 [4800/9730 (67%)]\tLoss: 2.781481\n",
      "496275\tTrain Epoch: 1 [6000/9730 (83%)]\tLoss: 2.756980\n",
      "496275\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: 2.762283\n",
      "496275\tTrain Epoch: 2 [1200/9730 (17%)]\tLoss: 2.762573\n",
      "496275\tTrain Epoch: 2 [2400/9730 (33%)]\tLoss: 2.739838\n",
      "496275\tTrain Epoch: 2 [3600/9730 (50%)]\tLoss: 2.738945\n",
      "496275\tTrain Epoch: 2 [4800/9730 (67%)]\tLoss: 2.733472\n",
      "496275\tTrain Epoch: 2 [6000/9730 (83%)]\tLoss: 2.730531\n",
      "496275\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: 2.741371\n",
      "496275\tTrain Epoch: 3 [1200/9730 (17%)]\tLoss: 2.725900\n",
      "496275\tTrain Epoch: 3 [2400/9730 (33%)]\tLoss: 2.732535\n",
      "496275\tTrain Epoch: 3 [3600/9730 (50%)]\tLoss: 2.731039\n",
      "496275\tTrain Epoch: 3 [4800/9730 (67%)]\tLoss: 2.715538\n",
      "496275\tTrain Epoch: 3 [6000/9730 (83%)]\tLoss: 2.694800\n",
      "496275\tTrain Epoch: 4 [0/9730 (0%)]\tLoss: 2.687846\n",
      "496275\tTrain Epoch: 4 [1200/9730 (17%)]\tLoss: 2.651060\n",
      "496275\tTrain Epoch: 4 [2400/9730 (33%)]\tLoss: 2.704383\n",
      "496275\tTrain Epoch: 4 [3600/9730 (50%)]\tLoss: 2.610762\n",
      "496275\tTrain Epoch: 4 [4800/9730 (67%)]\tLoss: 2.729790\n",
      "496275\tTrain Epoch: 4 [6000/9730 (83%)]\tLoss: 2.707009\n",
      "496275\tTrain Epoch: 5 [0/9730 (0%)]\tLoss: 2.622314\n",
      "496275\tTrain Epoch: 5 [1200/9730 (17%)]\tLoss: 2.661959\n",
      "496275\tTrain Epoch: 5 [2400/9730 (33%)]\tLoss: 2.705039\n",
      "496275\tTrain Epoch: 5 [3600/9730 (50%)]\tLoss: 2.637606\n",
      "496275\tTrain Epoch: 5 [4800/9730 (67%)]\tLoss: 2.661675\n",
      "496275\tTrain Epoch: 5 [6000/9730 (83%)]\tLoss: 2.710067\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "test(model=willy, device=device, test_loader=train_loader)\n",
    "test(model=willy, device=device, test_loader=valid_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 1.9487, Accuracy: 1347/9730 (14%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3256, Accuracy: 238/9730 (2%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "test(model=willy, device=device, test_loader=train_loader)\n",
    "test(model=willy, device=device, test_loader=valid_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 2.1293, Accuracy: 22/9730 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3552, Accuracy: 1/9730 (0%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 5, \"log_interval\": 3}\n",
    "device = torch.device('cuda:0')\n",
    "willy.to(device)\n",
    "willy.share_memory()\n",
    "train(model=willy, device=device, train_loaderIn=train_loader, args=args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "515045\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: 3.370296\n",
      "515045\tTrain Epoch: 1 [1200/9730 (17%)]\tLoss: 3.204740\n",
      "515045\tTrain Epoch: 1 [2400/9730 (33%)]\tLoss: 3.100020\n",
      "515045\tTrain Epoch: 1 [3600/9730 (50%)]\tLoss: 3.009669\n",
      "515045\tTrain Epoch: 1 [4800/9730 (67%)]\tLoss: 2.976737\n",
      "515045\tTrain Epoch: 1 [6000/9730 (83%)]\tLoss: 2.938450\n",
      "515045\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: 2.910660\n",
      "515045\tTrain Epoch: 2 [1200/9730 (17%)]\tLoss: 2.922548\n",
      "515045\tTrain Epoch: 2 [2400/9730 (33%)]\tLoss: 2.840514\n",
      "515045\tTrain Epoch: 2 [3600/9730 (50%)]\tLoss: 2.803405\n",
      "515045\tTrain Epoch: 2 [4800/9730 (67%)]\tLoss: 2.894019\n",
      "515045\tTrain Epoch: 2 [6000/9730 (83%)]\tLoss: 2.852977\n",
      "515045\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: 2.825532\n",
      "515045\tTrain Epoch: 3 [1200/9730 (17%)]\tLoss: 2.793926\n",
      "515045\tTrain Epoch: 3 [2400/9730 (33%)]\tLoss: 2.890319\n",
      "515045\tTrain Epoch: 3 [3600/9730 (50%)]\tLoss: 2.801518\n",
      "515045\tTrain Epoch: 3 [4800/9730 (67%)]\tLoss: 2.830374\n",
      "515045\tTrain Epoch: 3 [6000/9730 (83%)]\tLoss: 2.795450\n",
      "515045\tTrain Epoch: 4 [0/9730 (0%)]\tLoss: 2.819628\n",
      "515045\tTrain Epoch: 4 [1200/9730 (17%)]\tLoss: 2.816566\n",
      "515045\tTrain Epoch: 4 [2400/9730 (33%)]\tLoss: 2.732295\n",
      "515045\tTrain Epoch: 4 [3600/9730 (50%)]\tLoss: 2.682284\n",
      "515045\tTrain Epoch: 4 [4800/9730 (67%)]\tLoss: 2.772609\n",
      "515045\tTrain Epoch: 4 [6000/9730 (83%)]\tLoss: 2.774455\n",
      "515045\tTrain Epoch: 5 [0/9730 (0%)]\tLoss: 2.748366\n",
      "515045\tTrain Epoch: 5 [1200/9730 (17%)]\tLoss: 2.680588\n",
      "515045\tTrain Epoch: 5 [2400/9730 (33%)]\tLoss: 2.754518\n",
      "515045\tTrain Epoch: 5 [3600/9730 (50%)]\tLoss: 2.723118\n",
      "515045\tTrain Epoch: 5 [4800/9730 (67%)]\tLoss: 2.739094\n",
      "515045\tTrain Epoch: 5 [6000/9730 (83%)]\tLoss: 2.747403\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\n",
    "test(model=willy, device=device, test_loader=train_loader)\n",
    "test(model=willy, device=device, test_loader=valid_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 1.9835, Accuracy: 1301/9730 (13%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3285, Accuracy: 203/9730 (2%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, dilation=2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, dilation=2)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size=5, dilation=2)\n",
    "        self.conv4 = nn.Conv2d(30, 40, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.conv3_drop = nn.Dropout2d()\n",
    "        self.conv4_drop = nn.Dropout2d()\n",
    "        # self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1440, 720)\n",
    "        self.fc2 = nn.Linear(720, 360)\n",
    "        self.fc3 = nn.Linear(360, 180)\n",
    "        self.fc4 = nn.Linear(180, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.mish(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.mish(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.mish(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n",
    "        x = F.mish(F.max_pool2d(self.conv4_drop(self.conv4(x)), 2))\n",
    "\n",
    "        #x = x.view(-1, 7200)\n",
    "        x = self.flatten(x)\n",
    "        x = F.mish(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.mish(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.mish(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "willy = Net()\n",
    "summary(willy,input_size=(400,3,198,198))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      --                        --\n",
       "├─Conv2d: 1-1                            [400, 10, 190, 190]       760\n",
       "├─Conv2d: 1-2                            [400, 20, 87, 87]         5,020\n",
       "├─Dropout2d: 1-3                         [400, 20, 87, 87]         --\n",
       "├─Conv2d: 1-4                            [400, 30, 35, 35]         15,030\n",
       "├─Dropout2d: 1-5                         [400, 30, 35, 35]         --\n",
       "├─Conv2d: 1-6                            [400, 40, 13, 13]         30,040\n",
       "├─Dropout2d: 1-7                         [400, 40, 13, 13]         --\n",
       "├─Flatten: 1-8                           [400, 1440]               --\n",
       "├─Linear: 1-9                            [400, 720]                1,037,520\n",
       "├─Linear: 1-10                           [400, 360]                259,560\n",
       "├─Linear: 1-11                           [400, 180]                64,980\n",
       "├─Linear: 1-12                           [400, 18]                 3,258\n",
       "==========================================================================================\n",
       "Total params: 1,416,168\n",
       "Trainable params: 1,416,168\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 36.11\n",
       "==========================================================================================\n",
       "Input size (MB): 188.18\n",
       "Forward/backward pass size (MB): 1782.94\n",
       "Params size (MB): 5.66\n",
       "Estimated Total Size (MB): 1976.78\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 50, \"log_interval\": 3}\n",
    "#device = torch.device('cuda:0')\n",
    "#willy.to(device)\n",
    "#willy.share_memory()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "test(model=willy, device=device, test_loaderIn=train_loader)\n",
    "test(model=willy, device=device, test_loaderIn=valid_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 2.1395, Accuracy: 0/9730 (0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3574, Accuracy: 9/9730 (0%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 5, \"log_interval\": 3}\n",
    "# device = torch.device('cuda:0')\n",
    "# willy.to(device)\n",
    "# willy.share_memory()\n",
    "train(model=willy, device=device, train_loaderIn=train_loader, args=args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "557596\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: 2.889803\n",
      "557596\tTrain Epoch: 1 [1200/9730 (17%)]\tLoss: 2.893542\n",
      "557596\tTrain Epoch: 1 [2400/9730 (33%)]\tLoss: 2.885046\n",
      "557596\tTrain Epoch: 1 [3600/9730 (50%)]\tLoss: 2.880883\n",
      "557596\tTrain Epoch: 1 [4800/9730 (67%)]\tLoss: 2.881128\n",
      "557596\tTrain Epoch: 1 [6000/9730 (83%)]\tLoss: 2.882294\n",
      "557596\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: 2.876898\n",
      "557596\tTrain Epoch: 2 [1200/9730 (17%)]\tLoss: 2.874690\n",
      "557596\tTrain Epoch: 2 [2400/9730 (33%)]\tLoss: 2.870163\n",
      "557596\tTrain Epoch: 2 [3600/9730 (50%)]\tLoss: 2.869845\n",
      "557596\tTrain Epoch: 2 [4800/9730 (67%)]\tLoss: 2.869622\n",
      "557596\tTrain Epoch: 2 [6000/9730 (83%)]\tLoss: 2.863748\n",
      "557596\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: 2.864494\n",
      "557596\tTrain Epoch: 3 [1200/9730 (17%)]\tLoss: 2.861001\n",
      "557596\tTrain Epoch: 3 [2400/9730 (33%)]\tLoss: 2.859577\n",
      "557596\tTrain Epoch: 3 [3600/9730 (50%)]\tLoss: 2.847371\n",
      "557596\tTrain Epoch: 3 [4800/9730 (67%)]\tLoss: 2.852261\n",
      "557596\tTrain Epoch: 3 [6000/9730 (83%)]\tLoss: 2.851653\n",
      "557596\tTrain Epoch: 4 [0/9730 (0%)]\tLoss: 2.850224\n",
      "557596\tTrain Epoch: 4 [1200/9730 (17%)]\tLoss: 2.837436\n",
      "557596\tTrain Epoch: 4 [2400/9730 (33%)]\tLoss: 2.844940\n",
      "557596\tTrain Epoch: 4 [3600/9730 (50%)]\tLoss: 2.836527\n",
      "557596\tTrain Epoch: 4 [4800/9730 (67%)]\tLoss: 2.830121\n",
      "557596\tTrain Epoch: 4 [6000/9730 (83%)]\tLoss: 2.828812\n",
      "557596\tTrain Epoch: 5 [0/9730 (0%)]\tLoss: 2.829833\n",
      "557596\tTrain Epoch: 5 [1200/9730 (17%)]\tLoss: 2.830441\n",
      "557596\tTrain Epoch: 5 [2400/9730 (33%)]\tLoss: 2.822246\n",
      "557596\tTrain Epoch: 5 [3600/9730 (50%)]\tLoss: 2.818930\n",
      "557596\tTrain Epoch: 5 [4800/9730 (67%)]\tLoss: 2.811319\n",
      "557596\tTrain Epoch: 5 [6000/9730 (83%)]\tLoss: 2.814219\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "train(model=willy, device=device, train_loaderIn=train_loader, args=args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "557596\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: 2.817371\n",
      "557596\tTrain Epoch: 1 [1200/9730 (17%)]\tLoss: 2.809611\n",
      "557596\tTrain Epoch: 1 [2400/9730 (33%)]\tLoss: 2.802074\n",
      "557596\tTrain Epoch: 1 [3600/9730 (50%)]\tLoss: 2.799877\n",
      "557596\tTrain Epoch: 1 [4800/9730 (67%)]\tLoss: 2.799095\n",
      "557596\tTrain Epoch: 1 [6000/9730 (83%)]\tLoss: 2.784458\n",
      "557596\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: 2.787398\n",
      "557596\tTrain Epoch: 2 [1200/9730 (17%)]\tLoss: 2.773044\n",
      "557596\tTrain Epoch: 2 [2400/9730 (33%)]\tLoss: 2.773338\n",
      "557596\tTrain Epoch: 2 [3600/9730 (50%)]\tLoss: 2.772168\n",
      "557596\tTrain Epoch: 2 [4800/9730 (67%)]\tLoss: 2.758266\n",
      "557596\tTrain Epoch: 2 [6000/9730 (83%)]\tLoss: 2.771692\n",
      "557596\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: 2.722183\n",
      "557596\tTrain Epoch: 3 [1200/9730 (17%)]\tLoss: 2.722759\n",
      "557596\tTrain Epoch: 3 [2400/9730 (33%)]\tLoss: 2.747673\n",
      "557596\tTrain Epoch: 3 [3600/9730 (50%)]\tLoss: 2.729829\n",
      "557596\tTrain Epoch: 3 [4800/9730 (67%)]\tLoss: 2.722520\n",
      "557596\tTrain Epoch: 3 [6000/9730 (83%)]\tLoss: 2.728061\n",
      "557596\tTrain Epoch: 4 [0/9730 (0%)]\tLoss: 2.710036\n",
      "557596\tTrain Epoch: 4 [1200/9730 (17%)]\tLoss: 2.688787\n",
      "557596\tTrain Epoch: 4 [2400/9730 (33%)]\tLoss: 2.684270\n",
      "557596\tTrain Epoch: 4 [3600/9730 (50%)]\tLoss: 2.679353\n",
      "557596\tTrain Epoch: 4 [4800/9730 (67%)]\tLoss: 2.649357\n",
      "557596\tTrain Epoch: 4 [6000/9730 (83%)]\tLoss: 2.664538\n",
      "557596\tTrain Epoch: 5 [0/9730 (0%)]\tLoss: 2.656217\n",
      "557596\tTrain Epoch: 5 [1200/9730 (17%)]\tLoss: 2.692667\n",
      "557596\tTrain Epoch: 5 [2400/9730 (33%)]\tLoss: 2.698024\n",
      "557596\tTrain Epoch: 5 [3600/9730 (50%)]\tLoss: 2.652484\n",
      "557596\tTrain Epoch: 5 [4800/9730 (67%)]\tLoss: 2.622679\n",
      "557596\tTrain Epoch: 5 [6000/9730 (83%)]\tLoss: 2.613573\n",
      "557596\tTrain Epoch: 6 [0/9730 (0%)]\tLoss: 2.632643\n",
      "557596\tTrain Epoch: 6 [1200/9730 (17%)]\tLoss: 2.641989\n",
      "557596\tTrain Epoch: 6 [2400/9730 (33%)]\tLoss: 2.649658\n",
      "557596\tTrain Epoch: 6 [3600/9730 (50%)]\tLoss: 2.644794\n",
      "557596\tTrain Epoch: 6 [4800/9730 (67%)]\tLoss: 2.625377\n",
      "557596\tTrain Epoch: 6 [6000/9730 (83%)]\tLoss: 2.652650\n",
      "557596\tTrain Epoch: 7 [0/9730 (0%)]\tLoss: 2.571929\n",
      "557596\tTrain Epoch: 7 [1200/9730 (17%)]\tLoss: 2.606719\n",
      "557596\tTrain Epoch: 7 [2400/9730 (33%)]\tLoss: 2.654730\n",
      "557596\tTrain Epoch: 7 [3600/9730 (50%)]\tLoss: 2.616555\n",
      "557596\tTrain Epoch: 7 [4800/9730 (67%)]\tLoss: 2.584091\n",
      "557596\tTrain Epoch: 7 [6000/9730 (83%)]\tLoss: 2.631976\n",
      "557596\tTrain Epoch: 8 [0/9730 (0%)]\tLoss: 2.590169\n",
      "557596\tTrain Epoch: 8 [1200/9730 (17%)]\tLoss: 2.600388\n",
      "557596\tTrain Epoch: 8 [2400/9730 (33%)]\tLoss: 2.630746\n",
      "557596\tTrain Epoch: 8 [3600/9730 (50%)]\tLoss: 2.619075\n",
      "557596\tTrain Epoch: 8 [4800/9730 (67%)]\tLoss: 2.623245\n",
      "557596\tTrain Epoch: 8 [6000/9730 (83%)]\tLoss: 2.565690\n",
      "557596\tTrain Epoch: 9 [0/9730 (0%)]\tLoss: 2.615217\n",
      "557596\tTrain Epoch: 9 [1200/9730 (17%)]\tLoss: 2.596539\n",
      "557596\tTrain Epoch: 9 [2400/9730 (33%)]\tLoss: 2.547177\n",
      "557596\tTrain Epoch: 9 [3600/9730 (50%)]\tLoss: 2.569074\n",
      "557596\tTrain Epoch: 9 [4800/9730 (67%)]\tLoss: 2.619194\n",
      "557596\tTrain Epoch: 9 [6000/9730 (83%)]\tLoss: 2.596118\n",
      "557596\tTrain Epoch: 10 [0/9730 (0%)]\tLoss: 2.624213\n",
      "557596\tTrain Epoch: 10 [1200/9730 (17%)]\tLoss: 2.617817\n",
      "557596\tTrain Epoch: 10 [2400/9730 (33%)]\tLoss: 2.649412\n",
      "557596\tTrain Epoch: 10 [3600/9730 (50%)]\tLoss: 2.540990\n",
      "557596\tTrain Epoch: 10 [4800/9730 (67%)]\tLoss: 2.559933\n",
      "557596\tTrain Epoch: 10 [6000/9730 (83%)]\tLoss: 2.563226\n",
      "557596\tTrain Epoch: 11 [0/9730 (0%)]\tLoss: 2.596202\n",
      "557596\tTrain Epoch: 11 [1200/9730 (17%)]\tLoss: 2.570070\n",
      "557596\tTrain Epoch: 11 [2400/9730 (33%)]\tLoss: 2.604243\n",
      "557596\tTrain Epoch: 11 [3600/9730 (50%)]\tLoss: 2.548402\n",
      "557596\tTrain Epoch: 11 [4800/9730 (67%)]\tLoss: 2.570900\n",
      "557596\tTrain Epoch: 11 [6000/9730 (83%)]\tLoss: 2.558025\n",
      "557596\tTrain Epoch: 12 [0/9730 (0%)]\tLoss: 2.584215\n",
      "557596\tTrain Epoch: 12 [1200/9730 (17%)]\tLoss: 2.594350\n",
      "557596\tTrain Epoch: 12 [2400/9730 (33%)]\tLoss: 2.546836\n",
      "557596\tTrain Epoch: 12 [3600/9730 (50%)]\tLoss: 2.537827\n",
      "557596\tTrain Epoch: 12 [4800/9730 (67%)]\tLoss: 2.567613\n",
      "557596\tTrain Epoch: 12 [6000/9730 (83%)]\tLoss: 2.559380\n",
      "557596\tTrain Epoch: 13 [0/9730 (0%)]\tLoss: 2.548444\n",
      "557596\tTrain Epoch: 13 [1200/9730 (17%)]\tLoss: 2.546841\n",
      "557596\tTrain Epoch: 13 [2400/9730 (33%)]\tLoss: 2.534824\n",
      "557596\tTrain Epoch: 13 [3600/9730 (50%)]\tLoss: 2.590453\n",
      "557596\tTrain Epoch: 13 [4800/9730 (67%)]\tLoss: 2.552819\n",
      "557596\tTrain Epoch: 13 [6000/9730 (83%)]\tLoss: 2.574283\n",
      "557596\tTrain Epoch: 14 [0/9730 (0%)]\tLoss: 2.608114\n",
      "557596\tTrain Epoch: 14 [1200/9730 (17%)]\tLoss: 2.532061\n",
      "557596\tTrain Epoch: 14 [2400/9730 (33%)]\tLoss: 2.593344\n",
      "557596\tTrain Epoch: 14 [3600/9730 (50%)]\tLoss: 2.548588\n",
      "557596\tTrain Epoch: 14 [4800/9730 (67%)]\tLoss: 2.566640\n",
      "557596\tTrain Epoch: 14 [6000/9730 (83%)]\tLoss: 2.548855\n",
      "557596\tTrain Epoch: 15 [0/9730 (0%)]\tLoss: 2.545857\n",
      "557596\tTrain Epoch: 15 [1200/9730 (17%)]\tLoss: 2.551913\n",
      "557596\tTrain Epoch: 15 [2400/9730 (33%)]\tLoss: 2.585950\n",
      "557596\tTrain Epoch: 15 [3600/9730 (50%)]\tLoss: 2.525106\n",
      "557596\tTrain Epoch: 15 [4800/9730 (67%)]\tLoss: 2.548090\n",
      "557596\tTrain Epoch: 15 [6000/9730 (83%)]\tLoss: 2.522767\n",
      "557596\tTrain Epoch: 16 [0/9730 (0%)]\tLoss: 2.637387\n",
      "557596\tTrain Epoch: 16 [1200/9730 (17%)]\tLoss: 2.572869\n",
      "557596\tTrain Epoch: 16 [2400/9730 (33%)]\tLoss: 2.535884\n",
      "557596\tTrain Epoch: 16 [3600/9730 (50%)]\tLoss: 2.536921\n",
      "557596\tTrain Epoch: 16 [4800/9730 (67%)]\tLoss: 2.527370\n",
      "557596\tTrain Epoch: 16 [6000/9730 (83%)]\tLoss: 2.555997\n",
      "557596\tTrain Epoch: 17 [0/9730 (0%)]\tLoss: 2.529608\n",
      "557596\tTrain Epoch: 17 [1200/9730 (17%)]\tLoss: 2.569895\n",
      "557596\tTrain Epoch: 17 [2400/9730 (33%)]\tLoss: 2.578605\n",
      "557596\tTrain Epoch: 17 [3600/9730 (50%)]\tLoss: 2.526174\n",
      "557596\tTrain Epoch: 17 [4800/9730 (67%)]\tLoss: 2.557008\n",
      "557596\tTrain Epoch: 17 [6000/9730 (83%)]\tLoss: 2.494221\n",
      "557596\tTrain Epoch: 18 [0/9730 (0%)]\tLoss: 2.546708\n",
      "557596\tTrain Epoch: 18 [1200/9730 (17%)]\tLoss: 2.556224\n",
      "557596\tTrain Epoch: 18 [2400/9730 (33%)]\tLoss: 2.546115\n",
      "557596\tTrain Epoch: 18 [3600/9730 (50%)]\tLoss: 2.525785\n",
      "557596\tTrain Epoch: 18 [4800/9730 (67%)]\tLoss: 2.523673\n",
      "557596\tTrain Epoch: 18 [6000/9730 (83%)]\tLoss: 2.540277\n",
      "557596\tTrain Epoch: 19 [0/9730 (0%)]\tLoss: 2.548625\n",
      "557596\tTrain Epoch: 19 [1200/9730 (17%)]\tLoss: 2.502096\n",
      "557596\tTrain Epoch: 19 [2400/9730 (33%)]\tLoss: 2.540073\n",
      "557596\tTrain Epoch: 19 [3600/9730 (50%)]\tLoss: 2.533863\n",
      "557596\tTrain Epoch: 19 [4800/9730 (67%)]\tLoss: 2.495618\n",
      "557596\tTrain Epoch: 19 [6000/9730 (83%)]\tLoss: 2.560745\n",
      "557596\tTrain Epoch: 20 [0/9730 (0%)]\tLoss: 2.556369\n",
      "557596\tTrain Epoch: 20 [1200/9730 (17%)]\tLoss: 2.534768\n",
      "557596\tTrain Epoch: 20 [2400/9730 (33%)]\tLoss: 2.550864\n",
      "557596\tTrain Epoch: 20 [3600/9730 (50%)]\tLoss: 2.498376\n",
      "557596\tTrain Epoch: 20 [4800/9730 (67%)]\tLoss: 2.507410\n",
      "557596\tTrain Epoch: 20 [6000/9730 (83%)]\tLoss: 2.552815\n",
      "557596\tTrain Epoch: 21 [0/9730 (0%)]\tLoss: 2.558861\n",
      "557596\tTrain Epoch: 21 [1200/9730 (17%)]\tLoss: 2.516060\n",
      "557596\tTrain Epoch: 21 [2400/9730 (33%)]\tLoss: 2.523438\n",
      "557596\tTrain Epoch: 21 [3600/9730 (50%)]\tLoss: 2.523257\n",
      "557596\tTrain Epoch: 21 [4800/9730 (67%)]\tLoss: 2.539580\n",
      "557596\tTrain Epoch: 21 [6000/9730 (83%)]\tLoss: 2.530889\n",
      "557596\tTrain Epoch: 22 [0/9730 (0%)]\tLoss: 2.608711\n",
      "557596\tTrain Epoch: 22 [1200/9730 (17%)]\tLoss: 2.479306\n",
      "557596\tTrain Epoch: 22 [2400/9730 (33%)]\tLoss: 2.521599\n",
      "557596\tTrain Epoch: 22 [3600/9730 (50%)]\tLoss: 2.539037\n",
      "557596\tTrain Epoch: 22 [4800/9730 (67%)]\tLoss: 2.466382\n",
      "557596\tTrain Epoch: 22 [6000/9730 (83%)]\tLoss: 2.551231\n",
      "557596\tTrain Epoch: 23 [0/9730 (0%)]\tLoss: 2.519377\n",
      "557596\tTrain Epoch: 23 [1200/9730 (17%)]\tLoss: 2.526673\n",
      "557596\tTrain Epoch: 23 [2400/9730 (33%)]\tLoss: 2.568272\n",
      "557596\tTrain Epoch: 23 [3600/9730 (50%)]\tLoss: 2.519632\n",
      "557596\tTrain Epoch: 23 [4800/9730 (67%)]\tLoss: 2.516624\n",
      "557596\tTrain Epoch: 23 [6000/9730 (83%)]\tLoss: 2.556089\n",
      "557596\tTrain Epoch: 24 [0/9730 (0%)]\tLoss: 2.517603\n",
      "557596\tTrain Epoch: 24 [1200/9730 (17%)]\tLoss: 2.508987\n",
      "557596\tTrain Epoch: 24 [2400/9730 (33%)]\tLoss: 2.519031\n",
      "557596\tTrain Epoch: 24 [3600/9730 (50%)]\tLoss: 2.559810\n",
      "557596\tTrain Epoch: 24 [4800/9730 (67%)]\tLoss: 2.507333\n",
      "557596\tTrain Epoch: 24 [6000/9730 (83%)]\tLoss: 2.513104\n",
      "557596\tTrain Epoch: 25 [0/9730 (0%)]\tLoss: 2.551616\n",
      "557596\tTrain Epoch: 25 [1200/9730 (17%)]\tLoss: 2.523601\n",
      "557596\tTrain Epoch: 25 [2400/9730 (33%)]\tLoss: 2.556205\n",
      "557596\tTrain Epoch: 25 [3600/9730 (50%)]\tLoss: 2.546900\n",
      "557596\tTrain Epoch: 25 [4800/9730 (67%)]\tLoss: 2.521196\n",
      "557596\tTrain Epoch: 25 [6000/9730 (83%)]\tLoss: 2.529542\n",
      "557596\tTrain Epoch: 26 [0/9730 (0%)]\tLoss: 2.497064\n",
      "557596\tTrain Epoch: 26 [1200/9730 (17%)]\tLoss: 2.499885\n",
      "557596\tTrain Epoch: 26 [2400/9730 (33%)]\tLoss: 2.515497\n",
      "557596\tTrain Epoch: 26 [3600/9730 (50%)]\tLoss: 2.506312\n",
      "557596\tTrain Epoch: 26 [4800/9730 (67%)]\tLoss: 2.573678\n",
      "557596\tTrain Epoch: 26 [6000/9730 (83%)]\tLoss: 2.535694\n",
      "557596\tTrain Epoch: 27 [0/9730 (0%)]\tLoss: 2.573167\n",
      "557596\tTrain Epoch: 27 [1200/9730 (17%)]\tLoss: 2.518407\n",
      "557596\tTrain Epoch: 27 [2400/9730 (33%)]\tLoss: 2.486462\n",
      "557596\tTrain Epoch: 27 [3600/9730 (50%)]\tLoss: 2.569178\n",
      "557596\tTrain Epoch: 27 [4800/9730 (67%)]\tLoss: 2.508977\n",
      "557596\tTrain Epoch: 27 [6000/9730 (83%)]\tLoss: 2.490347\n",
      "557596\tTrain Epoch: 28 [0/9730 (0%)]\tLoss: 2.530723\n",
      "557596\tTrain Epoch: 28 [1200/9730 (17%)]\tLoss: 2.489111\n",
      "557596\tTrain Epoch: 28 [2400/9730 (33%)]\tLoss: 2.552932\n",
      "557596\tTrain Epoch: 28 [3600/9730 (50%)]\tLoss: 2.501387\n",
      "557596\tTrain Epoch: 28 [4800/9730 (67%)]\tLoss: 2.535451\n",
      "557596\tTrain Epoch: 28 [6000/9730 (83%)]\tLoss: 2.519932\n",
      "557596\tTrain Epoch: 29 [0/9730 (0%)]\tLoss: 2.539283\n",
      "557596\tTrain Epoch: 29 [1200/9730 (17%)]\tLoss: 2.537446\n",
      "557596\tTrain Epoch: 29 [2400/9730 (33%)]\tLoss: 2.513901\n",
      "557596\tTrain Epoch: 29 [3600/9730 (50%)]\tLoss: 2.485672\n",
      "557596\tTrain Epoch: 29 [4800/9730 (67%)]\tLoss: 2.535850\n",
      "557596\tTrain Epoch: 29 [6000/9730 (83%)]\tLoss: 2.514647\n",
      "557596\tTrain Epoch: 30 [0/9730 (0%)]\tLoss: 2.519018\n",
      "557596\tTrain Epoch: 30 [1200/9730 (17%)]\tLoss: 2.543978\n",
      "557596\tTrain Epoch: 30 [2400/9730 (33%)]\tLoss: 2.484845\n",
      "557596\tTrain Epoch: 30 [3600/9730 (50%)]\tLoss: 2.524795\n",
      "557596\tTrain Epoch: 30 [4800/9730 (67%)]\tLoss: 2.537611\n",
      "557596\tTrain Epoch: 30 [6000/9730 (83%)]\tLoss: 2.540281\n",
      "557596\tTrain Epoch: 31 [0/9730 (0%)]\tLoss: 2.531179\n",
      "557596\tTrain Epoch: 31 [1200/9730 (17%)]\tLoss: 2.491179\n",
      "557596\tTrain Epoch: 31 [2400/9730 (33%)]\tLoss: 2.510140\n",
      "557596\tTrain Epoch: 31 [3600/9730 (50%)]\tLoss: 2.522302\n",
      "557596\tTrain Epoch: 31 [4800/9730 (67%)]\tLoss: 2.565305\n",
      "557596\tTrain Epoch: 31 [6000/9730 (83%)]\tLoss: 2.494255\n",
      "557596\tTrain Epoch: 32 [0/9730 (0%)]\tLoss: 2.517449\n",
      "557596\tTrain Epoch: 32 [1200/9730 (17%)]\tLoss: 2.549672\n",
      "557596\tTrain Epoch: 32 [2400/9730 (33%)]\tLoss: 2.516528\n",
      "557596\tTrain Epoch: 32 [3600/9730 (50%)]\tLoss: 2.565152\n",
      "557596\tTrain Epoch: 32 [4800/9730 (67%)]\tLoss: 2.506187\n",
      "557596\tTrain Epoch: 32 [6000/9730 (83%)]\tLoss: 2.502883\n",
      "557596\tTrain Epoch: 33 [0/9730 (0%)]\tLoss: 2.559006\n",
      "557596\tTrain Epoch: 33 [1200/9730 (17%)]\tLoss: 2.507176\n",
      "557596\tTrain Epoch: 33 [2400/9730 (33%)]\tLoss: 2.544274\n",
      "557596\tTrain Epoch: 33 [3600/9730 (50%)]\tLoss: 2.525638\n",
      "557596\tTrain Epoch: 33 [4800/9730 (67%)]\tLoss: 2.484054\n",
      "557596\tTrain Epoch: 33 [6000/9730 (83%)]\tLoss: 2.510938\n",
      "557596\tTrain Epoch: 34 [0/9730 (0%)]\tLoss: 2.558923\n",
      "557596\tTrain Epoch: 34 [1200/9730 (17%)]\tLoss: 2.516892\n",
      "557596\tTrain Epoch: 34 [2400/9730 (33%)]\tLoss: 2.509488\n",
      "557596\tTrain Epoch: 34 [3600/9730 (50%)]\tLoss: 2.535261\n",
      "557596\tTrain Epoch: 34 [4800/9730 (67%)]\tLoss: 2.531837\n",
      "557596\tTrain Epoch: 34 [6000/9730 (83%)]\tLoss: 2.504478\n",
      "557596\tTrain Epoch: 35 [0/9730 (0%)]\tLoss: 2.533151\n",
      "557596\tTrain Epoch: 35 [1200/9730 (17%)]\tLoss: 2.493228\n",
      "557596\tTrain Epoch: 35 [2400/9730 (33%)]\tLoss: 2.523450\n",
      "557596\tTrain Epoch: 35 [3600/9730 (50%)]\tLoss: 2.538501\n",
      "557596\tTrain Epoch: 35 [4800/9730 (67%)]\tLoss: 2.517592\n",
      "557596\tTrain Epoch: 35 [6000/9730 (83%)]\tLoss: 2.494409\n",
      "557596\tTrain Epoch: 36 [0/9730 (0%)]\tLoss: 2.534951\n",
      "557596\tTrain Epoch: 36 [1200/9730 (17%)]\tLoss: 2.501858\n",
      "557596\tTrain Epoch: 36 [2400/9730 (33%)]\tLoss: 2.463849\n",
      "557596\tTrain Epoch: 36 [3600/9730 (50%)]\tLoss: 2.523860\n",
      "557596\tTrain Epoch: 36 [4800/9730 (67%)]\tLoss: 2.514893\n",
      "557596\tTrain Epoch: 36 [6000/9730 (83%)]\tLoss: 2.541999\n",
      "557596\tTrain Epoch: 37 [0/9730 (0%)]\tLoss: 2.511339\n",
      "557596\tTrain Epoch: 37 [1200/9730 (17%)]\tLoss: 2.462065\n",
      "557596\tTrain Epoch: 37 [2400/9730 (33%)]\tLoss: 2.529126\n",
      "557596\tTrain Epoch: 37 [3600/9730 (50%)]\tLoss: 2.503681\n",
      "557596\tTrain Epoch: 37 [4800/9730 (67%)]\tLoss: 2.538960\n",
      "557596\tTrain Epoch: 37 [6000/9730 (83%)]\tLoss: 2.556414\n",
      "557596\tTrain Epoch: 38 [0/9730 (0%)]\tLoss: 2.518733\n",
      "557596\tTrain Epoch: 38 [1200/9730 (17%)]\tLoss: 2.540970\n",
      "557596\tTrain Epoch: 38 [2400/9730 (33%)]\tLoss: 2.550027\n",
      "557596\tTrain Epoch: 38 [3600/9730 (50%)]\tLoss: 2.558410\n",
      "557596\tTrain Epoch: 38 [4800/9730 (67%)]\tLoss: 2.545097\n",
      "557596\tTrain Epoch: 38 [6000/9730 (83%)]\tLoss: 2.472853\n",
      "557596\tTrain Epoch: 39 [0/9730 (0%)]\tLoss: 2.498619\n",
      "557596\tTrain Epoch: 39 [1200/9730 (17%)]\tLoss: 2.506121\n",
      "557596\tTrain Epoch: 39 [2400/9730 (33%)]\tLoss: 2.532048\n",
      "557596\tTrain Epoch: 39 [3600/9730 (50%)]\tLoss: 2.487077\n",
      "557596\tTrain Epoch: 39 [4800/9730 (67%)]\tLoss: 2.546310\n",
      "557596\tTrain Epoch: 39 [6000/9730 (83%)]\tLoss: 2.524113\n",
      "557596\tTrain Epoch: 40 [0/9730 (0%)]\tLoss: 2.503129\n",
      "557596\tTrain Epoch: 40 [1200/9730 (17%)]\tLoss: 2.481778\n",
      "557596\tTrain Epoch: 40 [2400/9730 (33%)]\tLoss: 2.533292\n",
      "557596\tTrain Epoch: 40 [3600/9730 (50%)]\tLoss: 2.514812\n",
      "557596\tTrain Epoch: 40 [4800/9730 (67%)]\tLoss: 2.453429\n",
      "557596\tTrain Epoch: 40 [6000/9730 (83%)]\tLoss: 2.483711\n",
      "557596\tTrain Epoch: 41 [0/9730 (0%)]\tLoss: 2.498970\n",
      "557596\tTrain Epoch: 41 [1200/9730 (17%)]\tLoss: 2.505962\n",
      "557596\tTrain Epoch: 41 [2400/9730 (33%)]\tLoss: 2.483103\n",
      "557596\tTrain Epoch: 41 [3600/9730 (50%)]\tLoss: 2.563552\n",
      "557596\tTrain Epoch: 41 [4800/9730 (67%)]\tLoss: 2.516123\n",
      "557596\tTrain Epoch: 41 [6000/9730 (83%)]\tLoss: 2.489175\n",
      "557596\tTrain Epoch: 42 [0/9730 (0%)]\tLoss: 2.499430\n",
      "557596\tTrain Epoch: 42 [1200/9730 (17%)]\tLoss: 2.505882\n",
      "557596\tTrain Epoch: 42 [2400/9730 (33%)]\tLoss: 2.533540\n",
      "557596\tTrain Epoch: 42 [3600/9730 (50%)]\tLoss: 2.477489\n",
      "557596\tTrain Epoch: 42 [4800/9730 (67%)]\tLoss: 2.503505\n",
      "557596\tTrain Epoch: 42 [6000/9730 (83%)]\tLoss: 2.533106\n",
      "557596\tTrain Epoch: 43 [0/9730 (0%)]\tLoss: 2.526462\n",
      "557596\tTrain Epoch: 43 [1200/9730 (17%)]\tLoss: 2.510137\n",
      "557596\tTrain Epoch: 43 [2400/9730 (33%)]\tLoss: 2.535553\n",
      "557596\tTrain Epoch: 43 [3600/9730 (50%)]\tLoss: 2.489218\n",
      "557596\tTrain Epoch: 43 [4800/9730 (67%)]\tLoss: 2.493777\n",
      "557596\tTrain Epoch: 43 [6000/9730 (83%)]\tLoss: 2.479606\n",
      "557596\tTrain Epoch: 44 [0/9730 (0%)]\tLoss: 2.471075\n",
      "557596\tTrain Epoch: 44 [1200/9730 (17%)]\tLoss: 2.542081\n",
      "557596\tTrain Epoch: 44 [2400/9730 (33%)]\tLoss: 2.494448\n",
      "557596\tTrain Epoch: 44 [3600/9730 (50%)]\tLoss: 2.470497\n",
      "557596\tTrain Epoch: 44 [4800/9730 (67%)]\tLoss: 2.476112\n",
      "557596\tTrain Epoch: 44 [6000/9730 (83%)]\tLoss: 2.535185\n",
      "557596\tTrain Epoch: 45 [0/9730 (0%)]\tLoss: 2.506509\n",
      "557596\tTrain Epoch: 45 [1200/9730 (17%)]\tLoss: 2.507778\n",
      "557596\tTrain Epoch: 45 [2400/9730 (33%)]\tLoss: 2.541662\n",
      "557596\tTrain Epoch: 45 [3600/9730 (50%)]\tLoss: 2.544873\n",
      "557596\tTrain Epoch: 45 [4800/9730 (67%)]\tLoss: 2.478987\n",
      "557596\tTrain Epoch: 45 [6000/9730 (83%)]\tLoss: 2.530958\n",
      "557596\tTrain Epoch: 46 [0/9730 (0%)]\tLoss: 2.528557\n",
      "557596\tTrain Epoch: 46 [1200/9730 (17%)]\tLoss: 2.460476\n",
      "557596\tTrain Epoch: 46 [2400/9730 (33%)]\tLoss: 2.552788\n",
      "557596\tTrain Epoch: 46 [3600/9730 (50%)]\tLoss: 2.502988\n",
      "557596\tTrain Epoch: 46 [4800/9730 (67%)]\tLoss: 2.533409\n",
      "557596\tTrain Epoch: 46 [6000/9730 (83%)]\tLoss: 2.491658\n",
      "557596\tTrain Epoch: 47 [0/9730 (0%)]\tLoss: 2.461582\n",
      "557596\tTrain Epoch: 47 [1200/9730 (17%)]\tLoss: 2.527478\n",
      "557596\tTrain Epoch: 47 [2400/9730 (33%)]\tLoss: 2.525176\n",
      "557596\tTrain Epoch: 47 [3600/9730 (50%)]\tLoss: 2.476794\n",
      "557596\tTrain Epoch: 47 [4800/9730 (67%)]\tLoss: 2.511363\n",
      "557596\tTrain Epoch: 47 [6000/9730 (83%)]\tLoss: 2.533903\n",
      "557596\tTrain Epoch: 48 [0/9730 (0%)]\tLoss: 2.455980\n",
      "557596\tTrain Epoch: 48 [1200/9730 (17%)]\tLoss: 2.482628\n",
      "557596\tTrain Epoch: 48 [2400/9730 (33%)]\tLoss: 2.526722\n",
      "557596\tTrain Epoch: 48 [3600/9730 (50%)]\tLoss: 2.536201\n",
      "557596\tTrain Epoch: 48 [4800/9730 (67%)]\tLoss: 2.466433\n",
      "557596\tTrain Epoch: 48 [6000/9730 (83%)]\tLoss: 2.479440\n",
      "557596\tTrain Epoch: 49 [0/9730 (0%)]\tLoss: 2.513436\n",
      "557596\tTrain Epoch: 49 [1200/9730 (17%)]\tLoss: 2.475402\n",
      "557596\tTrain Epoch: 49 [2400/9730 (33%)]\tLoss: 2.523305\n",
      "557596\tTrain Epoch: 49 [3600/9730 (50%)]\tLoss: 2.542714\n",
      "557596\tTrain Epoch: 49 [4800/9730 (67%)]\tLoss: 2.552450\n",
      "557596\tTrain Epoch: 49 [6000/9730 (83%)]\tLoss: 2.519331\n",
      "557596\tTrain Epoch: 50 [0/9730 (0%)]\tLoss: 2.535526\n",
      "557596\tTrain Epoch: 50 [1200/9730 (17%)]\tLoss: 2.506382\n",
      "557596\tTrain Epoch: 50 [2400/9730 (33%)]\tLoss: 2.483197\n",
      "557596\tTrain Epoch: 50 [3600/9730 (50%)]\tLoss: 2.505351\n",
      "557596\tTrain Epoch: 50 [4800/9730 (67%)]\tLoss: 2.504158\n",
      "557596\tTrain Epoch: 50 [6000/9730 (83%)]\tLoss: 2.496789\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "test(model=willy, device=device, test_loaderIn=train_loader)\n",
    "test(model=willy, device=device, test_loaderIn=valid_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 2.0831, Accuracy: 928/9730 (10%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3488, Accuracy: 137/9730 (1%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 50, \"log_interval\": 3}\n",
    "device = torch.device('cuda:0')\n",
    "willy.to(device)\n",
    "willy.share_memory()\n",
    "test(model=willy, device=device, test_loaderIn=train_loader)\n",
    "test(model=willy, device=device, test_loaderIn=valid_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 2.1439, Accuracy: 892/9730 (9%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3570, Accuracy: 141/9730 (1%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train(model=willy, device=device, train_loaderIn=train_loader, args=args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "43922\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: 3.439215\n",
      "43922\tTrain Epoch: 1 [1200/9730 (17%)]\tLoss: 3.115792\n",
      "43922\tTrain Epoch: 1 [2400/9730 (33%)]\tLoss: 2.923263\n",
      "43922\tTrain Epoch: 1 [3600/9730 (50%)]\tLoss: 2.908574\n",
      "43922\tTrain Epoch: 1 [4800/9730 (67%)]\tLoss: 2.867324\n",
      "43922\tTrain Epoch: 1 [6000/9730 (83%)]\tLoss: 2.890974\n",
      "43922\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: 2.786769\n",
      "43922\tTrain Epoch: 2 [1200/9730 (17%)]\tLoss: 2.764891\n",
      "43922\tTrain Epoch: 2 [2400/9730 (33%)]\tLoss: 2.757283\n",
      "43922\tTrain Epoch: 2 [3600/9730 (50%)]\tLoss: 2.771966\n",
      "43922\tTrain Epoch: 2 [4800/9730 (67%)]\tLoss: 2.726995\n",
      "43922\tTrain Epoch: 2 [6000/9730 (83%)]\tLoss: 2.715253\n",
      "43922\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: 2.712765\n",
      "43922\tTrain Epoch: 3 [1200/9730 (17%)]\tLoss: 2.708520\n",
      "43922\tTrain Epoch: 3 [2400/9730 (33%)]\tLoss: 2.651708\n",
      "43922\tTrain Epoch: 3 [3600/9730 (50%)]\tLoss: 2.665691\n",
      "43922\tTrain Epoch: 3 [4800/9730 (67%)]\tLoss: 2.740607\n",
      "43922\tTrain Epoch: 3 [6000/9730 (83%)]\tLoss: 2.683159\n",
      "43922\tTrain Epoch: 4 [0/9730 (0%)]\tLoss: 2.711383\n",
      "43922\tTrain Epoch: 4 [1200/9730 (17%)]\tLoss: 2.743706\n",
      "43922\tTrain Epoch: 4 [2400/9730 (33%)]\tLoss: 2.669257\n",
      "43922\tTrain Epoch: 4 [3600/9730 (50%)]\tLoss: 2.704848\n",
      "43922\tTrain Epoch: 4 [4800/9730 (67%)]\tLoss: 2.654617\n",
      "43922\tTrain Epoch: 4 [6000/9730 (83%)]\tLoss: 2.686412\n",
      "43922\tTrain Epoch: 5 [0/9730 (0%)]\tLoss: 2.670880\n",
      "43922\tTrain Epoch: 5 [1200/9730 (17%)]\tLoss: 2.653893\n",
      "43922\tTrain Epoch: 5 [2400/9730 (33%)]\tLoss: 2.616971\n",
      "43922\tTrain Epoch: 5 [3600/9730 (50%)]\tLoss: 2.716483\n",
      "43922\tTrain Epoch: 5 [4800/9730 (67%)]\tLoss: 2.652611\n",
      "43922\tTrain Epoch: 5 [6000/9730 (83%)]\tLoss: 2.663497\n",
      "43922\tTrain Epoch: 6 [0/9730 (0%)]\tLoss: 2.692957\n",
      "43922\tTrain Epoch: 6 [1200/9730 (17%)]\tLoss: 2.639269\n",
      "43922\tTrain Epoch: 6 [2400/9730 (33%)]\tLoss: 2.669237\n",
      "43922\tTrain Epoch: 6 [3600/9730 (50%)]\tLoss: 2.674006\n",
      "43922\tTrain Epoch: 6 [4800/9730 (67%)]\tLoss: 2.703753\n",
      "43922\tTrain Epoch: 6 [6000/9730 (83%)]\tLoss: 2.646561\n",
      "43922\tTrain Epoch: 7 [0/9730 (0%)]\tLoss: 2.632425\n",
      "43922\tTrain Epoch: 7 [1200/9730 (17%)]\tLoss: 2.662806\n",
      "43922\tTrain Epoch: 7 [2400/9730 (33%)]\tLoss: 2.713668\n",
      "43922\tTrain Epoch: 7 [3600/9730 (50%)]\tLoss: 2.586321\n",
      "43922\tTrain Epoch: 7 [4800/9730 (67%)]\tLoss: 2.639169\n",
      "43922\tTrain Epoch: 7 [6000/9730 (83%)]\tLoss: 2.621709\n",
      "43922\tTrain Epoch: 8 [0/9730 (0%)]\tLoss: 2.645134\n",
      "43922\tTrain Epoch: 8 [1200/9730 (17%)]\tLoss: 2.618732\n",
      "43922\tTrain Epoch: 8 [2400/9730 (33%)]\tLoss: 2.599112\n",
      "43922\tTrain Epoch: 8 [3600/9730 (50%)]\tLoss: 2.563246\n",
      "43922\tTrain Epoch: 8 [4800/9730 (67%)]\tLoss: 2.638230\n",
      "43922\tTrain Epoch: 8 [6000/9730 (83%)]\tLoss: 2.565084\n",
      "43922\tTrain Epoch: 9 [0/9730 (0%)]\tLoss: 2.591830\n",
      "43922\tTrain Epoch: 9 [1200/9730 (17%)]\tLoss: 2.620014\n",
      "43922\tTrain Epoch: 9 [2400/9730 (33%)]\tLoss: 2.617481\n",
      "43922\tTrain Epoch: 9 [3600/9730 (50%)]\tLoss: 2.624851\n",
      "43922\tTrain Epoch: 9 [4800/9730 (67%)]\tLoss: 2.558592\n",
      "43922\tTrain Epoch: 9 [6000/9730 (83%)]\tLoss: 2.538103\n",
      "43922\tTrain Epoch: 10 [0/9730 (0%)]\tLoss: 2.582056\n",
      "43922\tTrain Epoch: 10 [1200/9730 (17%)]\tLoss: 2.591571\n",
      "43922\tTrain Epoch: 10 [2400/9730 (33%)]\tLoss: 2.551038\n",
      "43922\tTrain Epoch: 10 [3600/9730 (50%)]\tLoss: 2.556323\n",
      "43922\tTrain Epoch: 10 [4800/9730 (67%)]\tLoss: 2.498103\n",
      "43922\tTrain Epoch: 10 [6000/9730 (83%)]\tLoss: 2.543408\n",
      "43922\tTrain Epoch: 11 [0/9730 (0%)]\tLoss: 2.546594\n",
      "43922\tTrain Epoch: 11 [1200/9730 (17%)]\tLoss: 2.591858\n",
      "43922\tTrain Epoch: 11 [2400/9730 (33%)]\tLoss: 2.521473\n",
      "43922\tTrain Epoch: 11 [3600/9730 (50%)]\tLoss: 2.581537\n",
      "43922\tTrain Epoch: 11 [4800/9730 (67%)]\tLoss: 2.619099\n",
      "43922\tTrain Epoch: 11 [6000/9730 (83%)]\tLoss: 2.564912\n",
      "43922\tTrain Epoch: 12 [0/9730 (0%)]\tLoss: 2.520996\n",
      "43922\tTrain Epoch: 12 [1200/9730 (17%)]\tLoss: 2.542170\n",
      "43922\tTrain Epoch: 12 [2400/9730 (33%)]\tLoss: 2.517378\n",
      "43922\tTrain Epoch: 12 [3600/9730 (50%)]\tLoss: 2.561939\n",
      "43922\tTrain Epoch: 12 [4800/9730 (67%)]\tLoss: 2.636827\n",
      "43922\tTrain Epoch: 12 [6000/9730 (83%)]\tLoss: 2.538638\n",
      "43922\tTrain Epoch: 13 [0/9730 (0%)]\tLoss: 2.518656\n",
      "43922\tTrain Epoch: 13 [1200/9730 (17%)]\tLoss: 2.552708\n",
      "43922\tTrain Epoch: 13 [2400/9730 (33%)]\tLoss: 2.529625\n",
      "43922\tTrain Epoch: 13 [3600/9730 (50%)]\tLoss: 2.579762\n",
      "43922\tTrain Epoch: 13 [4800/9730 (67%)]\tLoss: 2.544401\n",
      "43922\tTrain Epoch: 13 [6000/9730 (83%)]\tLoss: 2.479040\n",
      "43922\tTrain Epoch: 14 [0/9730 (0%)]\tLoss: 2.541230\n",
      "43922\tTrain Epoch: 14 [1200/9730 (17%)]\tLoss: 2.539821\n",
      "43922\tTrain Epoch: 14 [2400/9730 (33%)]\tLoss: 2.565910\n",
      "43922\tTrain Epoch: 14 [3600/9730 (50%)]\tLoss: 2.545522\n",
      "43922\tTrain Epoch: 14 [4800/9730 (67%)]\tLoss: 2.551332\n",
      "43922\tTrain Epoch: 14 [6000/9730 (83%)]\tLoss: 2.531718\n",
      "43922\tTrain Epoch: 15 [0/9730 (0%)]\tLoss: 2.515519\n",
      "43922\tTrain Epoch: 15 [1200/9730 (17%)]\tLoss: 2.526327\n",
      "43922\tTrain Epoch: 15 [2400/9730 (33%)]\tLoss: 2.418223\n",
      "43922\tTrain Epoch: 15 [3600/9730 (50%)]\tLoss: 2.529099\n",
      "43922\tTrain Epoch: 15 [4800/9730 (67%)]\tLoss: 2.567265\n",
      "43922\tTrain Epoch: 15 [6000/9730 (83%)]\tLoss: 2.564447\n",
      "43922\tTrain Epoch: 16 [0/9730 (0%)]\tLoss: 2.573212\n",
      "43922\tTrain Epoch: 16 [1200/9730 (17%)]\tLoss: 2.515651\n",
      "43922\tTrain Epoch: 16 [2400/9730 (33%)]\tLoss: 2.453883\n",
      "43922\tTrain Epoch: 16 [3600/9730 (50%)]\tLoss: 2.419137\n",
      "43922\tTrain Epoch: 16 [4800/9730 (67%)]\tLoss: 2.518730\n",
      "43922\tTrain Epoch: 16 [6000/9730 (83%)]\tLoss: 2.448977\n",
      "43922\tTrain Epoch: 17 [0/9730 (0%)]\tLoss: 2.519446\n",
      "43922\tTrain Epoch: 17 [1200/9730 (17%)]\tLoss: 2.508451\n",
      "43922\tTrain Epoch: 17 [2400/9730 (33%)]\tLoss: 2.488892\n",
      "43922\tTrain Epoch: 17 [3600/9730 (50%)]\tLoss: 2.508424\n",
      "43922\tTrain Epoch: 17 [4800/9730 (67%)]\tLoss: 2.524230\n",
      "43922\tTrain Epoch: 17 [6000/9730 (83%)]\tLoss: 2.529200\n",
      "43922\tTrain Epoch: 18 [0/9730 (0%)]\tLoss: 2.444454\n",
      "43922\tTrain Epoch: 18 [1200/9730 (17%)]\tLoss: 2.486483\n",
      "43922\tTrain Epoch: 18 [2400/9730 (33%)]\tLoss: 2.471196\n",
      "43922\tTrain Epoch: 18 [3600/9730 (50%)]\tLoss: 2.488403\n",
      "43922\tTrain Epoch: 18 [4800/9730 (67%)]\tLoss: 2.433004\n",
      "43922\tTrain Epoch: 18 [6000/9730 (83%)]\tLoss: 2.410054\n",
      "43922\tTrain Epoch: 19 [0/9730 (0%)]\tLoss: 2.460467\n",
      "43922\tTrain Epoch: 19 [1200/9730 (17%)]\tLoss: 2.464796\n",
      "43922\tTrain Epoch: 19 [2400/9730 (33%)]\tLoss: 2.521452\n",
      "43922\tTrain Epoch: 19 [3600/9730 (50%)]\tLoss: 2.520238\n",
      "43922\tTrain Epoch: 19 [4800/9730 (67%)]\tLoss: 2.503243\n",
      "43922\tTrain Epoch: 19 [6000/9730 (83%)]\tLoss: 2.551322\n",
      "43922\tTrain Epoch: 20 [0/9730 (0%)]\tLoss: 2.420793\n",
      "43922\tTrain Epoch: 20 [1200/9730 (17%)]\tLoss: 2.514672\n",
      "43922\tTrain Epoch: 20 [2400/9730 (33%)]\tLoss: 2.512776\n",
      "43922\tTrain Epoch: 20 [3600/9730 (50%)]\tLoss: 2.445141\n",
      "43922\tTrain Epoch: 20 [4800/9730 (67%)]\tLoss: 2.426959\n",
      "43922\tTrain Epoch: 20 [6000/9730 (83%)]\tLoss: 2.367150\n",
      "43922\tTrain Epoch: 21 [0/9730 (0%)]\tLoss: 2.465771\n",
      "43922\tTrain Epoch: 21 [1200/9730 (17%)]\tLoss: 2.499573\n",
      "43922\tTrain Epoch: 21 [2400/9730 (33%)]\tLoss: 2.499431\n",
      "43922\tTrain Epoch: 21 [3600/9730 (50%)]\tLoss: 2.485225\n",
      "43922\tTrain Epoch: 21 [4800/9730 (67%)]\tLoss: 2.464286\n",
      "43922\tTrain Epoch: 21 [6000/9730 (83%)]\tLoss: 2.487304\n",
      "43922\tTrain Epoch: 22 [0/9730 (0%)]\tLoss: 2.385482\n",
      "43922\tTrain Epoch: 22 [1200/9730 (17%)]\tLoss: 2.459852\n",
      "43922\tTrain Epoch: 22 [2400/9730 (33%)]\tLoss: 2.432205\n",
      "43922\tTrain Epoch: 22 [3600/9730 (50%)]\tLoss: 2.449029\n",
      "43922\tTrain Epoch: 22 [4800/9730 (67%)]\tLoss: 2.460610\n",
      "43922\tTrain Epoch: 22 [6000/9730 (83%)]\tLoss: 2.442256\n",
      "43922\tTrain Epoch: 23 [0/9730 (0%)]\tLoss: 2.441365\n",
      "43922\tTrain Epoch: 23 [1200/9730 (17%)]\tLoss: 2.437983\n",
      "43922\tTrain Epoch: 23 [2400/9730 (33%)]\tLoss: 2.498283\n",
      "43922\tTrain Epoch: 23 [3600/9730 (50%)]\tLoss: 2.389471\n",
      "43922\tTrain Epoch: 23 [4800/9730 (67%)]\tLoss: 2.412897\n",
      "43922\tTrain Epoch: 23 [6000/9730 (83%)]\tLoss: 2.448201\n",
      "43922\tTrain Epoch: 24 [0/9730 (0%)]\tLoss: 2.445441\n",
      "43922\tTrain Epoch: 24 [1200/9730 (17%)]\tLoss: 2.403249\n",
      "43922\tTrain Epoch: 24 [2400/9730 (33%)]\tLoss: 2.417304\n",
      "43922\tTrain Epoch: 24 [3600/9730 (50%)]\tLoss: 2.488714\n",
      "43922\tTrain Epoch: 24 [4800/9730 (67%)]\tLoss: 2.354550\n",
      "43922\tTrain Epoch: 24 [6000/9730 (83%)]\tLoss: 2.454036\n",
      "43922\tTrain Epoch: 25 [0/9730 (0%)]\tLoss: 2.391760\n",
      "43922\tTrain Epoch: 25 [1200/9730 (17%)]\tLoss: 2.382252\n",
      "43922\tTrain Epoch: 25 [2400/9730 (33%)]\tLoss: 2.467117\n",
      "43922\tTrain Epoch: 25 [3600/9730 (50%)]\tLoss: 2.388880\n",
      "43922\tTrain Epoch: 25 [4800/9730 (67%)]\tLoss: 2.413873\n",
      "43922\tTrain Epoch: 25 [6000/9730 (83%)]\tLoss: 2.405993\n",
      "43922\tTrain Epoch: 26 [0/9730 (0%)]\tLoss: 2.369899\n",
      "43922\tTrain Epoch: 26 [1200/9730 (17%)]\tLoss: 2.463374\n",
      "43922\tTrain Epoch: 26 [2400/9730 (33%)]\tLoss: 2.418748\n",
      "43922\tTrain Epoch: 26 [3600/9730 (50%)]\tLoss: 2.390924\n",
      "43922\tTrain Epoch: 26 [4800/9730 (67%)]\tLoss: 2.421710\n",
      "43922\tTrain Epoch: 26 [6000/9730 (83%)]\tLoss: 2.416303\n",
      "43922\tTrain Epoch: 27 [0/9730 (0%)]\tLoss: 2.354789\n",
      "43922\tTrain Epoch: 27 [1200/9730 (17%)]\tLoss: 2.387969\n",
      "43922\tTrain Epoch: 27 [2400/9730 (33%)]\tLoss: 2.491115\n",
      "43922\tTrain Epoch: 27 [3600/9730 (50%)]\tLoss: 2.453525\n",
      "43922\tTrain Epoch: 27 [4800/9730 (67%)]\tLoss: 2.408439\n",
      "43922\tTrain Epoch: 27 [6000/9730 (83%)]\tLoss: 2.440084\n",
      "43922\tTrain Epoch: 28 [0/9730 (0%)]\tLoss: 2.344230\n",
      "43922\tTrain Epoch: 28 [1200/9730 (17%)]\tLoss: 2.393365\n",
      "43922\tTrain Epoch: 28 [2400/9730 (33%)]\tLoss: 2.382198\n",
      "43922\tTrain Epoch: 28 [3600/9730 (50%)]\tLoss: 2.403891\n",
      "43922\tTrain Epoch: 28 [4800/9730 (67%)]\tLoss: 2.384570\n",
      "43922\tTrain Epoch: 28 [6000/9730 (83%)]\tLoss: 2.443428\n",
      "43922\tTrain Epoch: 29 [0/9730 (0%)]\tLoss: 2.402187\n",
      "43922\tTrain Epoch: 29 [1200/9730 (17%)]\tLoss: 2.365374\n",
      "43922\tTrain Epoch: 29 [2400/9730 (33%)]\tLoss: 2.415235\n",
      "43922\tTrain Epoch: 29 [3600/9730 (50%)]\tLoss: 2.405585\n",
      "43922\tTrain Epoch: 29 [4800/9730 (67%)]\tLoss: 2.385805\n",
      "43922\tTrain Epoch: 29 [6000/9730 (83%)]\tLoss: 2.402746\n",
      "43922\tTrain Epoch: 30 [0/9730 (0%)]\tLoss: 2.339369\n",
      "43922\tTrain Epoch: 30 [1200/9730 (17%)]\tLoss: 2.408152\n",
      "43922\tTrain Epoch: 30 [2400/9730 (33%)]\tLoss: 2.399734\n",
      "43922\tTrain Epoch: 30 [3600/9730 (50%)]\tLoss: 2.371827\n",
      "43922\tTrain Epoch: 30 [4800/9730 (67%)]\tLoss: 2.366690\n",
      "43922\tTrain Epoch: 30 [6000/9730 (83%)]\tLoss: 2.337572\n",
      "43922\tTrain Epoch: 31 [0/9730 (0%)]\tLoss: 2.440392\n",
      "43922\tTrain Epoch: 31 [1200/9730 (17%)]\tLoss: 2.377511\n",
      "43922\tTrain Epoch: 31 [2400/9730 (33%)]\tLoss: 2.413731\n",
      "43922\tTrain Epoch: 31 [3600/9730 (50%)]\tLoss: 2.353409\n",
      "43922\tTrain Epoch: 31 [4800/9730 (67%)]\tLoss: 2.372761\n",
      "43922\tTrain Epoch: 31 [6000/9730 (83%)]\tLoss: 2.458560\n",
      "43922\tTrain Epoch: 32 [0/9730 (0%)]\tLoss: 2.371171\n",
      "43922\tTrain Epoch: 32 [1200/9730 (17%)]\tLoss: 2.342666\n",
      "43922\tTrain Epoch: 32 [2400/9730 (33%)]\tLoss: 2.368109\n",
      "43922\tTrain Epoch: 32 [3600/9730 (50%)]\tLoss: 2.378250\n",
      "43922\tTrain Epoch: 32 [4800/9730 (67%)]\tLoss: 2.419871\n",
      "43922\tTrain Epoch: 32 [6000/9730 (83%)]\tLoss: 2.374684\n",
      "43922\tTrain Epoch: 33 [0/9730 (0%)]\tLoss: 2.416672\n",
      "43922\tTrain Epoch: 33 [1200/9730 (17%)]\tLoss: 2.348097\n",
      "43922\tTrain Epoch: 33 [2400/9730 (33%)]\tLoss: 2.352073\n",
      "43922\tTrain Epoch: 33 [3600/9730 (50%)]\tLoss: 2.347976\n",
      "43922\tTrain Epoch: 33 [4800/9730 (67%)]\tLoss: 2.374337\n",
      "43922\tTrain Epoch: 33 [6000/9730 (83%)]\tLoss: 2.360322\n",
      "43922\tTrain Epoch: 34 [0/9730 (0%)]\tLoss: 2.449330\n",
      "43922\tTrain Epoch: 34 [1200/9730 (17%)]\tLoss: 2.347061\n",
      "43922\tTrain Epoch: 34 [2400/9730 (33%)]\tLoss: 2.405008\n",
      "43922\tTrain Epoch: 34 [3600/9730 (50%)]\tLoss: 2.415926\n",
      "43922\tTrain Epoch: 34 [4800/9730 (67%)]\tLoss: 2.394425\n",
      "43922\tTrain Epoch: 34 [6000/9730 (83%)]\tLoss: 2.420159\n",
      "43922\tTrain Epoch: 35 [0/9730 (0%)]\tLoss: 2.312422\n",
      "43922\tTrain Epoch: 35 [1200/9730 (17%)]\tLoss: 2.433369\n",
      "43922\tTrain Epoch: 35 [2400/9730 (33%)]\tLoss: 2.355265\n",
      "43922\tTrain Epoch: 35 [3600/9730 (50%)]\tLoss: 2.409548\n",
      "43922\tTrain Epoch: 35 [4800/9730 (67%)]\tLoss: 2.362109\n",
      "43922\tTrain Epoch: 35 [6000/9730 (83%)]\tLoss: 2.398440\n",
      "43922\tTrain Epoch: 36 [0/9730 (0%)]\tLoss: 2.438525\n",
      "43922\tTrain Epoch: 36 [1200/9730 (17%)]\tLoss: 2.383335\n",
      "43922\tTrain Epoch: 36 [2400/9730 (33%)]\tLoss: 2.372300\n",
      "43922\tTrain Epoch: 36 [3600/9730 (50%)]\tLoss: 2.388887\n",
      "43922\tTrain Epoch: 36 [4800/9730 (67%)]\tLoss: 2.371024\n",
      "43922\tTrain Epoch: 36 [6000/9730 (83%)]\tLoss: 2.336723\n",
      "43922\tTrain Epoch: 37 [0/9730 (0%)]\tLoss: 2.426906\n",
      "43922\tTrain Epoch: 37 [1200/9730 (17%)]\tLoss: 2.337305\n",
      "43922\tTrain Epoch: 37 [2400/9730 (33%)]\tLoss: 2.290949\n",
      "43922\tTrain Epoch: 37 [3600/9730 (50%)]\tLoss: 2.382052\n",
      "43922\tTrain Epoch: 37 [4800/9730 (67%)]\tLoss: 2.368330\n",
      "43922\tTrain Epoch: 37 [6000/9730 (83%)]\tLoss: 2.412643\n",
      "43922\tTrain Epoch: 38 [0/9730 (0%)]\tLoss: 2.370894\n",
      "43922\tTrain Epoch: 38 [1200/9730 (17%)]\tLoss: 2.349619\n",
      "43922\tTrain Epoch: 38 [2400/9730 (33%)]\tLoss: 2.368741\n",
      "43922\tTrain Epoch: 38 [3600/9730 (50%)]\tLoss: 2.340341\n",
      "43922\tTrain Epoch: 38 [4800/9730 (67%)]\tLoss: 2.332061\n",
      "43922\tTrain Epoch: 38 [6000/9730 (83%)]\tLoss: 2.416883\n",
      "43922\tTrain Epoch: 39 [0/9730 (0%)]\tLoss: 2.367855\n",
      "43922\tTrain Epoch: 39 [1200/9730 (17%)]\tLoss: 2.351131\n",
      "43922\tTrain Epoch: 39 [2400/9730 (33%)]\tLoss: 2.373882\n",
      "43922\tTrain Epoch: 39 [3600/9730 (50%)]\tLoss: 2.426536\n",
      "43922\tTrain Epoch: 39 [4800/9730 (67%)]\tLoss: 2.272683\n",
      "43922\tTrain Epoch: 39 [6000/9730 (83%)]\tLoss: 2.372925\n",
      "43922\tTrain Epoch: 40 [0/9730 (0%)]\tLoss: 2.420393\n",
      "43922\tTrain Epoch: 40 [1200/9730 (17%)]\tLoss: 2.384484\n",
      "43922\tTrain Epoch: 40 [2400/9730 (33%)]\tLoss: 2.289283\n",
      "43922\tTrain Epoch: 40 [3600/9730 (50%)]\tLoss: 2.342947\n",
      "43922\tTrain Epoch: 40 [4800/9730 (67%)]\tLoss: 2.479216\n",
      "43922\tTrain Epoch: 40 [6000/9730 (83%)]\tLoss: 2.307228\n",
      "43922\tTrain Epoch: 41 [0/9730 (0%)]\tLoss: 2.367470\n",
      "43922\tTrain Epoch: 41 [1200/9730 (17%)]\tLoss: 2.346891\n",
      "43922\tTrain Epoch: 41 [2400/9730 (33%)]\tLoss: 2.290004\n",
      "43922\tTrain Epoch: 41 [3600/9730 (50%)]\tLoss: 2.301050\n",
      "43922\tTrain Epoch: 41 [4800/9730 (67%)]\tLoss: 2.354135\n",
      "43922\tTrain Epoch: 41 [6000/9730 (83%)]\tLoss: 2.349544\n",
      "43922\tTrain Epoch: 42 [0/9730 (0%)]\tLoss: 2.356309\n",
      "43922\tTrain Epoch: 42 [1200/9730 (17%)]\tLoss: 2.304897\n",
      "43922\tTrain Epoch: 42 [2400/9730 (33%)]\tLoss: 2.301705\n",
      "43922\tTrain Epoch: 42 [3600/9730 (50%)]\tLoss: 2.347147\n",
      "43922\tTrain Epoch: 42 [4800/9730 (67%)]\tLoss: 2.288555\n",
      "43922\tTrain Epoch: 42 [6000/9730 (83%)]\tLoss: 2.313463\n",
      "43922\tTrain Epoch: 43 [0/9730 (0%)]\tLoss: 2.272784\n",
      "43922\tTrain Epoch: 43 [1200/9730 (17%)]\tLoss: 2.235563\n",
      "43922\tTrain Epoch: 43 [2400/9730 (33%)]\tLoss: 2.313386\n",
      "43922\tTrain Epoch: 43 [3600/9730 (50%)]\tLoss: 2.308681\n",
      "43922\tTrain Epoch: 43 [4800/9730 (67%)]\tLoss: 2.310099\n",
      "43922\tTrain Epoch: 43 [6000/9730 (83%)]\tLoss: 2.331183\n",
      "43922\tTrain Epoch: 44 [0/9730 (0%)]\tLoss: 2.268473\n",
      "43922\tTrain Epoch: 44 [1200/9730 (17%)]\tLoss: 2.448713\n",
      "43922\tTrain Epoch: 44 [2400/9730 (33%)]\tLoss: 2.271920\n",
      "43922\tTrain Epoch: 44 [3600/9730 (50%)]\tLoss: 2.281841\n",
      "43922\tTrain Epoch: 44 [4800/9730 (67%)]\tLoss: 2.292195\n",
      "43922\tTrain Epoch: 44 [6000/9730 (83%)]\tLoss: 2.359268\n",
      "43922\tTrain Epoch: 45 [0/9730 (0%)]\tLoss: 2.348664\n",
      "43922\tTrain Epoch: 45 [1200/9730 (17%)]\tLoss: 2.301764\n",
      "43922\tTrain Epoch: 45 [2400/9730 (33%)]\tLoss: 2.312929\n",
      "43922\tTrain Epoch: 45 [3600/9730 (50%)]\tLoss: 2.262315\n",
      "43922\tTrain Epoch: 45 [4800/9730 (67%)]\tLoss: 2.402060\n",
      "43922\tTrain Epoch: 45 [6000/9730 (83%)]\tLoss: 2.321027\n",
      "43922\tTrain Epoch: 46 [0/9730 (0%)]\tLoss: 2.358294\n",
      "43922\tTrain Epoch: 46 [1200/9730 (17%)]\tLoss: 2.266668\n",
      "43922\tTrain Epoch: 46 [2400/9730 (33%)]\tLoss: 2.329654\n",
      "43922\tTrain Epoch: 46 [3600/9730 (50%)]\tLoss: 2.340257\n",
      "43922\tTrain Epoch: 46 [4800/9730 (67%)]\tLoss: 2.366373\n",
      "43922\tTrain Epoch: 46 [6000/9730 (83%)]\tLoss: 2.306969\n",
      "43922\tTrain Epoch: 47 [0/9730 (0%)]\tLoss: 2.284672\n",
      "43922\tTrain Epoch: 47 [1200/9730 (17%)]\tLoss: 2.294043\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test(model=willy, device=device, test_loaderIn=train_loader)\n",
    "test(model=willy, device=device, test_loaderIn=valid_loader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from MileStone_3_Create_Neural_Network.WillyAlexNet import MyAlexNet\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "willyAlexNet = MyAlexNet()\n",
    "\n",
    "# for param in willyAlexNet.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in willyAlexNet.WillyAlexNet.classifier[-1].parameters():\n",
    "    # param.requires_grad = True\n",
    "summary(willyAlexNet, input_size=(batch_size,3,198,198))\n",
    "#print(willyAlexNet)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyAlexNet                                --                        --\n",
       "├─AlexNet: 1-1                           [400, 18]                 --\n",
       "│    └─Sequential: 2-1                   [400, 256, 5, 5]          --\n",
       "│    │    └─Conv2d: 3-1                  [400, 64, 48, 48]         23,296\n",
       "│    │    └─ReLU: 3-2                    [400, 64, 48, 48]         --\n",
       "│    │    └─MaxPool2d: 3-3               [400, 64, 23, 23]         --\n",
       "│    │    └─Conv2d: 3-4                  [400, 192, 23, 23]        307,392\n",
       "│    │    └─ReLU: 3-5                    [400, 192, 23, 23]        --\n",
       "│    │    └─MaxPool2d: 3-6               [400, 192, 11, 11]        --\n",
       "│    │    └─Conv2d: 3-7                  [400, 384, 11, 11]        663,936\n",
       "│    │    └─ReLU: 3-8                    [400, 384, 11, 11]        --\n",
       "│    │    └─Conv2d: 3-9                  [400, 256, 11, 11]        884,992\n",
       "│    │    └─ReLU: 3-10                   [400, 256, 11, 11]        --\n",
       "│    │    └─Conv2d: 3-11                 [400, 256, 11, 11]        590,080\n",
       "│    │    └─ReLU: 3-12                   [400, 256, 11, 11]        --\n",
       "│    │    └─MaxPool2d: 3-13              [400, 256, 5, 5]          --\n",
       "│    └─AdaptiveAvgPool2d: 2-2            [400, 256, 6, 6]          --\n",
       "│    └─Sequential: 2-3                   [400, 18]                 --\n",
       "│    │    └─Dropout: 3-14                [400, 9216]               --\n",
       "│    │    └─Linear: 3-15                 [400, 4096]               37,752,832\n",
       "│    │    └─ReLU: 3-16                   [400, 4096]               --\n",
       "│    │    └─Dropout: 3-17                [400, 4096]               --\n",
       "│    │    └─Linear: 3-18                 [400, 4096]               16,781,312\n",
       "│    │    └─ReLU: 3-19                   [400, 4096]               --\n",
       "│    │    └─Linear: 3-20                 [400, 18]                 73,746\n",
       "==========================================================================================\n",
       "Total params: 57,077,586\n",
       "Trainable params: 57,077,586\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 211.88\n",
       "==========================================================================================\n",
       "Input size (MB): 188.18\n",
       "Forward/backward pass size (MB): 1170.08\n",
       "Params size (MB): 228.31\n",
       "Estimated Total Size (MB): 1586.57\n",
       "=========================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "summary(willyAlexNet)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyAlexNet                                --\n",
       "├─AlexNet: 1-1                           --\n",
       "│    └─Sequential: 2-1                   --\n",
       "│    │    └─Conv2d: 3-1                  23,296\n",
       "│    │    └─ReLU: 3-2                    --\n",
       "│    │    └─MaxPool2d: 3-3               --\n",
       "│    │    └─Conv2d: 3-4                  307,392\n",
       "│    │    └─ReLU: 3-5                    --\n",
       "│    │    └─MaxPool2d: 3-6               --\n",
       "│    │    └─Conv2d: 3-7                  663,936\n",
       "│    │    └─ReLU: 3-8                    --\n",
       "│    │    └─Conv2d: 3-9                  884,992\n",
       "│    │    └─ReLU: 3-10                   --\n",
       "│    │    └─Conv2d: 3-11                 590,080\n",
       "│    │    └─ReLU: 3-12                   --\n",
       "│    │    └─MaxPool2d: 3-13              --\n",
       "│    └─AdaptiveAvgPool2d: 2-2            --\n",
       "│    └─Sequential: 2-3                   --\n",
       "│    │    └─Dropout: 3-14                --\n",
       "│    │    └─Linear: 3-15                 37,752,832\n",
       "│    │    └─ReLU: 3-16                   --\n",
       "│    │    └─Dropout: 3-17                --\n",
       "│    │    └─Linear: 3-18                 16,781,312\n",
       "│    │    └─ReLU: 3-19                   --\n",
       "│    │    └─Linear: 3-20                 73,746\n",
       "=================================================================\n",
       "Total params: 57,077,586\n",
       "Trainable params: 57,077,586\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 50, \"log_interval\": 3}\n",
    "device = torch.device('cuda:0')\n",
    "willyAlexNet.to(device)\n",
    "willyAlexNet.share_memory()\n",
    "\n",
    "test(model=willyAlexNet, device=device, test_loaderIn=train_loader)\n",
    "test(model=willyAlexNet, device=device, test_loaderIn=valid_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 2.4038, Accuracy: 273/9730 (3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3943, Accuracy: 56/9730 (1%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train(model=willyAlexNet, device=device, train_loaderIn=train_loader, args=args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33582\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: 3.438911\n",
      "33582\tTrain Epoch: 1 [1200/9730 (17%)]\tLoss: 2.364406\n",
      "33582\tTrain Epoch: 1 [2400/9730 (33%)]\tLoss: 2.194092\n",
      "33582\tTrain Epoch: 1 [3600/9730 (50%)]\tLoss: 2.232552\n",
      "33582\tTrain Epoch: 1 [4800/9730 (67%)]\tLoss: 2.136112\n",
      "33582\tTrain Epoch: 1 [6000/9730 (83%)]\tLoss: 2.155311\n",
      "33582\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: 2.019054\n",
      "33582\tTrain Epoch: 2 [1200/9730 (17%)]\tLoss: 2.058074\n",
      "33582\tTrain Epoch: 2 [2400/9730 (33%)]\tLoss: 2.036019\n",
      "33582\tTrain Epoch: 2 [3600/9730 (50%)]\tLoss: 2.015141\n",
      "33582\tTrain Epoch: 2 [4800/9730 (67%)]\tLoss: 1.982431\n",
      "33582\tTrain Epoch: 2 [6000/9730 (83%)]\tLoss: 1.920383\n",
      "33582\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: 1.892114\n",
      "33582\tTrain Epoch: 3 [1200/9730 (17%)]\tLoss: 1.951307\n",
      "33582\tTrain Epoch: 3 [2400/9730 (33%)]\tLoss: 1.865821\n",
      "33582\tTrain Epoch: 3 [3600/9730 (50%)]\tLoss: 1.917925\n",
      "33582\tTrain Epoch: 3 [4800/9730 (67%)]\tLoss: 1.867115\n",
      "33582\tTrain Epoch: 3 [6000/9730 (83%)]\tLoss: 1.906361\n",
      "33582\tTrain Epoch: 4 [0/9730 (0%)]\tLoss: 1.798075\n",
      "33582\tTrain Epoch: 4 [1200/9730 (17%)]\tLoss: 1.899624\n",
      "33582\tTrain Epoch: 4 [2400/9730 (33%)]\tLoss: 1.786669\n",
      "33582\tTrain Epoch: 4 [3600/9730 (50%)]\tLoss: 1.813520\n",
      "33582\tTrain Epoch: 4 [4800/9730 (67%)]\tLoss: 1.800502\n",
      "33582\tTrain Epoch: 4 [6000/9730 (83%)]\tLoss: 1.757056\n",
      "33582\tTrain Epoch: 5 [0/9730 (0%)]\tLoss: 1.718354\n",
      "33582\tTrain Epoch: 5 [1200/9730 (17%)]\tLoss: 1.846017\n",
      "33582\tTrain Epoch: 5 [2400/9730 (33%)]\tLoss: 1.700741\n",
      "33582\tTrain Epoch: 5 [3600/9730 (50%)]\tLoss: 1.727201\n",
      "33582\tTrain Epoch: 5 [4800/9730 (67%)]\tLoss: 1.759600\n",
      "33582\tTrain Epoch: 5 [6000/9730 (83%)]\tLoss: 1.758519\n",
      "33582\tTrain Epoch: 6 [0/9730 (0%)]\tLoss: 1.676674\n",
      "33582\tTrain Epoch: 6 [1200/9730 (17%)]\tLoss: 1.638751\n",
      "33582\tTrain Epoch: 6 [2400/9730 (33%)]\tLoss: 1.711376\n",
      "33582\tTrain Epoch: 6 [3600/9730 (50%)]\tLoss: 1.794152\n",
      "33582\tTrain Epoch: 6 [4800/9730 (67%)]\tLoss: 1.755201\n",
      "33582\tTrain Epoch: 6 [6000/9730 (83%)]\tLoss: 1.584408\n",
      "33582\tTrain Epoch: 7 [0/9730 (0%)]\tLoss: 1.696952\n",
      "33582\tTrain Epoch: 7 [1200/9730 (17%)]\tLoss: 1.666058\n",
      "33582\tTrain Epoch: 7 [2400/9730 (33%)]\tLoss: 1.637237\n",
      "33582\tTrain Epoch: 7 [3600/9730 (50%)]\tLoss: 1.666573\n",
      "33582\tTrain Epoch: 7 [4800/9730 (67%)]\tLoss: 1.641645\n",
      "33582\tTrain Epoch: 7 [6000/9730 (83%)]\tLoss: 1.524804\n",
      "33582\tTrain Epoch: 8 [0/9730 (0%)]\tLoss: 1.707829\n",
      "33582\tTrain Epoch: 8 [1200/9730 (17%)]\tLoss: 1.596607\n",
      "33582\tTrain Epoch: 8 [2400/9730 (33%)]\tLoss: 1.604322\n",
      "33582\tTrain Epoch: 8 [3600/9730 (50%)]\tLoss: 1.563822\n",
      "33582\tTrain Epoch: 8 [4800/9730 (67%)]\tLoss: 1.526967\n",
      "33582\tTrain Epoch: 8 [6000/9730 (83%)]\tLoss: 1.710557\n",
      "33582\tTrain Epoch: 9 [0/9730 (0%)]\tLoss: 1.593498\n",
      "33582\tTrain Epoch: 9 [1200/9730 (17%)]\tLoss: 1.568108\n",
      "33582\tTrain Epoch: 9 [2400/9730 (33%)]\tLoss: 1.583652\n",
      "33582\tTrain Epoch: 9 [3600/9730 (50%)]\tLoss: 1.513379\n",
      "33582\tTrain Epoch: 9 [4800/9730 (67%)]\tLoss: 1.607132\n",
      "33582\tTrain Epoch: 9 [6000/9730 (83%)]\tLoss: 1.539729\n",
      "33582\tTrain Epoch: 10 [0/9730 (0%)]\tLoss: 1.572443\n",
      "33582\tTrain Epoch: 10 [1200/9730 (17%)]\tLoss: 1.515723\n",
      "33582\tTrain Epoch: 10 [2400/9730 (33%)]\tLoss: 1.543203\n",
      "33582\tTrain Epoch: 10 [3600/9730 (50%)]\tLoss: 1.496608\n",
      "33582\tTrain Epoch: 10 [4800/9730 (67%)]\tLoss: 1.631115\n",
      "33582\tTrain Epoch: 10 [6000/9730 (83%)]\tLoss: 1.514324\n",
      "33582\tTrain Epoch: 11 [0/9730 (0%)]\tLoss: 1.466663\n",
      "33582\tTrain Epoch: 11 [1200/9730 (17%)]\tLoss: 1.412124\n",
      "33582\tTrain Epoch: 11 [2400/9730 (33%)]\tLoss: 1.446274\n",
      "33582\tTrain Epoch: 11 [3600/9730 (50%)]\tLoss: 1.409206\n",
      "33582\tTrain Epoch: 11 [4800/9730 (67%)]\tLoss: 1.576471\n",
      "33582\tTrain Epoch: 11 [6000/9730 (83%)]\tLoss: 1.435204\n",
      "33582\tTrain Epoch: 12 [0/9730 (0%)]\tLoss: 1.315943\n",
      "33582\tTrain Epoch: 12 [1200/9730 (17%)]\tLoss: 1.421660\n",
      "33582\tTrain Epoch: 12 [2400/9730 (33%)]\tLoss: 1.376832\n",
      "33582\tTrain Epoch: 12 [3600/9730 (50%)]\tLoss: 1.525141\n",
      "33582\tTrain Epoch: 12 [4800/9730 (67%)]\tLoss: 1.452913\n",
      "33582\tTrain Epoch: 12 [6000/9730 (83%)]\tLoss: 1.454098\n",
      "33582\tTrain Epoch: 13 [0/9730 (0%)]\tLoss: 1.445899\n",
      "33582\tTrain Epoch: 13 [1200/9730 (17%)]\tLoss: 1.527368\n",
      "33582\tTrain Epoch: 13 [2400/9730 (33%)]\tLoss: 1.430226\n",
      "33582\tTrain Epoch: 13 [3600/9730 (50%)]\tLoss: 1.495888\n",
      "33582\tTrain Epoch: 13 [4800/9730 (67%)]\tLoss: 1.362610\n",
      "33582\tTrain Epoch: 13 [6000/9730 (83%)]\tLoss: 1.349061\n",
      "33582\tTrain Epoch: 14 [0/9730 (0%)]\tLoss: 1.471050\n",
      "33582\tTrain Epoch: 14 [1200/9730 (17%)]\tLoss: 1.458652\n",
      "33582\tTrain Epoch: 14 [2400/9730 (33%)]\tLoss: 1.359155\n",
      "33582\tTrain Epoch: 14 [3600/9730 (50%)]\tLoss: 1.340643\n",
      "33582\tTrain Epoch: 14 [4800/9730 (67%)]\tLoss: 1.318700\n",
      "33582\tTrain Epoch: 14 [6000/9730 (83%)]\tLoss: 1.318950\n",
      "33582\tTrain Epoch: 15 [0/9730 (0%)]\tLoss: 1.413010\n",
      "33582\tTrain Epoch: 15 [1200/9730 (17%)]\tLoss: 1.328573\n",
      "33582\tTrain Epoch: 15 [2400/9730 (33%)]\tLoss: 1.443802\n",
      "33582\tTrain Epoch: 15 [3600/9730 (50%)]\tLoss: 1.245102\n",
      "33582\tTrain Epoch: 15 [4800/9730 (67%)]\tLoss: 1.417770\n",
      "33582\tTrain Epoch: 15 [6000/9730 (83%)]\tLoss: 1.415867\n",
      "33582\tTrain Epoch: 16 [0/9730 (0%)]\tLoss: 1.447449\n",
      "33582\tTrain Epoch: 16 [1200/9730 (17%)]\tLoss: 1.290139\n",
      "33582\tTrain Epoch: 16 [2400/9730 (33%)]\tLoss: 1.472587\n",
      "33582\tTrain Epoch: 16 [3600/9730 (50%)]\tLoss: 1.363377\n",
      "33582\tTrain Epoch: 16 [4800/9730 (67%)]\tLoss: 1.389599\n",
      "33582\tTrain Epoch: 16 [6000/9730 (83%)]\tLoss: 1.342725\n",
      "33582\tTrain Epoch: 17 [0/9730 (0%)]\tLoss: 1.289093\n",
      "33582\tTrain Epoch: 17 [1200/9730 (17%)]\tLoss: 1.362761\n",
      "33582\tTrain Epoch: 17 [2400/9730 (33%)]\tLoss: 1.408949\n",
      "33582\tTrain Epoch: 17 [3600/9730 (50%)]\tLoss: 1.327822\n",
      "33582\tTrain Epoch: 17 [4800/9730 (67%)]\tLoss: 1.325666\n",
      "33582\tTrain Epoch: 17 [6000/9730 (83%)]\tLoss: 1.276910\n",
      "33582\tTrain Epoch: 18 [0/9730 (0%)]\tLoss: 1.335728\n",
      "33582\tTrain Epoch: 18 [1200/9730 (17%)]\tLoss: 1.284877\n",
      "33582\tTrain Epoch: 18 [2400/9730 (33%)]\tLoss: 1.226202\n",
      "33582\tTrain Epoch: 18 [3600/9730 (50%)]\tLoss: 1.295703\n",
      "33582\tTrain Epoch: 18 [4800/9730 (67%)]\tLoss: 1.262440\n",
      "33582\tTrain Epoch: 18 [6000/9730 (83%)]\tLoss: 1.276171\n",
      "33582\tTrain Epoch: 19 [0/9730 (0%)]\tLoss: 1.201467\n",
      "33582\tTrain Epoch: 19 [1200/9730 (17%)]\tLoss: 1.260100\n",
      "33582\tTrain Epoch: 19 [2400/9730 (33%)]\tLoss: 1.249083\n",
      "33582\tTrain Epoch: 19 [3600/9730 (50%)]\tLoss: 1.440229\n",
      "33582\tTrain Epoch: 19 [4800/9730 (67%)]\tLoss: 1.283163\n",
      "33582\tTrain Epoch: 19 [6000/9730 (83%)]\tLoss: 1.302891\n",
      "33582\tTrain Epoch: 20 [0/9730 (0%)]\tLoss: 1.296841\n",
      "33582\tTrain Epoch: 20 [1200/9730 (17%)]\tLoss: 1.237841\n",
      "33582\tTrain Epoch: 20 [2400/9730 (33%)]\tLoss: 1.185094\n",
      "33582\tTrain Epoch: 20 [3600/9730 (50%)]\tLoss: 1.275377\n",
      "33582\tTrain Epoch: 20 [4800/9730 (67%)]\tLoss: 1.365692\n",
      "33582\tTrain Epoch: 20 [6000/9730 (83%)]\tLoss: 1.147210\n",
      "33582\tTrain Epoch: 21 [0/9730 (0%)]\tLoss: 1.292171\n",
      "33582\tTrain Epoch: 21 [1200/9730 (17%)]\tLoss: 1.251381\n",
      "33582\tTrain Epoch: 21 [2400/9730 (33%)]\tLoss: 1.269539\n",
      "33582\tTrain Epoch: 21 [3600/9730 (50%)]\tLoss: 1.320573\n",
      "33582\tTrain Epoch: 21 [4800/9730 (67%)]\tLoss: 1.265452\n",
      "33582\tTrain Epoch: 21 [6000/9730 (83%)]\tLoss: 1.201697\n",
      "33582\tTrain Epoch: 22 [0/9730 (0%)]\tLoss: 1.254148\n",
      "33582\tTrain Epoch: 22 [1200/9730 (17%)]\tLoss: 1.166605\n",
      "33582\tTrain Epoch: 22 [2400/9730 (33%)]\tLoss: 1.207526\n",
      "33582\tTrain Epoch: 22 [3600/9730 (50%)]\tLoss: 1.216015\n",
      "33582\tTrain Epoch: 22 [4800/9730 (67%)]\tLoss: 1.114060\n",
      "33582\tTrain Epoch: 22 [6000/9730 (83%)]\tLoss: 1.257067\n",
      "33582\tTrain Epoch: 23 [0/9730 (0%)]\tLoss: 1.148013\n",
      "33582\tTrain Epoch: 23 [1200/9730 (17%)]\tLoss: 1.106765\n",
      "33582\tTrain Epoch: 23 [2400/9730 (33%)]\tLoss: 1.116552\n",
      "33582\tTrain Epoch: 23 [3600/9730 (50%)]\tLoss: 1.172238\n",
      "33582\tTrain Epoch: 23 [4800/9730 (67%)]\tLoss: 1.194467\n",
      "33582\tTrain Epoch: 23 [6000/9730 (83%)]\tLoss: 1.195673\n",
      "33582\tTrain Epoch: 24 [0/9730 (0%)]\tLoss: 1.180654\n",
      "33582\tTrain Epoch: 24 [1200/9730 (17%)]\tLoss: 1.069795\n",
      "33582\tTrain Epoch: 24 [2400/9730 (33%)]\tLoss: 1.339644\n",
      "33582\tTrain Epoch: 24 [3600/9730 (50%)]\tLoss: 1.165031\n",
      "33582\tTrain Epoch: 24 [4800/9730 (67%)]\tLoss: 1.138749\n",
      "33582\tTrain Epoch: 24 [6000/9730 (83%)]\tLoss: 1.175177\n",
      "33582\tTrain Epoch: 25 [0/9730 (0%)]\tLoss: 1.074875\n",
      "33582\tTrain Epoch: 25 [1200/9730 (17%)]\tLoss: 1.173804\n",
      "33582\tTrain Epoch: 25 [2400/9730 (33%)]\tLoss: 1.148854\n",
      "33582\tTrain Epoch: 25 [3600/9730 (50%)]\tLoss: 1.205126\n",
      "33582\tTrain Epoch: 25 [4800/9730 (67%)]\tLoss: 1.158595\n",
      "33582\tTrain Epoch: 25 [6000/9730 (83%)]\tLoss: 1.270097\n",
      "33582\tTrain Epoch: 26 [0/9730 (0%)]\tLoss: 1.122747\n",
      "33582\tTrain Epoch: 26 [1200/9730 (17%)]\tLoss: 1.166143\n",
      "33582\tTrain Epoch: 26 [2400/9730 (33%)]\tLoss: 1.149915\n",
      "33582\tTrain Epoch: 26 [3600/9730 (50%)]\tLoss: 1.154969\n",
      "33582\tTrain Epoch: 26 [4800/9730 (67%)]\tLoss: 1.240146\n",
      "33582\tTrain Epoch: 26 [6000/9730 (83%)]\tLoss: 1.276835\n",
      "33582\tTrain Epoch: 27 [0/9730 (0%)]\tLoss: 1.157978\n",
      "33582\tTrain Epoch: 27 [1200/9730 (17%)]\tLoss: 1.103116\n",
      "33582\tTrain Epoch: 27 [2400/9730 (33%)]\tLoss: 1.132564\n",
      "33582\tTrain Epoch: 27 [3600/9730 (50%)]\tLoss: 1.163690\n",
      "33582\tTrain Epoch: 27 [4800/9730 (67%)]\tLoss: 1.252815\n",
      "33582\tTrain Epoch: 27 [6000/9730 (83%)]\tLoss: 1.124820\n",
      "33582\tTrain Epoch: 28 [0/9730 (0%)]\tLoss: 1.143123\n",
      "33582\tTrain Epoch: 28 [1200/9730 (17%)]\tLoss: 1.144092\n",
      "33582\tTrain Epoch: 28 [2400/9730 (33%)]\tLoss: 1.108814\n",
      "33582\tTrain Epoch: 28 [3600/9730 (50%)]\tLoss: 1.221846\n",
      "33582\tTrain Epoch: 28 [4800/9730 (67%)]\tLoss: 1.156929\n",
      "33582\tTrain Epoch: 28 [6000/9730 (83%)]\tLoss: 1.166748\n",
      "33582\tTrain Epoch: 29 [0/9730 (0%)]\tLoss: 1.126323\n",
      "33582\tTrain Epoch: 29 [1200/9730 (17%)]\tLoss: 1.092892\n",
      "33582\tTrain Epoch: 29 [2400/9730 (33%)]\tLoss: 1.137531\n",
      "33582\tTrain Epoch: 29 [3600/9730 (50%)]\tLoss: 1.137201\n",
      "33582\tTrain Epoch: 29 [4800/9730 (67%)]\tLoss: 1.029500\n",
      "33582\tTrain Epoch: 29 [6000/9730 (83%)]\tLoss: 1.133589\n",
      "33582\tTrain Epoch: 30 [0/9730 (0%)]\tLoss: 1.004115\n",
      "33582\tTrain Epoch: 30 [1200/9730 (17%)]\tLoss: 1.110180\n",
      "33582\tTrain Epoch: 30 [2400/9730 (33%)]\tLoss: 1.142553\n",
      "33582\tTrain Epoch: 30 [3600/9730 (50%)]\tLoss: 1.127096\n",
      "33582\tTrain Epoch: 30 [4800/9730 (67%)]\tLoss: 1.168298\n",
      "33582\tTrain Epoch: 30 [6000/9730 (83%)]\tLoss: 1.106091\n",
      "33582\tTrain Epoch: 31 [0/9730 (0%)]\tLoss: 0.989012\n",
      "33582\tTrain Epoch: 31 [1200/9730 (17%)]\tLoss: 0.961713\n",
      "33582\tTrain Epoch: 31 [2400/9730 (33%)]\tLoss: 1.089262\n",
      "33582\tTrain Epoch: 31 [3600/9730 (50%)]\tLoss: 1.135540\n",
      "33582\tTrain Epoch: 31 [4800/9730 (67%)]\tLoss: 1.086206\n",
      "33582\tTrain Epoch: 31 [6000/9730 (83%)]\tLoss: 1.139441\n",
      "33582\tTrain Epoch: 32 [0/9730 (0%)]\tLoss: 1.114182\n",
      "33582\tTrain Epoch: 32 [1200/9730 (17%)]\tLoss: 0.935540\n",
      "33582\tTrain Epoch: 32 [2400/9730 (33%)]\tLoss: 1.124387\n",
      "33582\tTrain Epoch: 32 [3600/9730 (50%)]\tLoss: 1.174325\n",
      "33582\tTrain Epoch: 32 [4800/9730 (67%)]\tLoss: 1.002369\n",
      "33582\tTrain Epoch: 32 [6000/9730 (83%)]\tLoss: 1.175330\n",
      "33582\tTrain Epoch: 33 [0/9730 (0%)]\tLoss: 1.121098\n",
      "33582\tTrain Epoch: 33 [1200/9730 (17%)]\tLoss: 1.085458\n",
      "33582\tTrain Epoch: 33 [2400/9730 (33%)]\tLoss: 1.038629\n",
      "33582\tTrain Epoch: 33 [3600/9730 (50%)]\tLoss: 1.083459\n",
      "33582\tTrain Epoch: 33 [4800/9730 (67%)]\tLoss: 1.056962\n",
      "33582\tTrain Epoch: 33 [6000/9730 (83%)]\tLoss: 1.100160\n",
      "33582\tTrain Epoch: 34 [0/9730 (0%)]\tLoss: 1.031110\n",
      "33582\tTrain Epoch: 34 [1200/9730 (17%)]\tLoss: 1.049110\n",
      "33582\tTrain Epoch: 34 [2400/9730 (33%)]\tLoss: 0.908077\n",
      "33582\tTrain Epoch: 34 [3600/9730 (50%)]\tLoss: 1.096300\n",
      "33582\tTrain Epoch: 34 [4800/9730 (67%)]\tLoss: 1.109172\n",
      "33582\tTrain Epoch: 34 [6000/9730 (83%)]\tLoss: 1.036036\n",
      "33582\tTrain Epoch: 35 [0/9730 (0%)]\tLoss: 1.026964\n",
      "33582\tTrain Epoch: 35 [1200/9730 (17%)]\tLoss: 1.024269\n",
      "33582\tTrain Epoch: 35 [2400/9730 (33%)]\tLoss: 1.011119\n",
      "33582\tTrain Epoch: 35 [3600/9730 (50%)]\tLoss: 0.977729\n",
      "33582\tTrain Epoch: 35 [4800/9730 (67%)]\tLoss: 1.143653\n",
      "33582\tTrain Epoch: 35 [6000/9730 (83%)]\tLoss: 0.957823\n",
      "33582\tTrain Epoch: 36 [0/9730 (0%)]\tLoss: 0.958845\n",
      "33582\tTrain Epoch: 36 [1200/9730 (17%)]\tLoss: 0.959892\n",
      "33582\tTrain Epoch: 36 [2400/9730 (33%)]\tLoss: 1.043932\n",
      "33582\tTrain Epoch: 36 [3600/9730 (50%)]\tLoss: 1.037369\n",
      "33582\tTrain Epoch: 36 [4800/9730 (67%)]\tLoss: 1.099114\n",
      "33582\tTrain Epoch: 36 [6000/9730 (83%)]\tLoss: 1.059138\n",
      "33582\tTrain Epoch: 37 [0/9730 (0%)]\tLoss: 0.965627\n",
      "33582\tTrain Epoch: 37 [1200/9730 (17%)]\tLoss: 1.011830\n",
      "33582\tTrain Epoch: 37 [2400/9730 (33%)]\tLoss: 0.950241\n",
      "33582\tTrain Epoch: 37 [3600/9730 (50%)]\tLoss: 1.016264\n",
      "33582\tTrain Epoch: 37 [4800/9730 (67%)]\tLoss: 1.054921\n",
      "33582\tTrain Epoch: 37 [6000/9730 (83%)]\tLoss: 0.997852\n",
      "33582\tTrain Epoch: 38 [0/9730 (0%)]\tLoss: 0.979961\n",
      "33582\tTrain Epoch: 38 [1200/9730 (17%)]\tLoss: 1.005961\n",
      "33582\tTrain Epoch: 38 [2400/9730 (33%)]\tLoss: 1.001799\n",
      "33582\tTrain Epoch: 38 [3600/9730 (50%)]\tLoss: 1.064658\n",
      "33582\tTrain Epoch: 38 [4800/9730 (67%)]\tLoss: 1.024619\n",
      "33582\tTrain Epoch: 38 [6000/9730 (83%)]\tLoss: 1.073840\n",
      "33582\tTrain Epoch: 39 [0/9730 (0%)]\tLoss: 0.999953\n",
      "33582\tTrain Epoch: 39 [1200/9730 (17%)]\tLoss: 1.048828\n",
      "33582\tTrain Epoch: 39 [2400/9730 (33%)]\tLoss: 1.079542\n",
      "33582\tTrain Epoch: 39 [3600/9730 (50%)]\tLoss: 0.937584\n",
      "33582\tTrain Epoch: 39 [4800/9730 (67%)]\tLoss: 1.089492\n",
      "33582\tTrain Epoch: 39 [6000/9730 (83%)]\tLoss: 0.987680\n",
      "33582\tTrain Epoch: 40 [0/9730 (0%)]\tLoss: 1.041047\n",
      "33582\tTrain Epoch: 40 [1200/9730 (17%)]\tLoss: 0.936727\n",
      "33582\tTrain Epoch: 40 [2400/9730 (33%)]\tLoss: 0.988982\n",
      "33582\tTrain Epoch: 40 [3600/9730 (50%)]\tLoss: 1.075029\n",
      "33582\tTrain Epoch: 40 [4800/9730 (67%)]\tLoss: 0.933122\n",
      "33582\tTrain Epoch: 40 [6000/9730 (83%)]\tLoss: 1.000479\n",
      "33582\tTrain Epoch: 41 [0/9730 (0%)]\tLoss: 0.996494\n",
      "33582\tTrain Epoch: 41 [1200/9730 (17%)]\tLoss: 1.003706\n",
      "33582\tTrain Epoch: 41 [2400/9730 (33%)]\tLoss: 0.988638\n",
      "33582\tTrain Epoch: 41 [3600/9730 (50%)]\tLoss: 0.948089\n",
      "33582\tTrain Epoch: 41 [4800/9730 (67%)]\tLoss: 0.947424\n",
      "33582\tTrain Epoch: 41 [6000/9730 (83%)]\tLoss: 0.841172\n",
      "33582\tTrain Epoch: 42 [0/9730 (0%)]\tLoss: 0.915503\n",
      "33582\tTrain Epoch: 42 [1200/9730 (17%)]\tLoss: 1.081382\n",
      "33582\tTrain Epoch: 42 [2400/9730 (33%)]\tLoss: 0.891754\n",
      "33582\tTrain Epoch: 42 [3600/9730 (50%)]\tLoss: 0.990688\n",
      "33582\tTrain Epoch: 42 [4800/9730 (67%)]\tLoss: 1.021323\n",
      "33582\tTrain Epoch: 42 [6000/9730 (83%)]\tLoss: 1.037132\n",
      "33582\tTrain Epoch: 43 [0/9730 (0%)]\tLoss: 0.943429\n",
      "33582\tTrain Epoch: 43 [1200/9730 (17%)]\tLoss: 0.842100\n",
      "33582\tTrain Epoch: 43 [2400/9730 (33%)]\tLoss: 0.982651\n",
      "33582\tTrain Epoch: 43 [3600/9730 (50%)]\tLoss: 0.986417\n",
      "33582\tTrain Epoch: 43 [4800/9730 (67%)]\tLoss: 0.975473\n",
      "33582\tTrain Epoch: 43 [6000/9730 (83%)]\tLoss: 0.979453\n",
      "33582\tTrain Epoch: 44 [0/9730 (0%)]\tLoss: 0.916068\n",
      "33582\tTrain Epoch: 44 [1200/9730 (17%)]\tLoss: 0.958403\n",
      "33582\tTrain Epoch: 44 [2400/9730 (33%)]\tLoss: 0.853998\n",
      "33582\tTrain Epoch: 44 [3600/9730 (50%)]\tLoss: 0.847012\n",
      "33582\tTrain Epoch: 44 [4800/9730 (67%)]\tLoss: 0.960561\n",
      "33582\tTrain Epoch: 44 [6000/9730 (83%)]\tLoss: 1.009453\n",
      "33582\tTrain Epoch: 45 [0/9730 (0%)]\tLoss: 1.031285\n",
      "33582\tTrain Epoch: 45 [1200/9730 (17%)]\tLoss: 0.825886\n",
      "33582\tTrain Epoch: 45 [2400/9730 (33%)]\tLoss: 0.983075\n",
      "33582\tTrain Epoch: 45 [3600/9730 (50%)]\tLoss: 0.999393\n",
      "33582\tTrain Epoch: 45 [4800/9730 (67%)]\tLoss: 0.946076\n",
      "33582\tTrain Epoch: 45 [6000/9730 (83%)]\tLoss: 0.838579\n",
      "33582\tTrain Epoch: 46 [0/9730 (0%)]\tLoss: 0.971986\n",
      "33582\tTrain Epoch: 46 [1200/9730 (17%)]\tLoss: 0.888238\n",
      "33582\tTrain Epoch: 46 [2400/9730 (33%)]\tLoss: 1.022346\n",
      "33582\tTrain Epoch: 46 [3600/9730 (50%)]\tLoss: 0.943614\n",
      "33582\tTrain Epoch: 46 [4800/9730 (67%)]\tLoss: 1.004451\n",
      "33582\tTrain Epoch: 46 [6000/9730 (83%)]\tLoss: 0.953178\n",
      "33582\tTrain Epoch: 47 [0/9730 (0%)]\tLoss: 0.914411\n",
      "33582\tTrain Epoch: 47 [1200/9730 (17%)]\tLoss: 0.948721\n",
      "33582\tTrain Epoch: 47 [2400/9730 (33%)]\tLoss: 0.838296\n",
      "33582\tTrain Epoch: 47 [3600/9730 (50%)]\tLoss: 0.997086\n",
      "33582\tTrain Epoch: 47 [4800/9730 (67%)]\tLoss: 0.801984\n",
      "33582\tTrain Epoch: 47 [6000/9730 (83%)]\tLoss: 0.904046\n",
      "33582\tTrain Epoch: 48 [0/9730 (0%)]\tLoss: 0.994390\n",
      "33582\tTrain Epoch: 48 [1200/9730 (17%)]\tLoss: 1.021990\n",
      "33582\tTrain Epoch: 48 [2400/9730 (33%)]\tLoss: 0.971401\n",
      "33582\tTrain Epoch: 48 [3600/9730 (50%)]\tLoss: 1.030638\n",
      "33582\tTrain Epoch: 48 [4800/9730 (67%)]\tLoss: 0.865388\n",
      "33582\tTrain Epoch: 48 [6000/9730 (83%)]\tLoss: 1.001224\n",
      "33582\tTrain Epoch: 49 [0/9730 (0%)]\tLoss: 0.898128\n",
      "33582\tTrain Epoch: 49 [1200/9730 (17%)]\tLoss: 0.997936\n",
      "33582\tTrain Epoch: 49 [2400/9730 (33%)]\tLoss: 1.032925\n",
      "33582\tTrain Epoch: 49 [3600/9730 (50%)]\tLoss: 0.898261\n",
      "33582\tTrain Epoch: 49 [4800/9730 (67%)]\tLoss: 0.898820\n",
      "33582\tTrain Epoch: 49 [6000/9730 (83%)]\tLoss: 1.004323\n",
      "33582\tTrain Epoch: 50 [0/9730 (0%)]\tLoss: 0.928631\n",
      "33582\tTrain Epoch: 50 [1200/9730 (17%)]\tLoss: 0.902154\n",
      "33582\tTrain Epoch: 50 [2400/9730 (33%)]\tLoss: 0.906047\n",
      "33582\tTrain Epoch: 50 [3600/9730 (50%)]\tLoss: 0.890549\n",
      "33582\tTrain Epoch: 50 [4800/9730 (67%)]\tLoss: 0.916322\n",
      "33582\tTrain Epoch: 50 [6000/9730 (83%)]\tLoss: 0.841175\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "test(model=willyAlexNet, device=device, test_loaderIn=train_loader)\n",
    "test(model=willyAlexNet, device=device, test_loaderIn=valid_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 0.5723, Accuracy: 5311/9730 (55%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1559, Accuracy: 775/9730 (8%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 50, \"log_interval\": 3}\n",
    "train(model=willyAlexNet, device=device, train_loaderIn=train_loader, args=args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33582\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: 0.968747\n",
      "33582\tTrain Epoch: 1 [1200/9730 (17%)]\tLoss: 0.903528\n",
      "33582\tTrain Epoch: 1 [2400/9730 (33%)]\tLoss: 0.919176\n",
      "33582\tTrain Epoch: 1 [3600/9730 (50%)]\tLoss: 0.851960\n",
      "33582\tTrain Epoch: 1 [4800/9730 (67%)]\tLoss: 0.987713\n",
      "33582\tTrain Epoch: 1 [6000/9730 (83%)]\tLoss: 0.990164\n",
      "33582\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: 0.914763\n",
      "33582\tTrain Epoch: 2 [1200/9730 (17%)]\tLoss: 0.906578\n",
      "33582\tTrain Epoch: 2 [2400/9730 (33%)]\tLoss: 0.894332\n",
      "33582\tTrain Epoch: 2 [3600/9730 (50%)]\tLoss: 0.955034\n",
      "33582\tTrain Epoch: 2 [4800/9730 (67%)]\tLoss: 1.044238\n",
      "33582\tTrain Epoch: 2 [6000/9730 (83%)]\tLoss: 0.839121\n",
      "33582\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: 0.861534\n",
      "33582\tTrain Epoch: 3 [1200/9730 (17%)]\tLoss: 0.849830\n",
      "33582\tTrain Epoch: 3 [2400/9730 (33%)]\tLoss: 0.967214\n",
      "33582\tTrain Epoch: 3 [3600/9730 (50%)]\tLoss: 0.921844\n",
      "33582\tTrain Epoch: 3 [4800/9730 (67%)]\tLoss: 0.846225\n",
      "33582\tTrain Epoch: 3 [6000/9730 (83%)]\tLoss: 0.932787\n",
      "33582\tTrain Epoch: 4 [0/9730 (0%)]\tLoss: 0.913354\n",
      "33582\tTrain Epoch: 4 [1200/9730 (17%)]\tLoss: 0.834329\n",
      "33582\tTrain Epoch: 4 [2400/9730 (33%)]\tLoss: 0.976191\n",
      "33582\tTrain Epoch: 4 [3600/9730 (50%)]\tLoss: 0.897820\n",
      "33582\tTrain Epoch: 4 [4800/9730 (67%)]\tLoss: 0.871005\n",
      "33582\tTrain Epoch: 4 [6000/9730 (83%)]\tLoss: 0.854785\n",
      "33582\tTrain Epoch: 5 [0/9730 (0%)]\tLoss: 0.934070\n",
      "33582\tTrain Epoch: 5 [1200/9730 (17%)]\tLoss: 0.932293\n",
      "33582\tTrain Epoch: 5 [2400/9730 (33%)]\tLoss: 0.909901\n",
      "33582\tTrain Epoch: 5 [3600/9730 (50%)]\tLoss: 0.920957\n",
      "33582\tTrain Epoch: 5 [4800/9730 (67%)]\tLoss: 0.902448\n",
      "33582\tTrain Epoch: 5 [6000/9730 (83%)]\tLoss: 0.878589\n",
      "33582\tTrain Epoch: 6 [0/9730 (0%)]\tLoss: 0.871988\n",
      "33582\tTrain Epoch: 6 [1200/9730 (17%)]\tLoss: 0.863947\n",
      "33582\tTrain Epoch: 6 [2400/9730 (33%)]\tLoss: 0.934424\n",
      "33582\tTrain Epoch: 6 [3600/9730 (50%)]\tLoss: 0.677931\n",
      "33582\tTrain Epoch: 6 [4800/9730 (67%)]\tLoss: 0.872433\n",
      "33582\tTrain Epoch: 6 [6000/9730 (83%)]\tLoss: 0.866623\n",
      "33582\tTrain Epoch: 7 [0/9730 (0%)]\tLoss: 0.868571\n",
      "33582\tTrain Epoch: 7 [1200/9730 (17%)]\tLoss: 0.809487\n",
      "33582\tTrain Epoch: 7 [2400/9730 (33%)]\tLoss: 0.870849\n",
      "33582\tTrain Epoch: 7 [3600/9730 (50%)]\tLoss: 0.892704\n",
      "33582\tTrain Epoch: 7 [4800/9730 (67%)]\tLoss: 0.948201\n",
      "33582\tTrain Epoch: 7 [6000/9730 (83%)]\tLoss: 0.932399\n",
      "33582\tTrain Epoch: 8 [0/9730 (0%)]\tLoss: 0.823117\n",
      "33582\tTrain Epoch: 8 [1200/9730 (17%)]\tLoss: 0.937929\n",
      "33582\tTrain Epoch: 8 [2400/9730 (33%)]\tLoss: 0.972153\n",
      "33582\tTrain Epoch: 8 [3600/9730 (50%)]\tLoss: 0.815041\n",
      "33582\tTrain Epoch: 8 [4800/9730 (67%)]\tLoss: 0.799720\n",
      "33582\tTrain Epoch: 8 [6000/9730 (83%)]\tLoss: 0.847270\n",
      "33582\tTrain Epoch: 9 [0/9730 (0%)]\tLoss: 0.870270\n",
      "33582\tTrain Epoch: 9 [1200/9730 (17%)]\tLoss: 0.804161\n",
      "33582\tTrain Epoch: 9 [2400/9730 (33%)]\tLoss: 0.856104\n",
      "33582\tTrain Epoch: 9 [3600/9730 (50%)]\tLoss: 0.794220\n",
      "33582\tTrain Epoch: 9 [4800/9730 (67%)]\tLoss: 0.904260\n",
      "33582\tTrain Epoch: 9 [6000/9730 (83%)]\tLoss: 0.777994\n",
      "33582\tTrain Epoch: 10 [0/9730 (0%)]\tLoss: 0.818823\n",
      "33582\tTrain Epoch: 10 [1200/9730 (17%)]\tLoss: 0.841150\n",
      "33582\tTrain Epoch: 10 [2400/9730 (33%)]\tLoss: 0.833614\n",
      "33582\tTrain Epoch: 10 [3600/9730 (50%)]\tLoss: 0.784615\n",
      "33582\tTrain Epoch: 10 [4800/9730 (67%)]\tLoss: 0.829577\n",
      "33582\tTrain Epoch: 10 [6000/9730 (83%)]\tLoss: 0.855218\n",
      "33582\tTrain Epoch: 11 [0/9730 (0%)]\tLoss: 0.846777\n",
      "33582\tTrain Epoch: 11 [1200/9730 (17%)]\tLoss: 0.844669\n",
      "33582\tTrain Epoch: 11 [2400/9730 (33%)]\tLoss: 0.831357\n",
      "33582\tTrain Epoch: 11 [3600/9730 (50%)]\tLoss: 0.842451\n",
      "33582\tTrain Epoch: 11 [4800/9730 (67%)]\tLoss: 0.904684\n",
      "33582\tTrain Epoch: 11 [6000/9730 (83%)]\tLoss: 0.820687\n",
      "33582\tTrain Epoch: 12 [0/9730 (0%)]\tLoss: 0.857755\n",
      "33582\tTrain Epoch: 12 [1200/9730 (17%)]\tLoss: 0.880233\n",
      "33582\tTrain Epoch: 12 [2400/9730 (33%)]\tLoss: 0.819892\n",
      "33582\tTrain Epoch: 12 [3600/9730 (50%)]\tLoss: 0.877060\n",
      "33582\tTrain Epoch: 12 [4800/9730 (67%)]\tLoss: 0.800964\n",
      "33582\tTrain Epoch: 12 [6000/9730 (83%)]\tLoss: 0.826620\n",
      "33582\tTrain Epoch: 13 [0/9730 (0%)]\tLoss: 0.885024\n",
      "33582\tTrain Epoch: 13 [1200/9730 (17%)]\tLoss: 0.828538\n",
      "33582\tTrain Epoch: 13 [2400/9730 (33%)]\tLoss: 0.921529\n",
      "33582\tTrain Epoch: 13 [3600/9730 (50%)]\tLoss: 0.742729\n",
      "33582\tTrain Epoch: 13 [4800/9730 (67%)]\tLoss: 0.803560\n",
      "33582\tTrain Epoch: 13 [6000/9730 (83%)]\tLoss: 0.857166\n",
      "33582\tTrain Epoch: 14 [0/9730 (0%)]\tLoss: 0.829236\n",
      "33582\tTrain Epoch: 14 [1200/9730 (17%)]\tLoss: 0.806198\n",
      "33582\tTrain Epoch: 14 [2400/9730 (33%)]\tLoss: 0.865365\n",
      "33582\tTrain Epoch: 14 [3600/9730 (50%)]\tLoss: 0.882916\n",
      "33582\tTrain Epoch: 14 [4800/9730 (67%)]\tLoss: 0.798613\n",
      "33582\tTrain Epoch: 14 [6000/9730 (83%)]\tLoss: 0.821994\n",
      "33582\tTrain Epoch: 15 [0/9730 (0%)]\tLoss: 0.757745\n",
      "33582\tTrain Epoch: 15 [1200/9730 (17%)]\tLoss: 0.792183\n",
      "33582\tTrain Epoch: 15 [2400/9730 (33%)]\tLoss: 0.789929\n",
      "33582\tTrain Epoch: 15 [3600/9730 (50%)]\tLoss: 0.735117\n",
      "33582\tTrain Epoch: 15 [4800/9730 (67%)]\tLoss: 0.912769\n",
      "33582\tTrain Epoch: 15 [6000/9730 (83%)]\tLoss: 0.736327\n",
      "33582\tTrain Epoch: 16 [0/9730 (0%)]\tLoss: 0.859126\n",
      "33582\tTrain Epoch: 16 [1200/9730 (17%)]\tLoss: 0.777960\n",
      "33582\tTrain Epoch: 16 [2400/9730 (33%)]\tLoss: 0.803228\n",
      "33582\tTrain Epoch: 16 [3600/9730 (50%)]\tLoss: 0.899277\n",
      "33582\tTrain Epoch: 16 [4800/9730 (67%)]\tLoss: 0.772701\n",
      "33582\tTrain Epoch: 16 [6000/9730 (83%)]\tLoss: 0.793929\n",
      "33582\tTrain Epoch: 17 [0/9730 (0%)]\tLoss: 0.810327\n",
      "33582\tTrain Epoch: 17 [1200/9730 (17%)]\tLoss: 0.848184\n",
      "33582\tTrain Epoch: 17 [2400/9730 (33%)]\tLoss: 0.856624\n",
      "33582\tTrain Epoch: 17 [3600/9730 (50%)]\tLoss: 0.833711\n",
      "33582\tTrain Epoch: 17 [4800/9730 (67%)]\tLoss: 0.787896\n",
      "33582\tTrain Epoch: 17 [6000/9730 (83%)]\tLoss: 0.893918\n",
      "33582\tTrain Epoch: 18 [0/9730 (0%)]\tLoss: 0.800619\n",
      "33582\tTrain Epoch: 18 [1200/9730 (17%)]\tLoss: 0.840047\n",
      "33582\tTrain Epoch: 18 [2400/9730 (33%)]\tLoss: 0.806807\n",
      "33582\tTrain Epoch: 18 [3600/9730 (50%)]\tLoss: 0.779867\n",
      "33582\tTrain Epoch: 18 [4800/9730 (67%)]\tLoss: 0.757504\n",
      "33582\tTrain Epoch: 18 [6000/9730 (83%)]\tLoss: 0.810738\n",
      "33582\tTrain Epoch: 19 [0/9730 (0%)]\tLoss: 0.694842\n",
      "33582\tTrain Epoch: 19 [1200/9730 (17%)]\tLoss: 0.779511\n",
      "33582\tTrain Epoch: 19 [2400/9730 (33%)]\tLoss: 0.752361\n",
      "33582\tTrain Epoch: 19 [3600/9730 (50%)]\tLoss: 0.820756\n",
      "33582\tTrain Epoch: 19 [4800/9730 (67%)]\tLoss: 0.753821\n",
      "33582\tTrain Epoch: 19 [6000/9730 (83%)]\tLoss: 0.848792\n",
      "33582\tTrain Epoch: 20 [0/9730 (0%)]\tLoss: 0.798370\n",
      "33582\tTrain Epoch: 20 [1200/9730 (17%)]\tLoss: 0.762087\n",
      "33582\tTrain Epoch: 20 [2400/9730 (33%)]\tLoss: 0.709305\n",
      "33582\tTrain Epoch: 20 [3600/9730 (50%)]\tLoss: 0.846400\n",
      "33582\tTrain Epoch: 20 [4800/9730 (67%)]\tLoss: 0.889375\n",
      "33582\tTrain Epoch: 20 [6000/9730 (83%)]\tLoss: 0.763530\n",
      "33582\tTrain Epoch: 21 [0/9730 (0%)]\tLoss: 0.752473\n",
      "33582\tTrain Epoch: 21 [1200/9730 (17%)]\tLoss: 0.850614\n",
      "33582\tTrain Epoch: 21 [2400/9730 (33%)]\tLoss: 0.833411\n",
      "33582\tTrain Epoch: 21 [3600/9730 (50%)]\tLoss: 0.811430\n",
      "33582\tTrain Epoch: 21 [4800/9730 (67%)]\tLoss: 0.822853\n",
      "33582\tTrain Epoch: 21 [6000/9730 (83%)]\tLoss: 0.769569\n",
      "33582\tTrain Epoch: 22 [0/9730 (0%)]\tLoss: 0.790757\n",
      "33582\tTrain Epoch: 22 [1200/9730 (17%)]\tLoss: 0.761443\n",
      "33582\tTrain Epoch: 22 [2400/9730 (33%)]\tLoss: 0.793849\n",
      "33582\tTrain Epoch: 22 [3600/9730 (50%)]\tLoss: 0.819713\n",
      "33582\tTrain Epoch: 22 [4800/9730 (67%)]\tLoss: 0.764602\n",
      "33582\tTrain Epoch: 22 [6000/9730 (83%)]\tLoss: 0.823912\n",
      "33582\tTrain Epoch: 23 [0/9730 (0%)]\tLoss: 0.772469\n",
      "33582\tTrain Epoch: 23 [1200/9730 (17%)]\tLoss: 0.762407\n",
      "33582\tTrain Epoch: 23 [2400/9730 (33%)]\tLoss: 0.723622\n",
      "33582\tTrain Epoch: 23 [3600/9730 (50%)]\tLoss: 0.780029\n",
      "33582\tTrain Epoch: 23 [4800/9730 (67%)]\tLoss: 0.763623\n",
      "33582\tTrain Epoch: 23 [6000/9730 (83%)]\tLoss: 0.931455\n",
      "33582\tTrain Epoch: 24 [0/9730 (0%)]\tLoss: 0.852183\n",
      "33582\tTrain Epoch: 24 [1200/9730 (17%)]\tLoss: 0.795818\n",
      "33582\tTrain Epoch: 24 [2400/9730 (33%)]\tLoss: 0.749921\n",
      "33582\tTrain Epoch: 24 [3600/9730 (50%)]\tLoss: 0.816239\n",
      "33582\tTrain Epoch: 24 [4800/9730 (67%)]\tLoss: 0.768555\n",
      "33582\tTrain Epoch: 24 [6000/9730 (83%)]\tLoss: 0.758799\n",
      "33582\tTrain Epoch: 25 [0/9730 (0%)]\tLoss: 0.770338\n",
      "33582\tTrain Epoch: 25 [1200/9730 (17%)]\tLoss: 0.856480\n",
      "33582\tTrain Epoch: 25 [2400/9730 (33%)]\tLoss: 0.817350\n",
      "33582\tTrain Epoch: 25 [3600/9730 (50%)]\tLoss: 0.789464\n",
      "33582\tTrain Epoch: 25 [4800/9730 (67%)]\tLoss: 0.710209\n",
      "33582\tTrain Epoch: 25 [6000/9730 (83%)]\tLoss: 0.884187\n",
      "33582\tTrain Epoch: 26 [0/9730 (0%)]\tLoss: 0.754655\n",
      "33582\tTrain Epoch: 26 [1200/9730 (17%)]\tLoss: 0.784674\n",
      "33582\tTrain Epoch: 26 [2400/9730 (33%)]\tLoss: 0.745905\n",
      "33582\tTrain Epoch: 26 [3600/9730 (50%)]\tLoss: 0.770605\n",
      "33582\tTrain Epoch: 26 [4800/9730 (67%)]\tLoss: 0.800576\n",
      "33582\tTrain Epoch: 26 [6000/9730 (83%)]\tLoss: 0.725593\n",
      "33582\tTrain Epoch: 27 [0/9730 (0%)]\tLoss: 0.841364\n",
      "33582\tTrain Epoch: 27 [1200/9730 (17%)]\tLoss: 0.863072\n",
      "33582\tTrain Epoch: 27 [2400/9730 (33%)]\tLoss: 0.869898\n",
      "33582\tTrain Epoch: 27 [3600/9730 (50%)]\tLoss: 0.716369\n",
      "33582\tTrain Epoch: 27 [4800/9730 (67%)]\tLoss: 0.810690\n",
      "33582\tTrain Epoch: 27 [6000/9730 (83%)]\tLoss: 0.757789\n",
      "33582\tTrain Epoch: 28 [0/9730 (0%)]\tLoss: 0.818239\n",
      "33582\tTrain Epoch: 28 [1200/9730 (17%)]\tLoss: 0.777584\n",
      "33582\tTrain Epoch: 28 [2400/9730 (33%)]\tLoss: 0.758884\n",
      "33582\tTrain Epoch: 28 [3600/9730 (50%)]\tLoss: 0.720101\n",
      "33582\tTrain Epoch: 28 [4800/9730 (67%)]\tLoss: 0.789577\n",
      "33582\tTrain Epoch: 28 [6000/9730 (83%)]\tLoss: 0.731808\n",
      "33582\tTrain Epoch: 29 [0/9730 (0%)]\tLoss: 0.720334\n",
      "33582\tTrain Epoch: 29 [1200/9730 (17%)]\tLoss: 0.740549\n",
      "33582\tTrain Epoch: 29 [2400/9730 (33%)]\tLoss: 0.716135\n",
      "33582\tTrain Epoch: 29 [3600/9730 (50%)]\tLoss: 0.804218\n",
      "33582\tTrain Epoch: 29 [4800/9730 (67%)]\tLoss: 0.736428\n",
      "33582\tTrain Epoch: 29 [6000/9730 (83%)]\tLoss: 0.764804\n",
      "33582\tTrain Epoch: 30 [0/9730 (0%)]\tLoss: 0.701832\n",
      "33582\tTrain Epoch: 30 [1200/9730 (17%)]\tLoss: 0.677290\n",
      "33582\tTrain Epoch: 30 [2400/9730 (33%)]\tLoss: 0.768086\n",
      "33582\tTrain Epoch: 30 [3600/9730 (50%)]\tLoss: 0.727141\n",
      "33582\tTrain Epoch: 30 [4800/9730 (67%)]\tLoss: 0.725300\n",
      "33582\tTrain Epoch: 30 [6000/9730 (83%)]\tLoss: 0.785734\n",
      "33582\tTrain Epoch: 31 [0/9730 (0%)]\tLoss: 0.657646\n",
      "33582\tTrain Epoch: 31 [1200/9730 (17%)]\tLoss: 0.713498\n",
      "33582\tTrain Epoch: 31 [2400/9730 (33%)]\tLoss: 0.769488\n",
      "33582\tTrain Epoch: 31 [3600/9730 (50%)]\tLoss: 0.716083\n",
      "33582\tTrain Epoch: 31 [4800/9730 (67%)]\tLoss: 0.770528\n",
      "33582\tTrain Epoch: 31 [6000/9730 (83%)]\tLoss: 0.783778\n",
      "33582\tTrain Epoch: 32 [0/9730 (0%)]\tLoss: 0.682861\n",
      "33582\tTrain Epoch: 32 [1200/9730 (17%)]\tLoss: 0.754590\n",
      "33582\tTrain Epoch: 32 [2400/9730 (33%)]\tLoss: 0.701336\n",
      "33582\tTrain Epoch: 32 [3600/9730 (50%)]\tLoss: 0.718975\n",
      "33582\tTrain Epoch: 32 [4800/9730 (67%)]\tLoss: 0.730331\n",
      "33582\tTrain Epoch: 32 [6000/9730 (83%)]\tLoss: 0.761470\n",
      "33582\tTrain Epoch: 33 [0/9730 (0%)]\tLoss: 0.748658\n",
      "33582\tTrain Epoch: 33 [1200/9730 (17%)]\tLoss: 0.681067\n",
      "33582\tTrain Epoch: 33 [2400/9730 (33%)]\tLoss: 0.847929\n",
      "33582\tTrain Epoch: 33 [3600/9730 (50%)]\tLoss: 0.836736\n",
      "33582\tTrain Epoch: 33 [4800/9730 (67%)]\tLoss: 0.710448\n",
      "33582\tTrain Epoch: 33 [6000/9730 (83%)]\tLoss: 0.726831\n",
      "33582\tTrain Epoch: 34 [0/9730 (0%)]\tLoss: 0.699076\n",
      "33582\tTrain Epoch: 34 [1200/9730 (17%)]\tLoss: 0.814251\n",
      "33582\tTrain Epoch: 34 [2400/9730 (33%)]\tLoss: 0.748731\n",
      "33582\tTrain Epoch: 34 [3600/9730 (50%)]\tLoss: 0.745750\n",
      "33582\tTrain Epoch: 34 [4800/9730 (67%)]\tLoss: 0.690050\n",
      "33582\tTrain Epoch: 34 [6000/9730 (83%)]\tLoss: 0.703946\n",
      "33582\tTrain Epoch: 35 [0/9730 (0%)]\tLoss: 0.693997\n",
      "33582\tTrain Epoch: 35 [1200/9730 (17%)]\tLoss: 0.884863\n",
      "33582\tTrain Epoch: 35 [2400/9730 (33%)]\tLoss: 0.696248\n",
      "33582\tTrain Epoch: 35 [3600/9730 (50%)]\tLoss: 0.733819\n",
      "33582\tTrain Epoch: 35 [4800/9730 (67%)]\tLoss: 0.711916\n",
      "33582\tTrain Epoch: 35 [6000/9730 (83%)]\tLoss: 0.688761\n",
      "33582\tTrain Epoch: 36 [0/9730 (0%)]\tLoss: 0.659040\n",
      "33582\tTrain Epoch: 36 [1200/9730 (17%)]\tLoss: 0.732175\n",
      "33582\tTrain Epoch: 36 [2400/9730 (33%)]\tLoss: 0.742874\n",
      "33582\tTrain Epoch: 36 [3600/9730 (50%)]\tLoss: 0.846316\n",
      "33582\tTrain Epoch: 36 [4800/9730 (67%)]\tLoss: 0.714009\n",
      "33582\tTrain Epoch: 36 [6000/9730 (83%)]\tLoss: 0.713069\n",
      "33582\tTrain Epoch: 37 [0/9730 (0%)]\tLoss: 0.705523\n",
      "33582\tTrain Epoch: 37 [1200/9730 (17%)]\tLoss: 0.821899\n",
      "33582\tTrain Epoch: 37 [2400/9730 (33%)]\tLoss: 0.707081\n",
      "33582\tTrain Epoch: 37 [3600/9730 (50%)]\tLoss: 0.796644\n",
      "33582\tTrain Epoch: 37 [4800/9730 (67%)]\tLoss: 0.708230\n",
      "33582\tTrain Epoch: 37 [6000/9730 (83%)]\tLoss: 0.662847\n",
      "33582\tTrain Epoch: 38 [0/9730 (0%)]\tLoss: 0.715843\n",
      "33582\tTrain Epoch: 38 [1200/9730 (17%)]\tLoss: 0.730414\n",
      "33582\tTrain Epoch: 38 [2400/9730 (33%)]\tLoss: 0.640308\n",
      "33582\tTrain Epoch: 38 [3600/9730 (50%)]\tLoss: 0.821124\n",
      "33582\tTrain Epoch: 38 [4800/9730 (67%)]\tLoss: 0.763131\n",
      "33582\tTrain Epoch: 38 [6000/9730 (83%)]\tLoss: 0.673082\n",
      "33582\tTrain Epoch: 39 [0/9730 (0%)]\tLoss: 0.662348\n",
      "33582\tTrain Epoch: 39 [1200/9730 (17%)]\tLoss: 0.695806\n",
      "33582\tTrain Epoch: 39 [2400/9730 (33%)]\tLoss: 0.669626\n",
      "33582\tTrain Epoch: 39 [3600/9730 (50%)]\tLoss: 0.722486\n",
      "33582\tTrain Epoch: 39 [4800/9730 (67%)]\tLoss: 0.723627\n",
      "33582\tTrain Epoch: 39 [6000/9730 (83%)]\tLoss: 0.721485\n",
      "33582\tTrain Epoch: 40 [0/9730 (0%)]\tLoss: 0.742507\n",
      "33582\tTrain Epoch: 40 [1200/9730 (17%)]\tLoss: 0.745086\n",
      "33582\tTrain Epoch: 40 [2400/9730 (33%)]\tLoss: 0.790480\n",
      "33582\tTrain Epoch: 40 [3600/9730 (50%)]\tLoss: 0.687481\n",
      "33582\tTrain Epoch: 40 [4800/9730 (67%)]\tLoss: 0.752272\n",
      "33582\tTrain Epoch: 40 [6000/9730 (83%)]\tLoss: 0.730115\n",
      "33582\tTrain Epoch: 41 [0/9730 (0%)]\tLoss: 0.658185\n",
      "33582\tTrain Epoch: 41 [1200/9730 (17%)]\tLoss: 0.751082\n",
      "33582\tTrain Epoch: 41 [2400/9730 (33%)]\tLoss: 0.728158\n",
      "33582\tTrain Epoch: 41 [3600/9730 (50%)]\tLoss: 0.686795\n",
      "33582\tTrain Epoch: 41 [4800/9730 (67%)]\tLoss: 0.730742\n",
      "33582\tTrain Epoch: 41 [6000/9730 (83%)]\tLoss: 0.817852\n",
      "33582\tTrain Epoch: 42 [0/9730 (0%)]\tLoss: 0.690848\n",
      "33582\tTrain Epoch: 42 [1200/9730 (17%)]\tLoss: 0.779902\n",
      "33582\tTrain Epoch: 42 [2400/9730 (33%)]\tLoss: 0.662314\n",
      "33582\tTrain Epoch: 42 [3600/9730 (50%)]\tLoss: 0.713210\n",
      "33582\tTrain Epoch: 42 [4800/9730 (67%)]\tLoss: 0.762373\n",
      "33582\tTrain Epoch: 42 [6000/9730 (83%)]\tLoss: 0.720203\n",
      "33582\tTrain Epoch: 43 [0/9730 (0%)]\tLoss: 0.629623\n",
      "33582\tTrain Epoch: 43 [1200/9730 (17%)]\tLoss: 0.766504\n",
      "33582\tTrain Epoch: 43 [2400/9730 (33%)]\tLoss: 0.792416\n",
      "33582\tTrain Epoch: 43 [3600/9730 (50%)]\tLoss: 0.775681\n",
      "33582\tTrain Epoch: 43 [4800/9730 (67%)]\tLoss: 0.760464\n",
      "33582\tTrain Epoch: 43 [6000/9730 (83%)]\tLoss: 0.740101\n",
      "33582\tTrain Epoch: 44 [0/9730 (0%)]\tLoss: 0.730838\n",
      "33582\tTrain Epoch: 44 [1200/9730 (17%)]\tLoss: 0.650648\n",
      "33582\tTrain Epoch: 44 [2400/9730 (33%)]\tLoss: 0.664077\n",
      "33582\tTrain Epoch: 44 [3600/9730 (50%)]\tLoss: 0.634209\n",
      "33582\tTrain Epoch: 44 [4800/9730 (67%)]\tLoss: 0.690808\n",
      "33582\tTrain Epoch: 44 [6000/9730 (83%)]\tLoss: 0.777574\n",
      "33582\tTrain Epoch: 45 [0/9730 (0%)]\tLoss: 0.726577\n",
      "33582\tTrain Epoch: 45 [1200/9730 (17%)]\tLoss: 0.756824\n",
      "33582\tTrain Epoch: 45 [2400/9730 (33%)]\tLoss: 0.665290\n",
      "33582\tTrain Epoch: 45 [3600/9730 (50%)]\tLoss: 0.686041\n",
      "33582\tTrain Epoch: 45 [4800/9730 (67%)]\tLoss: 0.676622\n",
      "33582\tTrain Epoch: 45 [6000/9730 (83%)]\tLoss: 0.816055\n",
      "33582\tTrain Epoch: 46 [0/9730 (0%)]\tLoss: 0.713249\n",
      "33582\tTrain Epoch: 46 [1200/9730 (17%)]\tLoss: 0.648256\n",
      "33582\tTrain Epoch: 46 [2400/9730 (33%)]\tLoss: 0.654823\n",
      "33582\tTrain Epoch: 46 [3600/9730 (50%)]\tLoss: 0.763894\n",
      "33582\tTrain Epoch: 46 [4800/9730 (67%)]\tLoss: 0.701938\n",
      "33582\tTrain Epoch: 46 [6000/9730 (83%)]\tLoss: 0.599140\n",
      "33582\tTrain Epoch: 47 [0/9730 (0%)]\tLoss: 0.681854\n",
      "33582\tTrain Epoch: 47 [1200/9730 (17%)]\tLoss: 0.668273\n",
      "33582\tTrain Epoch: 47 [2400/9730 (33%)]\tLoss: 0.633960\n",
      "33582\tTrain Epoch: 47 [3600/9730 (50%)]\tLoss: 0.799883\n",
      "33582\tTrain Epoch: 47 [4800/9730 (67%)]\tLoss: 0.730540\n",
      "33582\tTrain Epoch: 47 [6000/9730 (83%)]\tLoss: 0.678379\n",
      "33582\tTrain Epoch: 48 [0/9730 (0%)]\tLoss: 0.644811\n",
      "33582\tTrain Epoch: 48 [1200/9730 (17%)]\tLoss: 0.668372\n",
      "33582\tTrain Epoch: 48 [2400/9730 (33%)]\tLoss: 0.739301\n",
      "33582\tTrain Epoch: 48 [3600/9730 (50%)]\tLoss: 0.744973\n",
      "33582\tTrain Epoch: 48 [4800/9730 (67%)]\tLoss: 0.682285\n",
      "33582\tTrain Epoch: 48 [6000/9730 (83%)]\tLoss: 0.744265\n",
      "33582\tTrain Epoch: 49 [0/9730 (0%)]\tLoss: 0.624034\n",
      "33582\tTrain Epoch: 49 [1200/9730 (17%)]\tLoss: 0.713707\n",
      "33582\tTrain Epoch: 49 [2400/9730 (33%)]\tLoss: 0.583845\n",
      "33582\tTrain Epoch: 49 [3600/9730 (50%)]\tLoss: 0.708118\n",
      "33582\tTrain Epoch: 49 [4800/9730 (67%)]\tLoss: 0.663155\n",
      "33582\tTrain Epoch: 49 [6000/9730 (83%)]\tLoss: 0.581476\n",
      "33582\tTrain Epoch: 50 [0/9730 (0%)]\tLoss: 0.713202\n",
      "33582\tTrain Epoch: 50 [1200/9730 (17%)]\tLoss: 0.690013\n",
      "33582\tTrain Epoch: 50 [2400/9730 (33%)]\tLoss: 0.698670\n",
      "33582\tTrain Epoch: 50 [3600/9730 (50%)]\tLoss: 0.724325\n",
      "33582\tTrain Epoch: 50 [4800/9730 (67%)]\tLoss: 0.573721\n",
      "33582\tTrain Epoch: 50 [6000/9730 (83%)]\tLoss: 0.715351\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "test(model=willyAlexNet, device=device, test_loaderIn=train_loader)\n",
    "test(model=willyAlexNet, device=device, test_loaderIn=valid_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 0.4114, Accuracy: 5764/9730 (59%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1484, Accuracy: 834/9730 (9%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from MileStone_3_Create_Neural_Network.WillysShuffleNet_v2_x1_0 import myShuffleNet_v2_x1_0\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "willyShuffleNet = myShuffleNet_v2_x1_0()\n",
    "\n",
    "# for param in willyAlexNet.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in willyAlexNet.WillyAlexNet.classifier[-1].parameters():\n",
    "    # param.requires_grad = True\n",
    "summary(willyShuffleNet, input_size=(batch_size,3,198,198))\n",
    "#print(willyAlexNet)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "├─ShuffleNetV2: 1-1                           [400, 18]                 --\n",
       "│    └─Sequential: 2-1                        [400, 24, 99, 99]         --\n",
       "│    │    └─Conv2d: 3-1                       [400, 24, 99, 99]         648\n",
       "│    │    └─BatchNorm2d: 3-2                  [400, 24, 99, 99]         48\n",
       "│    │    └─ReLU: 3-3                         [400, 24, 99, 99]         --\n",
       "│    └─MaxPool2d: 2-2                         [400, 24, 50, 50]         --\n",
       "│    └─Sequential: 2-3                        [400, 48, 25, 25]         --\n",
       "│    │    └─InvertedResidual: 3-4             [400, 48, 25, 25]         2,400\n",
       "│    │    └─InvertedResidual: 3-5             [400, 48, 25, 25]         1,512\n",
       "│    │    └─InvertedResidual: 3-6             [400, 48, 25, 25]         1,512\n",
       "│    │    └─InvertedResidual: 3-7             [400, 48, 25, 25]         1,512\n",
       "│    └─Sequential: 2-4                        [400, 96, 13, 13]         --\n",
       "│    │    └─InvertedResidual: 3-8             [400, 96, 13, 13]         8,256\n",
       "│    │    └─InvertedResidual: 3-9             [400, 96, 13, 13]         5,328\n",
       "│    │    └─InvertedResidual: 3-10            [400, 96, 13, 13]         5,328\n",
       "│    │    └─InvertedResidual: 3-11            [400, 96, 13, 13]         5,328\n",
       "│    │    └─InvertedResidual: 3-12            [400, 96, 13, 13]         5,328\n",
       "│    │    └─InvertedResidual: 3-13            [400, 96, 13, 13]         5,328\n",
       "│    │    └─InvertedResidual: 3-14            [400, 96, 13, 13]         5,328\n",
       "│    │    └─InvertedResidual: 3-15            [400, 96, 13, 13]         5,328\n",
       "│    └─Sequential: 2-5                        [400, 192, 7, 7]          --\n",
       "│    │    └─InvertedResidual: 3-16            [400, 192, 7, 7]          30,336\n",
       "│    │    └─InvertedResidual: 3-17            [400, 192, 7, 7]          19,872\n",
       "│    │    └─InvertedResidual: 3-18            [400, 192, 7, 7]          19,872\n",
       "│    │    └─InvertedResidual: 3-19            [400, 192, 7, 7]          19,872\n",
       "│    └─Sequential: 2-6                        [400, 1024, 7, 7]         --\n",
       "│    │    └─Conv2d: 3-20                      [400, 1024, 7, 7]         196,608\n",
       "│    │    └─BatchNorm2d: 3-21                 [400, 1024, 7, 7]         2,048\n",
       "│    │    └─ReLU: 3-22                        [400, 1024, 7, 7]         --\n",
       "│    └─Linear: 2-7                            [400, 18]                 18,450\n",
       "===============================================================================================\n",
       "Total params: 360,242\n",
       "Trainable params: 360,242\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 13.89\n",
       "===============================================================================================\n",
       "Input size (MB): 188.18\n",
       "Forward/backward pass size (MB): 5443.74\n",
       "Params size (MB): 1.44\n",
       "Estimated Total Size (MB): 5633.36\n",
       "==============================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 50, \"log_interval\": 3}\n",
    "device = torch.device('cuda:0')\n",
    "willyShuffleNet.to(device)\n",
    "willyShuffleNet.share_memory()\n",
    "\n",
    "test(model=willyShuffleNet, device=device, test_loaderIn=train_loader)\n",
    "test(model=willyShuffleNet, device=device, test_loaderIn=valid_loader)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 2.1400, Accuracy: 432/9730 (4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3572, Accuracy: 69/9730 (1%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 50, \"log_interval\": 3}\n",
    "train(model=willyShuffleNet, device=device, train_loaderIn=train_loader, args=args)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "76559\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: 2.892886\n",
      "76559\tTrain Epoch: 1 [1200/9730 (17%)]\tLoss: 2.891061\n",
      "76559\tTrain Epoch: 1 [2400/9730 (33%)]\tLoss: 2.887151\n",
      "76559\tTrain Epoch: 1 [3600/9730 (50%)]\tLoss: 2.883877\n",
      "76559\tTrain Epoch: 1 [4800/9730 (67%)]\tLoss: 2.881113\n",
      "76559\tTrain Epoch: 1 [6000/9730 (83%)]\tLoss: 2.878371\n",
      "76559\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: 2.875867\n",
      "76559\tTrain Epoch: 2 [1200/9730 (17%)]\tLoss: 2.872226\n",
      "76559\tTrain Epoch: 2 [2400/9730 (33%)]\tLoss: 2.869443\n",
      "76559\tTrain Epoch: 2 [3600/9730 (50%)]\tLoss: 2.868200\n",
      "76559\tTrain Epoch: 2 [4800/9730 (67%)]\tLoss: 2.865467\n",
      "76559\tTrain Epoch: 2 [6000/9730 (83%)]\tLoss: 2.860615\n",
      "76559\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: 2.858515\n",
      "76559\tTrain Epoch: 3 [1200/9730 (17%)]\tLoss: 2.854532\n",
      "76559\tTrain Epoch: 3 [2400/9730 (33%)]\tLoss: 2.853480\n",
      "76559\tTrain Epoch: 3 [3600/9730 (50%)]\tLoss: 2.850220\n",
      "76559\tTrain Epoch: 3 [4800/9730 (67%)]\tLoss: 2.848416\n",
      "76559\tTrain Epoch: 3 [6000/9730 (83%)]\tLoss: 2.847564\n",
      "76559\tTrain Epoch: 4 [0/9730 (0%)]\tLoss: 2.843347\n",
      "76559\tTrain Epoch: 4 [1200/9730 (17%)]\tLoss: 2.841827\n",
      "76559\tTrain Epoch: 4 [2400/9730 (33%)]\tLoss: 2.840917\n",
      "76559\tTrain Epoch: 4 [3600/9730 (50%)]\tLoss: 2.836659\n",
      "76559\tTrain Epoch: 4 [4800/9730 (67%)]\tLoss: 2.835815\n",
      "76559\tTrain Epoch: 4 [6000/9730 (83%)]\tLoss: 2.836104\n",
      "76559\tTrain Epoch: 5 [0/9730 (0%)]\tLoss: 2.823370\n",
      "76559\tTrain Epoch: 5 [1200/9730 (17%)]\tLoss: 2.828373\n",
      "76559\tTrain Epoch: 5 [2400/9730 (33%)]\tLoss: 2.824117\n",
      "76559\tTrain Epoch: 5 [3600/9730 (50%)]\tLoss: 2.825079\n",
      "76559\tTrain Epoch: 5 [4800/9730 (67%)]\tLoss: 2.821140\n",
      "76559\tTrain Epoch: 5 [6000/9730 (83%)]\tLoss: 2.819074\n",
      "76559\tTrain Epoch: 6 [0/9730 (0%)]\tLoss: 2.815628\n",
      "76559\tTrain Epoch: 6 [1200/9730 (17%)]\tLoss: 2.815144\n",
      "76559\tTrain Epoch: 6 [2400/9730 (33%)]\tLoss: 2.815238\n",
      "76559\tTrain Epoch: 6 [3600/9730 (50%)]\tLoss: 2.811755\n",
      "76559\tTrain Epoch: 6 [4800/9730 (67%)]\tLoss: 2.802126\n",
      "76559\tTrain Epoch: 6 [6000/9730 (83%)]\tLoss: 2.802989\n",
      "76559\tTrain Epoch: 7 [0/9730 (0%)]\tLoss: 2.794370\n",
      "76559\tTrain Epoch: 7 [1200/9730 (17%)]\tLoss: 2.791355\n",
      "76559\tTrain Epoch: 7 [2400/9730 (33%)]\tLoss: 2.796239\n",
      "76559\tTrain Epoch: 7 [3600/9730 (50%)]\tLoss: 2.793256\n",
      "76559\tTrain Epoch: 7 [4800/9730 (67%)]\tLoss: 2.795506\n",
      "76559\tTrain Epoch: 7 [6000/9730 (83%)]\tLoss: 2.799661\n",
      "76559\tTrain Epoch: 8 [0/9730 (0%)]\tLoss: 2.784999\n",
      "76559\tTrain Epoch: 8 [1200/9730 (17%)]\tLoss: 2.786787\n",
      "76559\tTrain Epoch: 8 [2400/9730 (33%)]\tLoss: 2.782947\n",
      "76559\tTrain Epoch: 8 [3600/9730 (50%)]\tLoss: 2.781613\n",
      "76559\tTrain Epoch: 8 [4800/9730 (67%)]\tLoss: 2.782813\n",
      "76559\tTrain Epoch: 8 [6000/9730 (83%)]\tLoss: 2.786491\n",
      "76559\tTrain Epoch: 9 [0/9730 (0%)]\tLoss: 2.795110\n",
      "76559\tTrain Epoch: 9 [1200/9730 (17%)]\tLoss: 2.786128\n",
      "76559\tTrain Epoch: 9 [2400/9730 (33%)]\tLoss: 2.779983\n",
      "76559\tTrain Epoch: 9 [3600/9730 (50%)]\tLoss: 2.758843\n",
      "76559\tTrain Epoch: 9 [4800/9730 (67%)]\tLoss: 2.764565\n",
      "76559\tTrain Epoch: 9 [6000/9730 (83%)]\tLoss: 2.767960\n",
      "76559\tTrain Epoch: 10 [0/9730 (0%)]\tLoss: 2.774015\n",
      "76559\tTrain Epoch: 10 [1200/9730 (17%)]\tLoss: 2.764206\n",
      "76559\tTrain Epoch: 10 [2400/9730 (33%)]\tLoss: 2.763938\n",
      "76559\tTrain Epoch: 10 [3600/9730 (50%)]\tLoss: 2.754933\n",
      "76559\tTrain Epoch: 10 [4800/9730 (67%)]\tLoss: 2.745948\n",
      "76559\tTrain Epoch: 10 [6000/9730 (83%)]\tLoss: 2.758413\n",
      "76559\tTrain Epoch: 11 [0/9730 (0%)]\tLoss: 2.759876\n",
      "76559\tTrain Epoch: 11 [1200/9730 (17%)]\tLoss: 2.754014\n",
      "76559\tTrain Epoch: 11 [2400/9730 (33%)]\tLoss: 2.746647\n",
      "76559\tTrain Epoch: 11 [3600/9730 (50%)]\tLoss: 2.751547\n",
      "76559\tTrain Epoch: 11 [4800/9730 (67%)]\tLoss: 2.750723\n",
      "76559\tTrain Epoch: 11 [6000/9730 (83%)]\tLoss: 2.748107\n",
      "76559\tTrain Epoch: 12 [0/9730 (0%)]\tLoss: 2.747511\n",
      "76559\tTrain Epoch: 12 [1200/9730 (17%)]\tLoss: 2.747927\n",
      "76559\tTrain Epoch: 12 [2400/9730 (33%)]\tLoss: 2.750361\n",
      "76559\tTrain Epoch: 12 [3600/9730 (50%)]\tLoss: 2.733073\n",
      "76559\tTrain Epoch: 12 [4800/9730 (67%)]\tLoss: 2.726247\n",
      "76559\tTrain Epoch: 12 [6000/9730 (83%)]\tLoss: 2.736336\n",
      "76559\tTrain Epoch: 13 [0/9730 (0%)]\tLoss: 2.734874\n",
      "76559\tTrain Epoch: 13 [1200/9730 (17%)]\tLoss: 2.725936\n",
      "76559\tTrain Epoch: 13 [2400/9730 (33%)]\tLoss: 2.736697\n",
      "76559\tTrain Epoch: 13 [3600/9730 (50%)]\tLoss: 2.728788\n",
      "76559\tTrain Epoch: 13 [4800/9730 (67%)]\tLoss: 2.736508\n",
      "76559\tTrain Epoch: 13 [6000/9730 (83%)]\tLoss: 2.729784\n",
      "76559\tTrain Epoch: 14 [0/9730 (0%)]\tLoss: 2.720322\n",
      "76559\tTrain Epoch: 14 [1200/9730 (17%)]\tLoss: 2.736023\n",
      "76559\tTrain Epoch: 14 [2400/9730 (33%)]\tLoss: 2.731247\n",
      "76559\tTrain Epoch: 14 [3600/9730 (50%)]\tLoss: 2.705568\n",
      "76559\tTrain Epoch: 14 [4800/9730 (67%)]\tLoss: 2.711396\n",
      "76559\tTrain Epoch: 14 [6000/9730 (83%)]\tLoss: 2.709577\n",
      "76559\tTrain Epoch: 15 [0/9730 (0%)]\tLoss: 2.721873\n",
      "76559\tTrain Epoch: 15 [1200/9730 (17%)]\tLoss: 2.714375\n",
      "76559\tTrain Epoch: 15 [2400/9730 (33%)]\tLoss: 2.720841\n",
      "76559\tTrain Epoch: 15 [3600/9730 (50%)]\tLoss: 2.709934\n",
      "76559\tTrain Epoch: 15 [4800/9730 (67%)]\tLoss: 2.696666\n",
      "76559\tTrain Epoch: 15 [6000/9730 (83%)]\tLoss: 2.702310\n",
      "76559\tTrain Epoch: 16 [0/9730 (0%)]\tLoss: 2.705669\n",
      "76559\tTrain Epoch: 16 [1200/9730 (17%)]\tLoss: 2.701431\n",
      "76559\tTrain Epoch: 16 [2400/9730 (33%)]\tLoss: 2.712241\n",
      "76559\tTrain Epoch: 16 [3600/9730 (50%)]\tLoss: 2.701419\n",
      "76559\tTrain Epoch: 16 [4800/9730 (67%)]\tLoss: 2.704534\n",
      "76559\tTrain Epoch: 16 [6000/9730 (83%)]\tLoss: 2.703000\n",
      "76559\tTrain Epoch: 17 [0/9730 (0%)]\tLoss: 2.678273\n",
      "76559\tTrain Epoch: 17 [1200/9730 (17%)]\tLoss: 2.699387\n",
      "76559\tTrain Epoch: 17 [2400/9730 (33%)]\tLoss: 2.699723\n",
      "76559\tTrain Epoch: 17 [3600/9730 (50%)]\tLoss: 2.684453\n",
      "76559\tTrain Epoch: 17 [4800/9730 (67%)]\tLoss: 2.689545\n",
      "76559\tTrain Epoch: 17 [6000/9730 (83%)]\tLoss: 2.699796\n",
      "76559\tTrain Epoch: 18 [0/9730 (0%)]\tLoss: 2.688649\n",
      "76559\tTrain Epoch: 18 [1200/9730 (17%)]\tLoss: 2.697883\n",
      "76559\tTrain Epoch: 18 [2400/9730 (33%)]\tLoss: 2.698537\n",
      "76559\tTrain Epoch: 18 [3600/9730 (50%)]\tLoss: 2.670185\n",
      "76559\tTrain Epoch: 18 [4800/9730 (67%)]\tLoss: 2.687861\n",
      "76559\tTrain Epoch: 18 [6000/9730 (83%)]\tLoss: 2.690402\n",
      "76559\tTrain Epoch: 19 [0/9730 (0%)]\tLoss: 2.671666\n",
      "76559\tTrain Epoch: 19 [1200/9730 (17%)]\tLoss: 2.677340\n",
      "76559\tTrain Epoch: 19 [2400/9730 (33%)]\tLoss: 2.666870\n",
      "76559\tTrain Epoch: 19 [3600/9730 (50%)]\tLoss: 2.689125\n",
      "76559\tTrain Epoch: 19 [4800/9730 (67%)]\tLoss: 2.670540\n",
      "76559\tTrain Epoch: 19 [6000/9730 (83%)]\tLoss: 2.681298\n",
      "76559\tTrain Epoch: 20 [0/9730 (0%)]\tLoss: 2.662439\n",
      "76559\tTrain Epoch: 20 [1200/9730 (17%)]\tLoss: 2.672084\n",
      "76559\tTrain Epoch: 20 [2400/9730 (33%)]\tLoss: 2.660216\n",
      "76559\tTrain Epoch: 20 [3600/9730 (50%)]\tLoss: 2.668956\n",
      "76559\tTrain Epoch: 20 [4800/9730 (67%)]\tLoss: 2.658855\n",
      "76559\tTrain Epoch: 20 [6000/9730 (83%)]\tLoss: 2.662513\n",
      "76559\tTrain Epoch: 21 [0/9730 (0%)]\tLoss: 2.657710\n",
      "76559\tTrain Epoch: 21 [1200/9730 (17%)]\tLoss: 2.668633\n",
      "76559\tTrain Epoch: 21 [2400/9730 (33%)]\tLoss: 2.659718\n",
      "76559\tTrain Epoch: 21 [3600/9730 (50%)]\tLoss: 2.675771\n",
      "76559\tTrain Epoch: 21 [4800/9730 (67%)]\tLoss: 2.653207\n",
      "76559\tTrain Epoch: 21 [6000/9730 (83%)]\tLoss: 2.661219\n",
      "76559\tTrain Epoch: 22 [0/9730 (0%)]\tLoss: 2.661516\n",
      "76559\tTrain Epoch: 22 [1200/9730 (17%)]\tLoss: 2.668863\n",
      "76559\tTrain Epoch: 22 [2400/9730 (33%)]\tLoss: 2.658164\n",
      "76559\tTrain Epoch: 22 [3600/9730 (50%)]\tLoss: 2.670414\n",
      "76559\tTrain Epoch: 22 [4800/9730 (67%)]\tLoss: 2.629220\n",
      "76559\tTrain Epoch: 22 [6000/9730 (83%)]\tLoss: 2.654422\n",
      "76559\tTrain Epoch: 23 [0/9730 (0%)]\tLoss: 2.668327\n",
      "76559\tTrain Epoch: 23 [1200/9730 (17%)]\tLoss: 2.665279\n",
      "76559\tTrain Epoch: 23 [2400/9730 (33%)]\tLoss: 2.640432\n",
      "76559\tTrain Epoch: 23 [3600/9730 (50%)]\tLoss: 2.664961\n",
      "76559\tTrain Epoch: 23 [4800/9730 (67%)]\tLoss: 2.626309\n",
      "76559\tTrain Epoch: 23 [6000/9730 (83%)]\tLoss: 2.648904\n",
      "76559\tTrain Epoch: 24 [0/9730 (0%)]\tLoss: 2.656788\n",
      "76559\tTrain Epoch: 24 [1200/9730 (17%)]\tLoss: 2.658717\n",
      "76559\tTrain Epoch: 24 [2400/9730 (33%)]\tLoss: 2.628850\n",
      "76559\tTrain Epoch: 24 [3600/9730 (50%)]\tLoss: 2.640935\n",
      "76559\tTrain Epoch: 24 [4800/9730 (67%)]\tLoss: 2.650471\n",
      "76559\tTrain Epoch: 24 [6000/9730 (83%)]\tLoss: 2.652717\n",
      "76559\tTrain Epoch: 25 [0/9730 (0%)]\tLoss: 2.640498\n",
      "76559\tTrain Epoch: 25 [1200/9730 (17%)]\tLoss: 2.638649\n",
      "76559\tTrain Epoch: 25 [2400/9730 (33%)]\tLoss: 2.640413\n",
      "76559\tTrain Epoch: 25 [3600/9730 (50%)]\tLoss: 2.648809\n",
      "76559\tTrain Epoch: 25 [4800/9730 (67%)]\tLoss: 2.632556\n",
      "76559\tTrain Epoch: 25 [6000/9730 (83%)]\tLoss: 2.651158\n",
      "76559\tTrain Epoch: 26 [0/9730 (0%)]\tLoss: 2.624236\n",
      "76559\tTrain Epoch: 26 [1200/9730 (17%)]\tLoss: 2.633323\n",
      "76559\tTrain Epoch: 26 [2400/9730 (33%)]\tLoss: 2.634199\n",
      "76559\tTrain Epoch: 26 [3600/9730 (50%)]\tLoss: 2.634956\n",
      "76559\tTrain Epoch: 26 [4800/9730 (67%)]\tLoss: 2.631957\n",
      "76559\tTrain Epoch: 26 [6000/9730 (83%)]\tLoss: 2.633963\n",
      "76559\tTrain Epoch: 27 [0/9730 (0%)]\tLoss: 2.654443\n",
      "76559\tTrain Epoch: 27 [1200/9730 (17%)]\tLoss: 2.610194\n",
      "76559\tTrain Epoch: 27 [2400/9730 (33%)]\tLoss: 2.590268\n",
      "76559\tTrain Epoch: 27 [3600/9730 (50%)]\tLoss: 2.624243\n",
      "76559\tTrain Epoch: 27 [4800/9730 (67%)]\tLoss: 2.604222\n",
      "76559\tTrain Epoch: 27 [6000/9730 (83%)]\tLoss: 2.629443\n",
      "76559\tTrain Epoch: 28 [0/9730 (0%)]\tLoss: 2.619310\n",
      "76559\tTrain Epoch: 28 [1200/9730 (17%)]\tLoss: 2.628955\n",
      "76559\tTrain Epoch: 28 [2400/9730 (33%)]\tLoss: 2.642521\n",
      "76559\tTrain Epoch: 28 [3600/9730 (50%)]\tLoss: 2.615736\n",
      "76559\tTrain Epoch: 28 [4800/9730 (67%)]\tLoss: 2.626621\n",
      "76559\tTrain Epoch: 28 [6000/9730 (83%)]\tLoss: 2.635001\n",
      "76559\tTrain Epoch: 29 [0/9730 (0%)]\tLoss: 2.596208\n",
      "76559\tTrain Epoch: 29 [1200/9730 (17%)]\tLoss: 2.610970\n",
      "76559\tTrain Epoch: 29 [2400/9730 (33%)]\tLoss: 2.631477\n",
      "76559\tTrain Epoch: 29 [3600/9730 (50%)]\tLoss: 2.631614\n",
      "76559\tTrain Epoch: 29 [4800/9730 (67%)]\tLoss: 2.617397\n",
      "76559\tTrain Epoch: 29 [6000/9730 (83%)]\tLoss: 2.611943\n",
      "76559\tTrain Epoch: 30 [0/9730 (0%)]\tLoss: 2.621925\n",
      "76559\tTrain Epoch: 30 [1200/9730 (17%)]\tLoss: 2.599422\n",
      "76559\tTrain Epoch: 30 [2400/9730 (33%)]\tLoss: 2.634575\n",
      "76559\tTrain Epoch: 30 [3600/9730 (50%)]\tLoss: 2.626314\n",
      "76559\tTrain Epoch: 30 [4800/9730 (67%)]\tLoss: 2.623093\n",
      "76559\tTrain Epoch: 30 [6000/9730 (83%)]\tLoss: 2.625242\n",
      "76559\tTrain Epoch: 31 [0/9730 (0%)]\tLoss: 2.593224\n",
      "76559\tTrain Epoch: 31 [1200/9730 (17%)]\tLoss: 2.612402\n",
      "76559\tTrain Epoch: 31 [2400/9730 (33%)]\tLoss: 2.610423\n",
      "76559\tTrain Epoch: 31 [3600/9730 (50%)]\tLoss: 2.588088\n",
      "76559\tTrain Epoch: 31 [4800/9730 (67%)]\tLoss: 2.615151\n",
      "76559\tTrain Epoch: 31 [6000/9730 (83%)]\tLoss: 2.600782\n",
      "76559\tTrain Epoch: 32 [0/9730 (0%)]\tLoss: 2.602876\n",
      "76559\tTrain Epoch: 32 [1200/9730 (17%)]\tLoss: 2.609990\n",
      "76559\tTrain Epoch: 32 [2400/9730 (33%)]\tLoss: 2.619177\n",
      "76559\tTrain Epoch: 32 [3600/9730 (50%)]\tLoss: 2.625466\n",
      "76559\tTrain Epoch: 32 [4800/9730 (67%)]\tLoss: 2.596804\n",
      "76559\tTrain Epoch: 32 [6000/9730 (83%)]\tLoss: 2.599970\n",
      "76559\tTrain Epoch: 33 [0/9730 (0%)]\tLoss: 2.582666\n",
      "76559\tTrain Epoch: 33 [1200/9730 (17%)]\tLoss: 2.615291\n",
      "76559\tTrain Epoch: 33 [2400/9730 (33%)]\tLoss: 2.565335\n",
      "76559\tTrain Epoch: 33 [3600/9730 (50%)]\tLoss: 2.593725\n",
      "76559\tTrain Epoch: 33 [4800/9730 (67%)]\tLoss: 2.588101\n",
      "76559\tTrain Epoch: 33 [6000/9730 (83%)]\tLoss: 2.610904\n",
      "76559\tTrain Epoch: 34 [0/9730 (0%)]\tLoss: 2.587412\n",
      "76559\tTrain Epoch: 34 [1200/9730 (17%)]\tLoss: 2.572270\n",
      "76559\tTrain Epoch: 34 [2400/9730 (33%)]\tLoss: 2.567616\n",
      "76559\tTrain Epoch: 34 [3600/9730 (50%)]\tLoss: 2.592197\n",
      "76559\tTrain Epoch: 34 [4800/9730 (67%)]\tLoss: 2.584455\n",
      "76559\tTrain Epoch: 34 [6000/9730 (83%)]\tLoss: 2.584064\n",
      "76559\tTrain Epoch: 35 [0/9730 (0%)]\tLoss: 2.597880\n",
      "76559\tTrain Epoch: 35 [1200/9730 (17%)]\tLoss: 2.583973\n",
      "76559\tTrain Epoch: 35 [2400/9730 (33%)]\tLoss: 2.569726\n",
      "76559\tTrain Epoch: 35 [3600/9730 (50%)]\tLoss: 2.568362\n",
      "76559\tTrain Epoch: 35 [4800/9730 (67%)]\tLoss: 2.611173\n",
      "76559\tTrain Epoch: 35 [6000/9730 (83%)]\tLoss: 2.595558\n",
      "76559\tTrain Epoch: 36 [0/9730 (0%)]\tLoss: 2.574383\n",
      "76559\tTrain Epoch: 36 [1200/9730 (17%)]\tLoss: 2.623032\n",
      "76559\tTrain Epoch: 36 [2400/9730 (33%)]\tLoss: 2.593417\n",
      "76559\tTrain Epoch: 36 [3600/9730 (50%)]\tLoss: 2.578148\n",
      "76559\tTrain Epoch: 36 [4800/9730 (67%)]\tLoss: 2.583894\n",
      "76559\tTrain Epoch: 36 [6000/9730 (83%)]\tLoss: 2.579381\n",
      "76559\tTrain Epoch: 37 [0/9730 (0%)]\tLoss: 2.590165\n",
      "76559\tTrain Epoch: 37 [1200/9730 (17%)]\tLoss: 2.573953\n",
      "76559\tTrain Epoch: 37 [2400/9730 (33%)]\tLoss: 2.562171\n",
      "76559\tTrain Epoch: 37 [3600/9730 (50%)]\tLoss: 2.555149\n",
      "76559\tTrain Epoch: 37 [4800/9730 (67%)]\tLoss: 2.580190\n",
      "76559\tTrain Epoch: 37 [6000/9730 (83%)]\tLoss: 2.578576\n",
      "76559\tTrain Epoch: 38 [0/9730 (0%)]\tLoss: 2.604230\n",
      "76559\tTrain Epoch: 38 [1200/9730 (17%)]\tLoss: 2.569629\n",
      "76559\tTrain Epoch: 38 [2400/9730 (33%)]\tLoss: 2.555729\n",
      "76559\tTrain Epoch: 38 [3600/9730 (50%)]\tLoss: 2.558915\n",
      "76559\tTrain Epoch: 38 [4800/9730 (67%)]\tLoss: 2.606960\n",
      "76559\tTrain Epoch: 38 [6000/9730 (83%)]\tLoss: 2.579149\n",
      "76559\tTrain Epoch: 39 [0/9730 (0%)]\tLoss: 2.590477\n",
      "76559\tTrain Epoch: 39 [1200/9730 (17%)]\tLoss: 2.554275\n",
      "76559\tTrain Epoch: 39 [2400/9730 (33%)]\tLoss: 2.605932\n",
      "76559\tTrain Epoch: 39 [3600/9730 (50%)]\tLoss: 2.554456\n",
      "76559\tTrain Epoch: 39 [4800/9730 (67%)]\tLoss: 2.591263\n",
      "76559\tTrain Epoch: 39 [6000/9730 (83%)]\tLoss: 2.558158\n",
      "76559\tTrain Epoch: 40 [0/9730 (0%)]\tLoss: 2.580740\n",
      "76559\tTrain Epoch: 40 [1200/9730 (17%)]\tLoss: 2.586331\n",
      "76559\tTrain Epoch: 40 [2400/9730 (33%)]\tLoss: 2.544472\n",
      "76559\tTrain Epoch: 40 [3600/9730 (50%)]\tLoss: 2.555183\n",
      "76559\tTrain Epoch: 40 [4800/9730 (67%)]\tLoss: 2.567235\n",
      "76559\tTrain Epoch: 40 [6000/9730 (83%)]\tLoss: 2.556170\n",
      "76559\tTrain Epoch: 41 [0/9730 (0%)]\tLoss: 2.574510\n",
      "76559\tTrain Epoch: 41 [1200/9730 (17%)]\tLoss: 2.588538\n",
      "76559\tTrain Epoch: 41 [2400/9730 (33%)]\tLoss: 2.568132\n",
      "76559\tTrain Epoch: 41 [3600/9730 (50%)]\tLoss: 2.574080\n",
      "76559\tTrain Epoch: 41 [4800/9730 (67%)]\tLoss: 2.585377\n",
      "76559\tTrain Epoch: 41 [6000/9730 (83%)]\tLoss: 2.562588\n",
      "76559\tTrain Epoch: 42 [0/9730 (0%)]\tLoss: 2.547149\n",
      "76559\tTrain Epoch: 42 [1200/9730 (17%)]\tLoss: 2.577112\n",
      "76559\tTrain Epoch: 42 [2400/9730 (33%)]\tLoss: 2.556176\n",
      "76559\tTrain Epoch: 42 [3600/9730 (50%)]\tLoss: 2.564754\n",
      "76559\tTrain Epoch: 42 [4800/9730 (67%)]\tLoss: 2.544528\n",
      "76559\tTrain Epoch: 42 [6000/9730 (83%)]\tLoss: 2.596894\n",
      "76559\tTrain Epoch: 43 [0/9730 (0%)]\tLoss: 2.559042\n",
      "76559\tTrain Epoch: 43 [1200/9730 (17%)]\tLoss: 2.562143\n",
      "76559\tTrain Epoch: 43 [2400/9730 (33%)]\tLoss: 2.559127\n",
      "76559\tTrain Epoch: 43 [3600/9730 (50%)]\tLoss: 2.515630\n",
      "76559\tTrain Epoch: 43 [4800/9730 (67%)]\tLoss: 2.543067\n",
      "76559\tTrain Epoch: 43 [6000/9730 (83%)]\tLoss: 2.563706\n",
      "76559\tTrain Epoch: 44 [0/9730 (0%)]\tLoss: 2.572246\n",
      "76559\tTrain Epoch: 44 [1200/9730 (17%)]\tLoss: 2.524145\n",
      "76559\tTrain Epoch: 44 [2400/9730 (33%)]\tLoss: 2.565887\n",
      "76559\tTrain Epoch: 44 [3600/9730 (50%)]\tLoss: 2.564106\n",
      "76559\tTrain Epoch: 44 [4800/9730 (67%)]\tLoss: 2.552302\n",
      "76559\tTrain Epoch: 44 [6000/9730 (83%)]\tLoss: 2.545996\n",
      "76559\tTrain Epoch: 45 [0/9730 (0%)]\tLoss: 2.582770\n",
      "76559\tTrain Epoch: 45 [1200/9730 (17%)]\tLoss: 2.529940\n",
      "76559\tTrain Epoch: 45 [2400/9730 (33%)]\tLoss: 2.558280\n",
      "76559\tTrain Epoch: 45 [3600/9730 (50%)]\tLoss: 2.572402\n",
      "76559\tTrain Epoch: 45 [4800/9730 (67%)]\tLoss: 2.578917\n",
      "76559\tTrain Epoch: 45 [6000/9730 (83%)]\tLoss: 2.549326\n",
      "76559\tTrain Epoch: 46 [0/9730 (0%)]\tLoss: 2.547936\n",
      "76559\tTrain Epoch: 46 [1200/9730 (17%)]\tLoss: 2.533113\n",
      "76559\tTrain Epoch: 46 [2400/9730 (33%)]\tLoss: 2.552836\n",
      "76559\tTrain Epoch: 46 [3600/9730 (50%)]\tLoss: 2.550044\n",
      "76559\tTrain Epoch: 46 [4800/9730 (67%)]\tLoss: 2.563663\n",
      "76559\tTrain Epoch: 46 [6000/9730 (83%)]\tLoss: 2.550832\n",
      "76559\tTrain Epoch: 47 [0/9730 (0%)]\tLoss: 2.549737\n",
      "76559\tTrain Epoch: 47 [1200/9730 (17%)]\tLoss: 2.534447\n",
      "76559\tTrain Epoch: 47 [2400/9730 (33%)]\tLoss: 2.560955\n",
      "76559\tTrain Epoch: 47 [3600/9730 (50%)]\tLoss: 2.547753\n",
      "76559\tTrain Epoch: 47 [4800/9730 (67%)]\tLoss: 2.512960\n",
      "76559\tTrain Epoch: 47 [6000/9730 (83%)]\tLoss: 2.549339\n",
      "76559\tTrain Epoch: 48 [0/9730 (0%)]\tLoss: 2.527250\n",
      "76559\tTrain Epoch: 48 [1200/9730 (17%)]\tLoss: 2.543081\n",
      "76559\tTrain Epoch: 48 [2400/9730 (33%)]\tLoss: 2.535979\n",
      "76559\tTrain Epoch: 48 [3600/9730 (50%)]\tLoss: 2.532005\n",
      "76559\tTrain Epoch: 48 [4800/9730 (67%)]\tLoss: 2.518275\n",
      "76559\tTrain Epoch: 48 [6000/9730 (83%)]\tLoss: 2.521432\n",
      "76559\tTrain Epoch: 49 [0/9730 (0%)]\tLoss: 2.516046\n",
      "76559\tTrain Epoch: 49 [1200/9730 (17%)]\tLoss: 2.535680\n",
      "76559\tTrain Epoch: 49 [2400/9730 (33%)]\tLoss: 2.513003\n",
      "76559\tTrain Epoch: 49 [3600/9730 (50%)]\tLoss: 2.510710\n",
      "76559\tTrain Epoch: 49 [4800/9730 (67%)]\tLoss: 2.555775\n",
      "76559\tTrain Epoch: 49 [6000/9730 (83%)]\tLoss: 2.527180\n",
      "76559\tTrain Epoch: 50 [0/9730 (0%)]\tLoss: 2.543588\n",
      "76559\tTrain Epoch: 50 [1200/9730 (17%)]\tLoss: 2.579393\n",
      "76559\tTrain Epoch: 50 [2400/9730 (33%)]\tLoss: 2.543744\n",
      "76559\tTrain Epoch: 50 [3600/9730 (50%)]\tLoss: 2.511978\n",
      "76559\tTrain Epoch: 50 [4800/9730 (67%)]\tLoss: 2.504741\n",
      "76559\tTrain Epoch: 50 [6000/9730 (83%)]\tLoss: 2.545362\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "test(model=willyShuffleNet, device=device, test_loaderIn=train_loader)\n",
    "test(model=willyShuffleNet, device=device, test_loaderIn=valid_loader)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 1.8702, Accuracy: 1208/9730 (12%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3170, Accuracy: 165/9730 (2%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "args = {\"lr\": 0.01, \"momentum\": 0.5, \"epochs\": 50, \"log_interval\": 3}\n",
    "train(model=willyShuffleNet, device=device,\n",
    "      train_loaderIn=train_loader, args=args)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "76559\tTrain Epoch: 1 [0/9730 (0%)]\tLoss: 2.558404\n",
      "76559\tTrain Epoch: 1 [1200/9730 (17%)]\tLoss: 2.498841\n",
      "76559\tTrain Epoch: 1 [2400/9730 (33%)]\tLoss: 2.517309\n",
      "76559\tTrain Epoch: 1 [3600/9730 (50%)]\tLoss: 2.561944\n",
      "76559\tTrain Epoch: 1 [4800/9730 (67%)]\tLoss: 2.557342\n",
      "76559\tTrain Epoch: 1 [6000/9730 (83%)]\tLoss: 2.522970\n",
      "76559\tTrain Epoch: 2 [0/9730 (0%)]\tLoss: 2.528353\n",
      "76559\tTrain Epoch: 2 [1200/9730 (17%)]\tLoss: 2.514053\n",
      "76559\tTrain Epoch: 2 [2400/9730 (33%)]\tLoss: 2.533762\n",
      "76559\tTrain Epoch: 2 [3600/9730 (50%)]\tLoss: 2.531390\n",
      "76559\tTrain Epoch: 2 [4800/9730 (67%)]\tLoss: 2.516706\n",
      "76559\tTrain Epoch: 2 [6000/9730 (83%)]\tLoss: 2.534300\n",
      "76559\tTrain Epoch: 3 [0/9730 (0%)]\tLoss: 2.535845\n",
      "76559\tTrain Epoch: 3 [1200/9730 (17%)]\tLoss: 2.503897\n",
      "76559\tTrain Epoch: 3 [2400/9730 (33%)]\tLoss: 2.541872\n",
      "76559\tTrain Epoch: 3 [3600/9730 (50%)]\tLoss: 2.528813\n",
      "76559\tTrain Epoch: 3 [4800/9730 (67%)]\tLoss: 2.529763\n",
      "76559\tTrain Epoch: 3 [6000/9730 (83%)]\tLoss: 2.546840\n",
      "76559\tTrain Epoch: 4 [0/9730 (0%)]\tLoss: 2.517415\n",
      "76559\tTrain Epoch: 4 [1200/9730 (17%)]\tLoss: 2.505742\n",
      "76559\tTrain Epoch: 4 [2400/9730 (33%)]\tLoss: 2.523010\n",
      "76559\tTrain Epoch: 4 [3600/9730 (50%)]\tLoss: 2.519287\n",
      "76559\tTrain Epoch: 4 [4800/9730 (67%)]\tLoss: 2.533894\n",
      "76559\tTrain Epoch: 4 [6000/9730 (83%)]\tLoss: 2.502676\n",
      "76559\tTrain Epoch: 5 [0/9730 (0%)]\tLoss: 2.553387\n",
      "76559\tTrain Epoch: 5 [1200/9730 (17%)]\tLoss: 2.508687\n",
      "76559\tTrain Epoch: 5 [2400/9730 (33%)]\tLoss: 2.505652\n",
      "76559\tTrain Epoch: 5 [3600/9730 (50%)]\tLoss: 2.530738\n",
      "76559\tTrain Epoch: 5 [4800/9730 (67%)]\tLoss: 2.546456\n",
      "76559\tTrain Epoch: 5 [6000/9730 (83%)]\tLoss: 2.507670\n",
      "76559\tTrain Epoch: 6 [0/9730 (0%)]\tLoss: 2.517048\n",
      "76559\tTrain Epoch: 6 [1200/9730 (17%)]\tLoss: 2.509856\n",
      "76559\tTrain Epoch: 6 [2400/9730 (33%)]\tLoss: 2.497760\n",
      "76559\tTrain Epoch: 6 [3600/9730 (50%)]\tLoss: 2.533650\n",
      "76559\tTrain Epoch: 6 [4800/9730 (67%)]\tLoss: 2.491583\n",
      "76559\tTrain Epoch: 6 [6000/9730 (83%)]\tLoss: 2.521881\n",
      "76559\tTrain Epoch: 7 [0/9730 (0%)]\tLoss: 2.500978\n",
      "76559\tTrain Epoch: 7 [1200/9730 (17%)]\tLoss: 2.481929\n",
      "76559\tTrain Epoch: 7 [2400/9730 (33%)]\tLoss: 2.509699\n",
      "76559\tTrain Epoch: 7 [3600/9730 (50%)]\tLoss: 2.486205\n",
      "76559\tTrain Epoch: 7 [4800/9730 (67%)]\tLoss: 2.508562\n",
      "76559\tTrain Epoch: 7 [6000/9730 (83%)]\tLoss: 2.532054\n",
      "76559\tTrain Epoch: 8 [0/9730 (0%)]\tLoss: 2.479503\n",
      "76559\tTrain Epoch: 8 [1200/9730 (17%)]\tLoss: 2.511414\n",
      "76559\tTrain Epoch: 8 [2400/9730 (33%)]\tLoss: 2.515939\n",
      "76559\tTrain Epoch: 8 [3600/9730 (50%)]\tLoss: 2.503563\n",
      "76559\tTrain Epoch: 8 [4800/9730 (67%)]\tLoss: 2.476512\n",
      "76559\tTrain Epoch: 8 [6000/9730 (83%)]\tLoss: 2.491609\n",
      "76559\tTrain Epoch: 9 [0/9730 (0%)]\tLoss: 2.493044\n",
      "76559\tTrain Epoch: 9 [1200/9730 (17%)]\tLoss: 2.494422\n",
      "76559\tTrain Epoch: 9 [2400/9730 (33%)]\tLoss: 2.535478\n",
      "76559\tTrain Epoch: 9 [3600/9730 (50%)]\tLoss: 2.473334\n",
      "76559\tTrain Epoch: 9 [4800/9730 (67%)]\tLoss: 2.503879\n",
      "76559\tTrain Epoch: 9 [6000/9730 (83%)]\tLoss: 2.504427\n",
      "76559\tTrain Epoch: 10 [0/9730 (0%)]\tLoss: 2.464532\n",
      "76559\tTrain Epoch: 10 [1200/9730 (17%)]\tLoss: 2.505920\n",
      "76559\tTrain Epoch: 10 [2400/9730 (33%)]\tLoss: 2.520951\n",
      "76559\tTrain Epoch: 10 [3600/9730 (50%)]\tLoss: 2.472383\n",
      "76559\tTrain Epoch: 10 [4800/9730 (67%)]\tLoss: 2.485309\n",
      "76559\tTrain Epoch: 10 [6000/9730 (83%)]\tLoss: 2.489614\n",
      "76559\tTrain Epoch: 11 [0/9730 (0%)]\tLoss: 2.500335\n",
      "76559\tTrain Epoch: 11 [1200/9730 (17%)]\tLoss: 2.501988\n",
      "76559\tTrain Epoch: 11 [2400/9730 (33%)]\tLoss: 2.495963\n",
      "76559\tTrain Epoch: 11 [3600/9730 (50%)]\tLoss: 2.473836\n",
      "76559\tTrain Epoch: 11 [4800/9730 (67%)]\tLoss: 2.529645\n",
      "76559\tTrain Epoch: 11 [6000/9730 (83%)]\tLoss: 2.464512\n",
      "76559\tTrain Epoch: 12 [0/9730 (0%)]\tLoss: 2.454390\n",
      "76559\tTrain Epoch: 12 [1200/9730 (17%)]\tLoss: 2.514589\n",
      "76559\tTrain Epoch: 12 [2400/9730 (33%)]\tLoss: 2.479008\n",
      "76559\tTrain Epoch: 12 [3600/9730 (50%)]\tLoss: 2.459851\n",
      "76559\tTrain Epoch: 12 [4800/9730 (67%)]\tLoss: 2.450444\n",
      "76559\tTrain Epoch: 12 [6000/9730 (83%)]\tLoss: 2.476111\n",
      "76559\tTrain Epoch: 13 [0/9730 (0%)]\tLoss: 2.486869\n",
      "76559\tTrain Epoch: 13 [1200/9730 (17%)]\tLoss: 2.473385\n",
      "76559\tTrain Epoch: 13 [2400/9730 (33%)]\tLoss: 2.509969\n",
      "76559\tTrain Epoch: 13 [3600/9730 (50%)]\tLoss: 2.485506\n",
      "76559\tTrain Epoch: 13 [4800/9730 (67%)]\tLoss: 2.486031\n",
      "76559\tTrain Epoch: 13 [6000/9730 (83%)]\tLoss: 2.498765\n",
      "76559\tTrain Epoch: 14 [0/9730 (0%)]\tLoss: 2.506384\n",
      "76559\tTrain Epoch: 14 [1200/9730 (17%)]\tLoss: 2.458773\n",
      "76559\tTrain Epoch: 14 [2400/9730 (33%)]\tLoss: 2.444196\n",
      "76559\tTrain Epoch: 14 [3600/9730 (50%)]\tLoss: 2.499677\n",
      "76559\tTrain Epoch: 14 [4800/9730 (67%)]\tLoss: 2.491930\n",
      "76559\tTrain Epoch: 14 [6000/9730 (83%)]\tLoss: 2.496759\n",
      "76559\tTrain Epoch: 15 [0/9730 (0%)]\tLoss: 2.492004\n",
      "76559\tTrain Epoch: 15 [1200/9730 (17%)]\tLoss: 2.476350\n",
      "76559\tTrain Epoch: 15 [2400/9730 (33%)]\tLoss: 2.467295\n",
      "76559\tTrain Epoch: 15 [3600/9730 (50%)]\tLoss: 2.524150\n",
      "76559\tTrain Epoch: 15 [4800/9730 (67%)]\tLoss: 2.491248\n",
      "76559\tTrain Epoch: 15 [6000/9730 (83%)]\tLoss: 2.448149\n",
      "76559\tTrain Epoch: 16 [0/9730 (0%)]\tLoss: 2.474539\n",
      "76559\tTrain Epoch: 16 [1200/9730 (17%)]\tLoss: 2.444264\n",
      "76559\tTrain Epoch: 16 [2400/9730 (33%)]\tLoss: 2.477144\n",
      "76559\tTrain Epoch: 16 [3600/9730 (50%)]\tLoss: 2.418840\n",
      "76559\tTrain Epoch: 16 [4800/9730 (67%)]\tLoss: 2.441851\n",
      "76559\tTrain Epoch: 16 [6000/9730 (83%)]\tLoss: 2.450535\n",
      "76559\tTrain Epoch: 17 [0/9730 (0%)]\tLoss: 2.457893\n",
      "76559\tTrain Epoch: 17 [1200/9730 (17%)]\tLoss: 2.499832\n",
      "76559\tTrain Epoch: 17 [2400/9730 (33%)]\tLoss: 2.481300\n",
      "76559\tTrain Epoch: 17 [3600/9730 (50%)]\tLoss: 2.475955\n",
      "76559\tTrain Epoch: 17 [4800/9730 (67%)]\tLoss: 2.530556\n",
      "76559\tTrain Epoch: 17 [6000/9730 (83%)]\tLoss: 2.465680\n",
      "76559\tTrain Epoch: 18 [0/9730 (0%)]\tLoss: 2.440224\n",
      "76559\tTrain Epoch: 18 [1200/9730 (17%)]\tLoss: 2.475669\n",
      "76559\tTrain Epoch: 18 [2400/9730 (33%)]\tLoss: 2.500027\n",
      "76559\tTrain Epoch: 18 [3600/9730 (50%)]\tLoss: 2.461776\n",
      "76559\tTrain Epoch: 18 [4800/9730 (67%)]\tLoss: 2.492232\n",
      "76559\tTrain Epoch: 18 [6000/9730 (83%)]\tLoss: 2.481565\n",
      "76559\tTrain Epoch: 19 [0/9730 (0%)]\tLoss: 2.485763\n",
      "76559\tTrain Epoch: 19 [1200/9730 (17%)]\tLoss: 2.465178\n",
      "76559\tTrain Epoch: 19 [2400/9730 (33%)]\tLoss: 2.457750\n",
      "76559\tTrain Epoch: 19 [3600/9730 (50%)]\tLoss: 2.449970\n",
      "76559\tTrain Epoch: 19 [4800/9730 (67%)]\tLoss: 2.441696\n",
      "76559\tTrain Epoch: 19 [6000/9730 (83%)]\tLoss: 2.461072\n",
      "76559\tTrain Epoch: 20 [0/9730 (0%)]\tLoss: 2.495785\n",
      "76559\tTrain Epoch: 20 [1200/9730 (17%)]\tLoss: 2.470402\n",
      "76559\tTrain Epoch: 20 [2400/9730 (33%)]\tLoss: 2.526952\n",
      "76559\tTrain Epoch: 20 [3600/9730 (50%)]\tLoss: 2.454756\n",
      "76559\tTrain Epoch: 20 [4800/9730 (67%)]\tLoss: 2.492604\n",
      "76559\tTrain Epoch: 20 [6000/9730 (83%)]\tLoss: 2.420802\n",
      "76559\tTrain Epoch: 21 [0/9730 (0%)]\tLoss: 2.427102\n",
      "76559\tTrain Epoch: 21 [1200/9730 (17%)]\tLoss: 2.434710\n",
      "76559\tTrain Epoch: 21 [2400/9730 (33%)]\tLoss: 2.472093\n",
      "76559\tTrain Epoch: 21 [3600/9730 (50%)]\tLoss: 2.470497\n",
      "76559\tTrain Epoch: 21 [4800/9730 (67%)]\tLoss: 2.478388\n",
      "76559\tTrain Epoch: 21 [6000/9730 (83%)]\tLoss: 2.453536\n",
      "76559\tTrain Epoch: 22 [0/9730 (0%)]\tLoss: 2.458063\n",
      "76559\tTrain Epoch: 22 [1200/9730 (17%)]\tLoss: 2.416360\n",
      "76559\tTrain Epoch: 22 [2400/9730 (33%)]\tLoss: 2.417754\n",
      "76559\tTrain Epoch: 22 [3600/9730 (50%)]\tLoss: 2.463613\n",
      "76559\tTrain Epoch: 22 [4800/9730 (67%)]\tLoss: 2.459834\n",
      "76559\tTrain Epoch: 22 [6000/9730 (83%)]\tLoss: 2.450284\n",
      "76559\tTrain Epoch: 23 [0/9730 (0%)]\tLoss: 2.466789\n",
      "76559\tTrain Epoch: 23 [1200/9730 (17%)]\tLoss: 2.425502\n",
      "76559\tTrain Epoch: 23 [2400/9730 (33%)]\tLoss: 2.451954\n",
      "76559\tTrain Epoch: 23 [3600/9730 (50%)]\tLoss: 2.414548\n",
      "76559\tTrain Epoch: 23 [4800/9730 (67%)]\tLoss: 2.420637\n",
      "76559\tTrain Epoch: 23 [6000/9730 (83%)]\tLoss: 2.464775\n",
      "76559\tTrain Epoch: 24 [0/9730 (0%)]\tLoss: 2.450169\n",
      "76559\tTrain Epoch: 24 [1200/9730 (17%)]\tLoss: 2.485847\n",
      "76559\tTrain Epoch: 24 [2400/9730 (33%)]\tLoss: 2.400858\n",
      "76559\tTrain Epoch: 24 [3600/9730 (50%)]\tLoss: 2.442340\n",
      "76559\tTrain Epoch: 24 [4800/9730 (67%)]\tLoss: 2.409087\n",
      "76559\tTrain Epoch: 24 [6000/9730 (83%)]\tLoss: 2.449734\n",
      "76559\tTrain Epoch: 25 [0/9730 (0%)]\tLoss: 2.457790\n",
      "76559\tTrain Epoch: 25 [1200/9730 (17%)]\tLoss: 2.458848\n",
      "76559\tTrain Epoch: 25 [2400/9730 (33%)]\tLoss: 2.453185\n",
      "76559\tTrain Epoch: 25 [3600/9730 (50%)]\tLoss: 2.509903\n",
      "76559\tTrain Epoch: 25 [4800/9730 (67%)]\tLoss: 2.421801\n",
      "76559\tTrain Epoch: 25 [6000/9730 (83%)]\tLoss: 2.471144\n",
      "76559\tTrain Epoch: 26 [0/9730 (0%)]\tLoss: 2.437644\n",
      "76559\tTrain Epoch: 26 [1200/9730 (17%)]\tLoss: 2.458860\n",
      "76559\tTrain Epoch: 26 [2400/9730 (33%)]\tLoss: 2.413110\n",
      "76559\tTrain Epoch: 26 [3600/9730 (50%)]\tLoss: 2.412277\n",
      "76559\tTrain Epoch: 26 [4800/9730 (67%)]\tLoss: 2.441936\n",
      "76559\tTrain Epoch: 26 [6000/9730 (83%)]\tLoss: 2.422845\n",
      "76559\tTrain Epoch: 27 [0/9730 (0%)]\tLoss: 2.469214\n",
      "76559\tTrain Epoch: 27 [1200/9730 (17%)]\tLoss: 2.424892\n",
      "76559\tTrain Epoch: 27 [2400/9730 (33%)]\tLoss: 2.463583\n",
      "76559\tTrain Epoch: 27 [3600/9730 (50%)]\tLoss: 2.455096\n",
      "76559\tTrain Epoch: 27 [4800/9730 (67%)]\tLoss: 2.422729\n",
      "76559\tTrain Epoch: 27 [6000/9730 (83%)]\tLoss: 2.410082\n",
      "76559\tTrain Epoch: 28 [0/9730 (0%)]\tLoss: 2.418713\n",
      "76559\tTrain Epoch: 28 [1200/9730 (17%)]\tLoss: 2.437835\n",
      "76559\tTrain Epoch: 28 [2400/9730 (33%)]\tLoss: 2.457197\n",
      "76559\tTrain Epoch: 28 [3600/9730 (50%)]\tLoss: 2.467352\n",
      "76559\tTrain Epoch: 28 [4800/9730 (67%)]\tLoss: 2.390646\n",
      "76559\tTrain Epoch: 28 [6000/9730 (83%)]\tLoss: 2.478703\n",
      "76559\tTrain Epoch: 29 [0/9730 (0%)]\tLoss: 2.437844\n",
      "76559\tTrain Epoch: 29 [1200/9730 (17%)]\tLoss: 2.427609\n",
      "76559\tTrain Epoch: 29 [2400/9730 (33%)]\tLoss: 2.411544\n",
      "76559\tTrain Epoch: 29 [3600/9730 (50%)]\tLoss: 2.445006\n",
      "76559\tTrain Epoch: 29 [4800/9730 (67%)]\tLoss: 2.430468\n",
      "76559\tTrain Epoch: 29 [6000/9730 (83%)]\tLoss: 2.431916\n",
      "76559\tTrain Epoch: 30 [0/9730 (0%)]\tLoss: 2.394565\n",
      "76559\tTrain Epoch: 30 [1200/9730 (17%)]\tLoss: 2.455709\n",
      "76559\tTrain Epoch: 30 [2400/9730 (33%)]\tLoss: 2.424566\n",
      "76559\tTrain Epoch: 30 [3600/9730 (50%)]\tLoss: 2.406597\n",
      "76559\tTrain Epoch: 30 [4800/9730 (67%)]\tLoss: 2.389303\n",
      "76559\tTrain Epoch: 30 [6000/9730 (83%)]\tLoss: 2.419574\n",
      "76559\tTrain Epoch: 31 [0/9730 (0%)]\tLoss: 2.439140\n",
      "76559\tTrain Epoch: 31 [1200/9730 (17%)]\tLoss: 2.403097\n",
      "76559\tTrain Epoch: 31 [2400/9730 (33%)]\tLoss: 2.392466\n",
      "76559\tTrain Epoch: 31 [3600/9730 (50%)]\tLoss: 2.409350\n",
      "76559\tTrain Epoch: 31 [4800/9730 (67%)]\tLoss: 2.447827\n",
      "76559\tTrain Epoch: 31 [6000/9730 (83%)]\tLoss: 2.444595\n",
      "76559\tTrain Epoch: 32 [0/9730 (0%)]\tLoss: 2.413631\n",
      "76559\tTrain Epoch: 32 [1200/9730 (17%)]\tLoss: 2.443578\n",
      "76559\tTrain Epoch: 32 [2400/9730 (33%)]\tLoss: 2.425171\n",
      "76559\tTrain Epoch: 32 [3600/9730 (50%)]\tLoss: 2.404451\n",
      "76559\tTrain Epoch: 32 [4800/9730 (67%)]\tLoss: 2.433790\n",
      "76559\tTrain Epoch: 32 [6000/9730 (83%)]\tLoss: 2.480627\n",
      "76559\tTrain Epoch: 33 [0/9730 (0%)]\tLoss: 2.434043\n",
      "76559\tTrain Epoch: 33 [1200/9730 (17%)]\tLoss: 2.395062\n",
      "76559\tTrain Epoch: 33 [2400/9730 (33%)]\tLoss: 2.409941\n",
      "76559\tTrain Epoch: 33 [3600/9730 (50%)]\tLoss: 2.387577\n",
      "76559\tTrain Epoch: 33 [4800/9730 (67%)]\tLoss: 2.445234\n",
      "76559\tTrain Epoch: 33 [6000/9730 (83%)]\tLoss: 2.483460\n",
      "76559\tTrain Epoch: 34 [0/9730 (0%)]\tLoss: 2.396336\n",
      "76559\tTrain Epoch: 34 [1200/9730 (17%)]\tLoss: 2.411489\n",
      "76559\tTrain Epoch: 34 [2400/9730 (33%)]\tLoss: 2.465009\n",
      "76559\tTrain Epoch: 34 [3600/9730 (50%)]\tLoss: 2.416023\n",
      "76559\tTrain Epoch: 34 [4800/9730 (67%)]\tLoss: 2.393916\n",
      "76559\tTrain Epoch: 34 [6000/9730 (83%)]\tLoss: 2.383310\n",
      "76559\tTrain Epoch: 35 [0/9730 (0%)]\tLoss: 2.484050\n",
      "76559\tTrain Epoch: 35 [1200/9730 (17%)]\tLoss: 2.419811\n",
      "76559\tTrain Epoch: 35 [2400/9730 (33%)]\tLoss: 2.426970\n",
      "76559\tTrain Epoch: 35 [3600/9730 (50%)]\tLoss: 2.391217\n",
      "76559\tTrain Epoch: 35 [4800/9730 (67%)]\tLoss: 2.383230\n",
      "76559\tTrain Epoch: 35 [6000/9730 (83%)]\tLoss: 2.326446\n",
      "76559\tTrain Epoch: 36 [0/9730 (0%)]\tLoss: 2.433830\n",
      "76559\tTrain Epoch: 36 [1200/9730 (17%)]\tLoss: 2.408341\n",
      "76559\tTrain Epoch: 36 [2400/9730 (33%)]\tLoss: 2.418069\n",
      "76559\tTrain Epoch: 36 [3600/9730 (50%)]\tLoss: 2.430338\n",
      "76559\tTrain Epoch: 36 [4800/9730 (67%)]\tLoss: 2.393787\n",
      "76559\tTrain Epoch: 36 [6000/9730 (83%)]\tLoss: 2.459253\n",
      "76559\tTrain Epoch: 37 [0/9730 (0%)]\tLoss: 2.408435\n",
      "76559\tTrain Epoch: 37 [1200/9730 (17%)]\tLoss: 2.359631\n",
      "76559\tTrain Epoch: 37 [2400/9730 (33%)]\tLoss: 2.374389\n",
      "76559\tTrain Epoch: 37 [3600/9730 (50%)]\tLoss: 2.388022\n",
      "76559\tTrain Epoch: 37 [4800/9730 (67%)]\tLoss: 2.374509\n",
      "76559\tTrain Epoch: 37 [6000/9730 (83%)]\tLoss: 2.378320\n",
      "76559\tTrain Epoch: 38 [0/9730 (0%)]\tLoss: 2.367277\n",
      "76559\tTrain Epoch: 38 [1200/9730 (17%)]\tLoss: 2.434541\n",
      "76559\tTrain Epoch: 38 [2400/9730 (33%)]\tLoss: 2.406374\n",
      "76559\tTrain Epoch: 38 [3600/9730 (50%)]\tLoss: 2.398090\n",
      "76559\tTrain Epoch: 38 [4800/9730 (67%)]\tLoss: 2.405137\n",
      "76559\tTrain Epoch: 38 [6000/9730 (83%)]\tLoss: 2.388128\n",
      "76559\tTrain Epoch: 39 [0/9730 (0%)]\tLoss: 2.394280\n",
      "76559\tTrain Epoch: 39 [1200/9730 (17%)]\tLoss: 2.412147\n",
      "76559\tTrain Epoch: 39 [2400/9730 (33%)]\tLoss: 2.386224\n",
      "76559\tTrain Epoch: 39 [3600/9730 (50%)]\tLoss: 2.395953\n",
      "76559\tTrain Epoch: 39 [4800/9730 (67%)]\tLoss: 2.425935\n",
      "76559\tTrain Epoch: 39 [6000/9730 (83%)]\tLoss: 2.414211\n",
      "76559\tTrain Epoch: 40 [0/9730 (0%)]\tLoss: 2.422317\n",
      "76559\tTrain Epoch: 40 [1200/9730 (17%)]\tLoss: 2.428628\n",
      "76559\tTrain Epoch: 40 [2400/9730 (33%)]\tLoss: 2.421194\n",
      "76559\tTrain Epoch: 40 [3600/9730 (50%)]\tLoss: 2.370885\n",
      "76559\tTrain Epoch: 40 [4800/9730 (67%)]\tLoss: 2.427369\n",
      "76559\tTrain Epoch: 40 [6000/9730 (83%)]\tLoss: 2.416491\n",
      "76559\tTrain Epoch: 41 [0/9730 (0%)]\tLoss: 2.403919\n",
      "76559\tTrain Epoch: 41 [1200/9730 (17%)]\tLoss: 2.370365\n",
      "76559\tTrain Epoch: 41 [2400/9730 (33%)]\tLoss: 2.429954\n",
      "76559\tTrain Epoch: 41 [3600/9730 (50%)]\tLoss: 2.416570\n",
      "76559\tTrain Epoch: 41 [4800/9730 (67%)]\tLoss: 2.423056\n",
      "76559\tTrain Epoch: 41 [6000/9730 (83%)]\tLoss: 2.390291\n",
      "76559\tTrain Epoch: 42 [0/9730 (0%)]\tLoss: 2.372094\n",
      "76559\tTrain Epoch: 42 [1200/9730 (17%)]\tLoss: 2.350648\n",
      "76559\tTrain Epoch: 42 [2400/9730 (33%)]\tLoss: 2.436833\n",
      "76559\tTrain Epoch: 42 [3600/9730 (50%)]\tLoss: 2.413463\n",
      "76559\tTrain Epoch: 42 [4800/9730 (67%)]\tLoss: 2.387607\n",
      "76559\tTrain Epoch: 42 [6000/9730 (83%)]\tLoss: 2.413497\n",
      "76559\tTrain Epoch: 43 [0/9730 (0%)]\tLoss: 2.372316\n",
      "76559\tTrain Epoch: 43 [1200/9730 (17%)]\tLoss: 2.350971\n",
      "76559\tTrain Epoch: 43 [2400/9730 (33%)]\tLoss: 2.354872\n",
      "76559\tTrain Epoch: 43 [3600/9730 (50%)]\tLoss: 2.414361\n",
      "76559\tTrain Epoch: 43 [4800/9730 (67%)]\tLoss: 2.309257\n",
      "76559\tTrain Epoch: 43 [6000/9730 (83%)]\tLoss: 2.439157\n",
      "76559\tTrain Epoch: 44 [0/9730 (0%)]\tLoss: 2.400642\n",
      "76559\tTrain Epoch: 44 [1200/9730 (17%)]\tLoss: 2.391278\n",
      "76559\tTrain Epoch: 44 [2400/9730 (33%)]\tLoss: 2.381585\n",
      "76559\tTrain Epoch: 44 [3600/9730 (50%)]\tLoss: 2.411411\n",
      "76559\tTrain Epoch: 44 [4800/9730 (67%)]\tLoss: 2.367406\n",
      "76559\tTrain Epoch: 44 [6000/9730 (83%)]\tLoss: 2.382833\n",
      "76559\tTrain Epoch: 45 [0/9730 (0%)]\tLoss: 2.414619\n",
      "76559\tTrain Epoch: 45 [1200/9730 (17%)]\tLoss: 2.398879\n",
      "76559\tTrain Epoch: 45 [2400/9730 (33%)]\tLoss: 2.387979\n",
      "76559\tTrain Epoch: 45 [3600/9730 (50%)]\tLoss: 2.385363\n",
      "76559\tTrain Epoch: 45 [4800/9730 (67%)]\tLoss: 2.366275\n",
      "76559\tTrain Epoch: 45 [6000/9730 (83%)]\tLoss: 2.372033\n",
      "76559\tTrain Epoch: 46 [0/9730 (0%)]\tLoss: 2.362988\n",
      "76559\tTrain Epoch: 46 [1200/9730 (17%)]\tLoss: 2.398698\n",
      "76559\tTrain Epoch: 46 [2400/9730 (33%)]\tLoss: 2.404940\n",
      "76559\tTrain Epoch: 46 [3600/9730 (50%)]\tLoss: 2.375750\n",
      "76559\tTrain Epoch: 46 [4800/9730 (67%)]\tLoss: 2.385687\n",
      "76559\tTrain Epoch: 46 [6000/9730 (83%)]\tLoss: 2.330066\n",
      "76559\tTrain Epoch: 47 [0/9730 (0%)]\tLoss: 2.376269\n",
      "76559\tTrain Epoch: 47 [1200/9730 (17%)]\tLoss: 2.338253\n",
      "76559\tTrain Epoch: 47 [2400/9730 (33%)]\tLoss: 2.403297\n",
      "76559\tTrain Epoch: 47 [3600/9730 (50%)]\tLoss: 2.382593\n",
      "76559\tTrain Epoch: 47 [4800/9730 (67%)]\tLoss: 2.387509\n",
      "76559\tTrain Epoch: 47 [6000/9730 (83%)]\tLoss: 2.352371\n",
      "76559\tTrain Epoch: 48 [0/9730 (0%)]\tLoss: 2.415395\n",
      "76559\tTrain Epoch: 48 [1200/9730 (17%)]\tLoss: 2.366534\n",
      "76559\tTrain Epoch: 48 [2400/9730 (33%)]\tLoss: 2.352505\n",
      "76559\tTrain Epoch: 48 [3600/9730 (50%)]\tLoss: 2.389877\n",
      "76559\tTrain Epoch: 48 [4800/9730 (67%)]\tLoss: 2.360282\n",
      "76559\tTrain Epoch: 48 [6000/9730 (83%)]\tLoss: 2.351536\n",
      "76559\tTrain Epoch: 49 [0/9730 (0%)]\tLoss: 2.315524\n",
      "76559\tTrain Epoch: 49 [1200/9730 (17%)]\tLoss: 2.351420\n",
      "76559\tTrain Epoch: 49 [2400/9730 (33%)]\tLoss: 2.352623\n",
      "76559\tTrain Epoch: 49 [3600/9730 (50%)]\tLoss: 2.362947\n",
      "76559\tTrain Epoch: 49 [4800/9730 (67%)]\tLoss: 2.399594\n",
      "76559\tTrain Epoch: 49 [6000/9730 (83%)]\tLoss: 2.377675\n",
      "76559\tTrain Epoch: 50 [0/9730 (0%)]\tLoss: 2.429506\n",
      "76559\tTrain Epoch: 50 [1200/9730 (17%)]\tLoss: 2.385669\n",
      "76559\tTrain Epoch: 50 [2400/9730 (33%)]\tLoss: 2.352331\n",
      "76559\tTrain Epoch: 50 [3600/9730 (50%)]\tLoss: 2.342376\n",
      "76559\tTrain Epoch: 50 [4800/9730 (67%)]\tLoss: 2.343920\n",
      "76559\tTrain Epoch: 50 [6000/9730 (83%)]\tLoss: 2.310272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "test(model=willyShuffleNet, device=device, test_loaderIn=train_loader)\n",
    "test(model=willyShuffleNet, device=device, test_loaderIn=valid_loader)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test set: Average loss: 1.7504, Accuracy: 1527/9730 (16%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3011, Accuracy: 186/9730 (2%)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(rank, args, model, device, dataset, dataloader_kwargs):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr,\n",
    "                          momentum=args.momentum)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_epoch(epoch, args, model, device, train_loader, optimizer)\n",
    "\n",
    "\n",
    "def test(args, model, device, dataset, dataloader_kwargs):\n",
    "    torch.manual_seed(args.seed)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)\n",
    "    test_epoch(model, device, test_loader)\n",
    "\n",
    "\n",
    "def train_epoch(epoch, args, model, device, data_loader, optimizer):\n",
    "    model.train()\n",
    "    pid = os.getpid()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        loss = F.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                pid, epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test_epoch(model, device, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = model(data.to(device))\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target.to(device),\n",
    "                                    reduction='sum').item()\n",
    "            pred = output.max(1)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device)).sum().item()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('umu-pytorch': conda)"
  },
  "interpreter": {
   "hash": "52604bc78b09674e91f4fa9bc994d961bb2ba050c8b816e2c0e907b2766bc32f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}