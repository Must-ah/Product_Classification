{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile InitializeModel.py\n",
    "from MileStone_3_Create_Neural_Network.WillyAlexNet import MyAlexNet\n",
    "from MileStone_3_Create_Neural_Network.WillyNet import WillyNet\n",
    "from MileStone_3_Create_Neural_Network.WillyMnasNet0_5 import MyMnasNet0_5\n",
    "from MileStone_3_Create_Neural_Network.WillysShuffleNet_v2_x1_0 import myShuffleNet_v2_x1_0\n",
    "from torch import optim\n",
    "from torch.nn import NLLLoss\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "\n",
    "def initializeModel(config):\n",
    "    # ToDo return model from my private zoo\n",
    "    modelName = config[\"model\"]\n",
    "    if modelName == \"MyAlexNet\":\n",
    "        model = MyAlexNet()\n",
    "    elif modelName == \"WillyNet\":\n",
    "        model = WillyNet()\n",
    "    elif modelName == \"WillyMnasNet0_5\":\n",
    "        model = MyMnasNet0_5()\n",
    "    elif modelName == \"WillysShuffleNet_v2_x1_0\":\n",
    "        model = myShuffleNet_v2_x1_0()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Model {modelName} not supported or whatever something else\")\n",
    "\n",
    "    # model = WillyNet()\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.get(\"lr\", 1e-2),\n",
    "        momentum=config.get(\"momentum\", 0.5),\n",
    "        weight_decay=config.get('weight_decay', 1e-6),\n",
    "        nesterov=True,\n",
    "    )\n",
    "    criterion = NLLLoss()\n",
    "    le = config['num_iters_per_epoch']\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer,\n",
    "                                             step_size=le,\n",
    "                                             gamma=config.get(\n",
    "                                                 'gamma', 0.9)\n",
    "                                             )\n",
    "    return model, optimizer, criterion, lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile CreateTrainer.py\n",
    "from ignite.engine import Engine, Events, create_supervised_evaluator\n",
    "from pathlib import Path\n",
    "import json\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.handlers import ModelCheckpoint, Checkpoint, DiskSaver\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage, ClassificationReport\n",
    "from ignite.contrib.engines.common import save_best_model_by_val_score\n",
    "import torch\n",
    "# from ignite.metrics.precision import Precision\n",
    "# from ignite.metrics.recall import Recall\n",
    "# source https://colab.research.google.com/github/pytorch/ignite/blob/master/assets/tldr/teaser.ipynb#scrollTo=dFglXKeKOgdW\n",
    "\n",
    "\n",
    "def create_trainer(model, optimizer, criterion, lr_scheduler, config, data):\n",
    "    device = config['device']\n",
    "    (train_loader, train_sampler), (valid_loader,\n",
    "                                    valid_sampler), (test_loader, test_sampler) = data\n",
    "    # Define any training logic for iteration update\n",
    "\n",
    "    def train_step(engine, batch):\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        model.train()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def eval_model_on_batch(engine, batch):\n",
    "        \"\"\"\n",
    "        Evaluation of the model on a single batch\n",
    "\n",
    "        Args:\n",
    "            engine: ignite.engine.Engine\n",
    "            batch: tuple contains the training sample and their labels\n",
    "\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data, target = batch\n",
    "            y_pred = model(data.to(device))\n",
    "            return y_pred, target.to(devtraining(config)ice)\n",
    "\n",
    "    trainer = Engine(train_step)\n",
    "    # train_evaluator = Engine(eval_model_on_batch)\n",
    "    # validation_evaluator = Engine(eval_model_on_batch)\n",
    "    # test_evaluator = Engine(eval_model_on_batch)\n",
    "\n",
    "    evaluator = create_supervised_evaluator(\n",
    "        model,\n",
    "        metrics={\"accuracy\": Accuracy(),\n",
    "                 #  \"loss\": RunningAverage(Accuracy()),\n",
    "                 #  \"precision\": Precision(average=False),\n",
    "                 #  'recall': Recall(average=False),\n",
    "                 #            'cr': ClassificationReport(output_dict=True, is_multilabel=False)\n",
    "                 },\n",
    "        device=config.get('device', 'cpu')\n",
    "    )\n",
    "    # evaluatorF1 = create_supervised_evaluator(\n",
    "    #     model, metrics={'cr': ClassificationReport(output_dict=True, is_multilabel=False)\n",
    "    #                     },\n",
    "    #     device=config.get('device', 'cpu')\n",
    "    # )\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED(every=10))\n",
    "    def save_checkpoint():\n",
    "        fp = Path(config.get(\"output_path\", \"./output\")) / \"checkpoint.pt\"\n",
    "        torch.save(model.state_dict(), fp)\n",
    "\n",
    "    pbar = ProgressBar(persist=True, bar_format=\"\")\n",
    "\n",
    "    # Run model evaluation every 3 epochs and show results\n",
    "    @trainer.on(Events.EPOCH_COMPLETED(every=1))\n",
    "    def evaluate_model():\n",
    "        state = evaluator.run(valid_loader)\n",
    "        metrics = state.metrics\n",
    "    #     # acc = metrics['accuracy']\n",
    "    #     # loss = metrics['loss']\n",
    "    #     # train_evaluator.run(train_loader)\n",
    "    #     # validation_evaluator.run(valid_loader)\n",
    "\n",
    "    # @trainer.on(Events.EPOCH_COMPLETED(every=5))\n",
    "    # def F1ScoreEval(engine):\n",
    "    #     state = evaluatorF1.run(valid_loader)\n",
    "    #     toPrint = \"\"\n",
    "    #     res_dic = state.metrics['cr']\n",
    "    #     for idx, key in enumerate(res_dic):\n",
    "    #         temp = \"Class: {} f1-score: {:.2f}, \".format(\n",
    "    #             key, res_dic[key]['f1-score'])\n",
    "    #         if idx == 4 or idx == 8 or idx == 12 or idx == 16:\n",
    "    #             toPrint += '\\n'\n",
    "    #         if key == 'macro avg':\n",
    "    #             temp = \"\\nMacro Average:\\nPrecision: {:.3f}, Recall: {:.3f}, F1 {:.3f}\".format(\n",
    "    #                 res_dic[key]['precision'], res_dic[key]['recall'], res_dic[key]['f1-score'],)\n",
    "    #         toPrint += temp\n",
    "    #     pbar.log_message(\"Scores:\\n{}\".format(toPrint))\n",
    "\n",
    "    RunningAverage(output_transform=lambda x: x).attach(\n",
    "        trainer, 'loss'\n",
    "    )\n",
    "    # pbar.attach(trainer, ['loss'])\n",
    "    pbar.attach(trainer, output_transform=lambda x: {\"batch loss\": x})\n",
    "\n",
    "    # Accuracy(output_transform=lambda x: x).attach(train_evaluator, 'train_acc')\n",
    "    # Loss(criterion).attach(train_evaluator, 'train_NLLLoss')\n",
    "\n",
    "    # Accuracy(output_transform=lambda x: x).attach(\n",
    "    #     validation_evaluator, 'val_acc')\n",
    "    # Loss(criterion).attach(validation_evaluator, 'val_NLLLoss')\n",
    "\n",
    "    # checkpointer = ModelCheckpoint(\n",
    "    #     config.get('checkpointPath', './tmp/models'), config.get('model',\n",
    "    #                                                              'Unknown_you_lazy_bastart'),\n",
    "    #     n_saved=config.get('max_saved_checkout', 1), create_dir=True,\n",
    "    #     save_as_state_dict=True, require_empty=config.get('require_empty', True)\n",
    "    # )\n",
    "    # trainer.add_event_handler(Events.EPOCH_COMPLETED(every=3),\n",
    "    #                           checkpointer, {config.get('model'): model}\n",
    "    #                           )\n",
    "\n",
    "    # to_save = {'trainer': trainer,\n",
    "    #            'model': model,\n",
    "    #            'optimizer': optimizer,\n",
    "    #            'lr_scheduler': lr_scheduler,\n",
    "    #            }\n",
    "\n",
    "    # handler = Checkpoint(to_save,\n",
    "    #                      DiskSaver(config['trainer_save_path'],\n",
    "    #                                create_dir=True, require_empty=config['require_empty']\n",
    "    #                                )\n",
    "    #                      )\n",
    "    # bestModelSaver = save_best_model_by_val_score(output_path=config['best_model_path'],\n",
    "    #                                               evaluator=evaluator, model=model,\n",
    "    #                                               metric_name='accuracy',\n",
    "    #                                               n_saved=config.get(\n",
    "    #     \"best_n_saved\", 2)\n",
    "    # )\n",
    "    # evaluator.add_event_handler(Events.EPOCH_COMPLETED, bestModelSaver)\n",
    "\n",
    "    # trainer.add_event_handler(Events.EPOCH_COMPLETED, handler)\n",
    "\n",
    "    # return trainer, train_evaluator, validation_evaluator, test_evaluator, evaluator, pbar\n",
    "    return trainer, evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile TrainingIgnite.py\n",
    "from InitializeModel import initializeModel\n",
    "from CreateTrainer import create_trainer\n",
    "from getData import getData\n",
    "#from ignite.engine import create_supervised_evaluator, Events\n",
    "# from ignite.metrics import Accuracy, Loss, RunningAverage\n",
    "from ignite.contrib.engines import common\n",
    "# from ignite.handlers import EarlyStopping\n",
    "# from ignite.metrics import ClassificationReport\n",
    "import ignite.distributed as idist\n",
    "# import torch\n",
    "\n",
    "# source https://colab.research.google.com/github/pytorch/ignite/blob/master/assets/tldr/teaser.ipynb#scrollTo=dFglXKeKOgdW\n",
    "# dependenciues\n",
    "\n",
    "# ignite\n",
    "\n",
    "# PyTorch\n",
    "\n",
    "\n",
    "def training(config):\n",
    "    # Setup dataflow and\n",
    "    data = getData(config)\n",
    "    model, optimizer, criterion, lr_scheduler = initializeModel(config)\n",
    "    model.to(config.get('device', 'cpu'))\n",
    "    # Setup model trainer and evaluator\n",
    "    trainer, evaluator = create_trainer(model, optimizer, criterion,\n",
    "                                        lr_scheduler, config, data\n",
    "                                        )\n",
    "    data_loader, _, _ = data\n",
    "    train_loader, train_sampler = data_loader\n",
    "\n",
    "    if config['train_now'] is not True:\n",
    "        return trainer, model, optimizer, criterion, lr_scheduler, train_loader\n",
    "\n",
    "    if idist.get_rank() == 0:\n",
    "        # print('Start logging', config['tensorboard_logger_path'])\n",
    "        tb_logger = common.setup_tb_logging(\n",
    "            output_path=config.get(\"tensorboard_logger_path\", \"output\"),\n",
    "            trainer=trainer, optimizers=optimizer,\n",
    "            evaluators={\"validation\": evaluator,\n",
    "                        # 'cr': evaluator,\n",
    "                        # 'loss': trainer,\n",
    "                        # 'val_NLLLoss': validation_evaluator,\n",
    "                        # 'val_acc': validation_evaluator,\n",
    "                        # 'train_NLLLoss': train_evaluator,\n",
    "                        # 'train_acc': train_evaluator,\n",
    "                        },\n",
    "            log_every_iters=1,\n",
    "        )\n",
    "\n",
    "    trainer.run(train_loader, max_epochs=config.get(\"max_epochs\", 3))\n",
    "\n",
    "    if idist.get_rank() == 0:\n",
    "        # print(\"cloosing tb_logger...\")\n",
    "        tb_logger.close()\n",
    "    # tb_logger.close()\n",
    "    # return trainer, (train_loader, train_sampler), (valid_loader,\n",
    "    #                                                 valid_sampler), (test_loader, test_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initConfig(model_name: str) -> dict:\n",
    "    from datetime import date, datetime\n",
    "    datestamp = date.today().strftime(\"%d_%m_%Y\")\n",
    "    timestamp = datetime.now().strftime(\"%H\")\n",
    "    from torch import device\n",
    "    config = {\n",
    "        \"model\": f\"{model_name}\",\n",
    "        \"dataset\": \"Willys\",\n",
    "        'batch_size': 400,\n",
    "        \"valid_procentage\": 0.15,\n",
    "        \"test_procentage\": 0.1,\n",
    "        'data_path': \"../Dataset_Willys_2020/ORGINAL/\",\n",
    "        'output_path': f'./training_files/training_Checkpoints/{model_name}/date_{datestamp}/time_{timestamp}/checkpoints/',\n",
    "        'tensorboard_logger_path': f'./training_files/tensorboard/{model_name}/date_{datestamp}/time_{timestamp}/log',\n",
    "        'device': device('cuda:0'),\n",
    "        'checkpointPath': f'./training_files/training_Checkpoints/{model_name}/date_{datestamp}/time_{timestamp}/checkpoints/',\n",
    "        \"require_empty\": False,\n",
    "        \"max_epochs\": 15,\n",
    "        \"lr\": 1e-2,\n",
    "        \"momentum\": 0.5,\n",
    "        \"weight_decay\": 1e-6,\n",
    "        \"gamma\": 0.9,\n",
    "        \"best_model_path\": f'./training_files/bestModels/{model_name}/date_{datestamp}/time_{timestamp}/bestModel/',\n",
    "        'trainer_save_path': f'./training_files/training_Checkpoints/{model_name}/date_{datestamp}/time_{timestamp}/trainer/',\n",
    "        'train_now': True\n",
    "    }\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNamesDict = {'AlexNet': \"MyAlexNet\",\n",
    "                  'MustafaNet': \"WillyNet\",\n",
    "                  'MnasNet0_5': \"WillyMnasNet0_5\",\n",
    "                  'ShuffleNet_V2_x1_0': \"WillysShuffleNet_v2_x1_0\",\n",
    "                  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "%load_ext tensorboard\n",
    "# %tensorboard - -logdir = config['tensorboard_logger_path']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WillyNet True\n"
     ]
    }
   ],
   "source": [
    "from TrainingIgnite import training\n",
    "config = initConfig(modelNamesDict['MustafaNet'])\n",
    "config[\"max_epochs\"] = 100\n",
    "config[\"train_now\"] = True\n",
    "config['batch_size'] = 800\n",
    "# config[\"lr\"] = 10e-1\n",
    "# config[\"momentum\"] = 9e-1\n",
    "# config[\"gamma\"] = 9.5e-1\n",
    "print(config[\"model\"], config['train_now'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Dataset_Willys_2020/ORGINAL/\n",
      "total number of images 9730, which 7444 of them is used for training\n",
      "And 1459 for validation and 827 for testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]: 100%|██████████| 9/9 [00:51<00:00,  6.46s/it, batch loss=2.76]\n",
      "Epoch [2/20]: 100%|██████████| 9/9 [00:51<00:00,  6.50s/it, batch loss=2.69]\n",
      "Epoch [3/20]: 100%|██████████| 9/9 [00:52<00:00,  6.52s/it, batch loss=2.57]\n",
      "Epoch [4/20]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.6]\n",
      "Epoch [5/20]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.55]\n",
      "Epoch [6/20]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.5]\n",
      "Epoch [7/20]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.46]\n",
      "Epoch [8/20]: 100%|██████████| 9/9 [00:52<00:00,  6.58s/it, batch loss=2.5]\n",
      "Epoch [9/20]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.43]\n",
      "Epoch [10/20]: 100%|██████████| 9/9 [00:52<00:00,  6.58s/it, batch loss=2.45]\n",
      "Epoch [11/20]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.43]\n",
      "Epoch [12/20]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.33]\n",
      "Epoch [13/20]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.39]\n",
      "Epoch [14/20]: 100%|██████████| 9/9 [00:52<00:00,  6.53s/it, batch loss=2.29]\n",
      "Epoch [15/20]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.33]\n",
      "Epoch [16/20]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.32]\n",
      "Epoch [17/20]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.29]\n",
      "Epoch [18/20]: 100%|██████████| 9/9 [00:52<00:00,  6.52s/it, batch loss=2.3]\n",
      "Epoch [19/20]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.29]\n",
      "Epoch [20/20]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.22]\n"
     ]
    }
   ],
   "source": [
    "training(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Dataset_Willys_2020/ORGINAL/\n",
      "total number of images 9730, which 7444 of them is used for training\n",
      "And 1459 for validation and 827 for testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]: 100%|██████████| 9/9 [00:52<00:00,  6.53s/it, batch loss=2.71]\n",
      "Epoch [2/100]: 100%|██████████| 9/9 [00:52<00:00,  6.53s/it, batch loss=2.68]\n",
      "Epoch [3/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.64]\n",
      "Epoch [4/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.59]\n",
      "Epoch [5/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.6]\n",
      "Epoch [6/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.52]\n",
      "Epoch [7/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.53]\n",
      "Epoch [8/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.43]\n",
      "Epoch [9/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.49]\n",
      "Epoch [10/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.48]\n",
      "Epoch [11/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.44]\n",
      "Epoch [12/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.37]\n",
      "Epoch [13/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.39]\n",
      "Epoch [14/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.37]\n",
      "Epoch [15/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.36]\n",
      "Epoch [16/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.33]\n",
      "Epoch [17/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.26]\n",
      "Epoch [18/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.37]\n",
      "Epoch [19/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.31]\n",
      "Epoch [20/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.3]\n",
      "Epoch [21/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.29]\n",
      "Epoch [22/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.25]\n",
      "Epoch [23/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.21]\n",
      "Epoch [24/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.3]\n",
      "Epoch [25/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.25]\n",
      "Epoch [26/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.27]\n",
      "Epoch [27/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.23]\n",
      "Epoch [28/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.23]\n",
      "Epoch [29/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.2]\n",
      "Epoch [30/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.2]\n",
      "Epoch [31/100]: 100%|██████████| 9/9 [00:52<00:00,  6.53s/it, batch loss=2.2]\n",
      "Epoch [32/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.22]\n",
      "Epoch [33/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.26]\n",
      "Epoch [34/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.21]\n",
      "Epoch [35/100]: 100%|██████████| 9/9 [00:52<00:00,  6.58s/it, batch loss=2.18]\n",
      "Epoch [36/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.25]\n",
      "Epoch [37/100]: 100%|██████████| 9/9 [00:52<00:00,  6.53s/it, batch loss=2.22]\n",
      "Epoch [38/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.2]\n",
      "Epoch [39/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.21]\n",
      "Epoch [40/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.2]\n",
      "Epoch [41/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.21]\n",
      "Epoch [42/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.22]\n",
      "Epoch [43/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.19]\n",
      "Epoch [44/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.22]\n",
      "Epoch [45/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.16]\n",
      "Epoch [46/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.19]\n",
      "Epoch [47/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.19]\n",
      "Epoch [48/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.14]\n",
      "Epoch [49/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.18]\n",
      "Epoch [50/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.21]\n",
      "Epoch [51/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.13]\n",
      "Epoch [52/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.18]\n",
      "Epoch [53/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.23]\n",
      "Epoch [54/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.22]\n",
      "Epoch [55/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.19]\n",
      "Epoch [56/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.2]\n",
      "Epoch [57/100]: 100%|██████████| 9/9 [00:52<00:00,  6.58s/it, batch loss=2.21]\n",
      "Epoch [58/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.16]\n",
      "Epoch [59/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.16]\n",
      "Epoch [60/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.2]\n",
      "Epoch [61/100]: 100%|██████████| 9/9 [00:52<00:00,  6.53s/it, batch loss=2.19]\n",
      "Epoch [62/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.19]\n",
      "Epoch [63/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.22]\n",
      "Epoch [64/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.2]\n",
      "Epoch [65/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.22]\n",
      "Epoch [66/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.17]\n",
      "Epoch [67/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.14]\n",
      "Epoch [68/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.12]\n",
      "Epoch [69/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.21]\n",
      "Epoch [70/100]: 100%|██████████| 9/9 [00:52<00:00,  6.53s/it, batch loss=2.16]\n",
      "Epoch [71/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.21]\n",
      "Epoch [72/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.17]\n",
      "Epoch [73/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.26]\n",
      "Epoch [74/100]: 100%|██████████| 9/9 [00:52<00:00,  6.58s/it, batch loss=2.19]\n",
      "Epoch [75/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.21]\n",
      "Epoch [76/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.17]\n",
      "Epoch [77/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.18]\n",
      "Epoch [78/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.18]\n",
      "Epoch [79/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.21]\n",
      "Epoch [80/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.25]\n",
      "Epoch [81/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.19]\n",
      "Epoch [82/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.17]\n",
      "Epoch [83/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.21]\n",
      "Epoch [84/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.2]\n",
      "Epoch [85/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.21]\n",
      "Epoch [86/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.2]\n",
      "Epoch [87/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.15]\n",
      "Epoch [88/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.16]\n",
      "Epoch [89/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.15]\n",
      "Epoch [90/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.2]\n",
      "Epoch [91/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.17]\n",
      "Epoch [92/100]: 100%|██████████| 9/9 [00:52<00:00,  6.54s/it, batch loss=2.2]\n",
      "Epoch [93/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.21]\n",
      "Epoch [94/100]: 100%|██████████| 9/9 [00:52<00:00,  6.55s/it, batch loss=2.18]\n",
      "Epoch [95/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.19]\n",
      "Epoch [96/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.2]\n",
      "Epoch [97/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.19]\n",
      "Epoch [98/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.19]\n",
      "Epoch [99/100]: 100%|██████████| 9/9 [00:52<00:00,  6.57s/it, batch loss=2.16]\n",
      "Epoch [100/100]: 100%|██████████| 9/9 [00:52<00:00,  6.56s/it, batch loss=2.12]\n"
     ]
    }
   ],
   "source": [
    "training(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WillyNet True\n",
      "../Dataset_Willys_2020/ORGINAL/\n",
      "total number of images 9730, which 7444 of them is used for training\n",
      "And 1459 for validation and 827 for testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/180]: 100%|██████████| 18/18 [00:49<00:00,  2.94s/it, batch loss=2.43]\n",
      "Epoch [2/180]: 100%|██████████| 18/18 [00:50<00:00,  2.95s/it, batch loss=2.42]\n",
      "Epoch [3/180]: 100%|██████████| 18/18 [00:50<00:00,  2.96s/it, batch loss=2.42]\n",
      "Epoch [4/180]: 100%|██████████| 18/18 [00:50<00:00,  2.97s/it, batch loss=2.31]\n",
      "Epoch [5/180]: 100%|██████████| 18/18 [00:50<00:00,  2.97s/it, batch loss=2.34]\n",
      "Epoch [6/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.41]\n",
      "Epoch [7/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.3]\n",
      "Epoch [8/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.32]\n",
      "Epoch [9/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.34]\n",
      "Epoch [10/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.3]\n",
      "Epoch [11/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.28]\n",
      "Epoch [12/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.31]\n",
      "Epoch [13/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.3]\n",
      "Epoch [14/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.29]\n",
      "Epoch [15/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.26]\n",
      "Epoch [16/180]: 100%|██████████| 18/18 [00:50<00:00,  2.97s/it, batch loss=2.22]\n",
      "Epoch [17/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.2]\n",
      "Epoch [18/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.26]\n",
      "Epoch [19/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.23]\n",
      "Epoch [20/180]: 100%|██████████| 18/18 [00:50<00:00,  3.00s/it, batch loss=2.26]\n",
      "Epoch [21/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.16]\n",
      "Epoch [22/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.26]\n",
      "Epoch [23/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.21]\n",
      "Epoch [24/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.19]\n",
      "Epoch [25/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.22]\n",
      "Epoch [26/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.22]\n",
      "Epoch [27/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.25]\n",
      "Epoch [28/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.18]\n",
      "Epoch [29/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.19]\n",
      "Epoch [30/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.2]\n",
      "Epoch [31/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.16]\n",
      "Epoch [32/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.19]\n",
      "Epoch [33/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.19]\n",
      "Epoch [34/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.18]\n",
      "Epoch [35/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.21]\n",
      "Epoch [36/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.15]\n",
      "Epoch [37/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.21]\n",
      "Epoch [38/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.15]\n",
      "Epoch [39/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.17]\n",
      "Epoch [40/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.14]\n",
      "Epoch [41/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.27]\n",
      "Epoch [42/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.18]\n",
      "Epoch [43/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.2]\n",
      "Epoch [44/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.16]\n",
      "Epoch [45/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.14]\n",
      "Epoch [46/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.2]\n",
      "Epoch [47/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.18]\n",
      "Epoch [48/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.07]\n",
      "Epoch [49/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.18]\n",
      "Epoch [50/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.13]\n",
      "Epoch [51/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.23]\n",
      "Epoch [52/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.12]\n",
      "Epoch [53/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.11]\n",
      "Epoch [54/180]: 100%|██████████| 18/18 [00:50<00:00,  3.00s/it, batch loss=2.12]\n",
      "Epoch [55/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.09]\n",
      "Epoch [56/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.12]\n",
      "Epoch [57/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.15]\n",
      "Epoch [58/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.16]\n",
      "Epoch [59/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.15]\n",
      "Epoch [60/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.11]\n",
      "Epoch [61/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.14]\n",
      "Epoch [62/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.14]\n",
      "Epoch [63/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.04]\n",
      "Epoch [64/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.1]\n",
      "Epoch [65/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.05]\n",
      "Epoch [66/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.08]\n",
      "Epoch [67/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.12]\n",
      "Epoch [68/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.18]\n",
      "Epoch [69/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.13]\n",
      "Epoch [70/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.09]\n",
      "Epoch [71/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.16]\n",
      "Epoch [72/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.11]\n",
      "Epoch [73/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.09]\n",
      "Epoch [74/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.22]\n",
      "Epoch [75/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.13]\n",
      "Epoch [76/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.13]\n",
      "Epoch [77/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.08]\n",
      "Epoch [78/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.13]\n",
      "Epoch [79/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.1]\n",
      "Epoch [80/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.09]\n",
      "Epoch [81/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.14]\n",
      "Epoch [82/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.14]\n",
      "Epoch [83/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.15]\n",
      "Epoch [84/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.06]\n",
      "Epoch [85/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.11]\n",
      "Epoch [86/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.1]\n",
      "Epoch [87/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.09]\n",
      "Epoch [88/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.15]\n",
      "Epoch [89/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.14]\n",
      "Epoch [90/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.16]\n",
      "Epoch [91/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.16]\n",
      "Epoch [92/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.15]\n",
      "Epoch [93/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.07]\n",
      "Epoch [94/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.1]\n",
      "Epoch [95/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.11]\n",
      "Epoch [96/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.11]\n",
      "Epoch [97/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.01]\n",
      "Epoch [98/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.08]\n",
      "Epoch [99/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.18]\n",
      "Epoch [100/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.1]\n",
      "Epoch [101/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.19]\n",
      "Epoch [102/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.05]\n",
      "Epoch [103/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.13]\n",
      "Epoch [104/180]: 100%|██████████| 18/18 [00:50<00:00,  2.97s/it, batch loss=2.15]\n",
      "Epoch [105/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.16]\n",
      "Epoch [106/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.14]\n",
      "Epoch [107/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.1]\n",
      "Epoch [108/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.15]\n",
      "Epoch [109/180]: 100%|██████████| 18/18 [00:50<00:00,  2.97s/it, batch loss=2.1]\n",
      "Epoch [110/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.05]\n",
      "Epoch [111/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.1]\n",
      "Epoch [112/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.09]\n",
      "Epoch [113/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.14]\n",
      "Epoch [114/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.05]\n",
      "Epoch [115/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.15]\n",
      "Epoch [116/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.07]\n",
      "Epoch [117/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.09]\n",
      "Epoch [118/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.04]\n",
      "Epoch [119/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.1]\n",
      "Epoch [120/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.04]\n",
      "Epoch [121/180]: 100%|██████████| 18/18 [00:50<00:00,  2.97s/it, batch loss=2.09]\n",
      "Epoch [122/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.12]\n",
      "Epoch [123/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.11]\n",
      "Epoch [124/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.08]\n",
      "Epoch [125/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.01]\n",
      "Epoch [126/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.1]\n",
      "Epoch [127/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.12]\n",
      "Epoch [128/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.08]\n",
      "Epoch [129/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.18]\n",
      "Epoch [130/180]: 100%|██████████| 18/18 [00:50<00:00,  3.00s/it, batch loss=2.09]\n",
      "Epoch [131/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=1.99]\n",
      "Epoch [132/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.13]\n",
      "Epoch [133/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.13]\n",
      "Epoch [134/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.13]\n",
      "Epoch [135/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.09]\n",
      "Epoch [136/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.07]\n",
      "Epoch [137/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.16]\n",
      "Epoch [138/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.09]\n",
      "Epoch [139/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.15]\n",
      "Epoch [140/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.09]\n",
      "Epoch [141/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.18]\n",
      "Epoch [142/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.12]\n",
      "Epoch [143/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.15]\n",
      "Epoch [144/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.19]\n",
      "Epoch [145/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.11]\n",
      "Epoch [146/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.02]\n",
      "Epoch [147/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.08]\n",
      "Epoch [148/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.08]\n",
      "Epoch [149/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.14]\n",
      "Epoch [150/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.11]\n",
      "Epoch [151/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.04]\n",
      "Epoch [152/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.13]\n",
      "Epoch [153/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.11]\n",
      "Epoch [154/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.17]\n",
      "Epoch [155/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.13]\n",
      "Epoch [156/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.09]\n",
      "Epoch [157/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.15]\n",
      "Epoch [158/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.08]\n",
      "Epoch [159/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.08]\n",
      "Epoch [160/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.16]\n",
      "Epoch [161/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.1]\n",
      "Epoch [162/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.11]\n",
      "Epoch [163/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.13]\n",
      "Epoch [164/180]: 100%|██████████| 18/18 [00:50<00:00,  3.00s/it, batch loss=2.11]\n",
      "Epoch [165/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.03]\n",
      "Epoch [166/180]: 100%|██████████| 18/18 [00:50<00:00,  2.97s/it, batch loss=2.09]\n",
      "Epoch [167/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.13]\n",
      "Epoch [168/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.05]\n",
      "Epoch [169/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.01]\n",
      "Epoch [170/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.12]\n",
      "Epoch [171/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.13]\n",
      "Epoch [172/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.07]\n",
      "Epoch [173/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.1]\n",
      "Epoch [174/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.16]\n",
      "Epoch [175/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.13]\n",
      "Epoch [176/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=1.99]\n",
      "Epoch [177/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.05]\n",
      "Epoch [178/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.06]\n",
      "Epoch [179/180]: 100%|██████████| 18/18 [00:50<00:00,  2.98s/it, batch loss=2.13]\n",
      "Epoch [180/180]: 100%|██████████| 18/18 [00:50<00:00,  2.99s/it, batch loss=2.02]\n"
     ]
    }
   ],
   "source": [
    "# from TrainingIgnite import training\n",
    "#config = initConfig(modelNamesDict['MustafaNet'])\n",
    "config[\"max_epochs\"] = 180\n",
    "config[\"train_now\"] = True\n",
    "config['batch_size'] = 400\n",
    "config[\"lr\"] = 10e-1\n",
    "config[\"momentum\"] = 9e-1\n",
    "config[\"gamma\"] = 9.5e-1\n",
    "print(config[\"model\"], config['train_now'])\n",
    "training(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CreateTrainer import create_trainer\n",
    "# trainer, train_evaluator, validation_evaluator, test_evaluator, evaluator, pbar \n",
    "# Setup model trainer and evaluator\n",
    "whatever = create_trainer(model, optimizer, criterion, lr_scheduler, config, data)\n",
    "len(whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model trainer and evaluator\n",
    "trainer, train_evaluator, validation_evaluator, test_evaluator, evaluator, pbar = create_trainer(\n",
    "    model, optimizer, criterion, lr_scheduler, config, data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, model, optimizer, criterion, lr_scheduler, train_loader = training(\n",
    "    config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "to_load = {'trainer': trainer, 'model': model,\n",
    "           'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "checkpoint = torch.load(config['trainer_save_path']+'checkpoint_90.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.handlers import Checkpoint\n",
    "Checkpoint.load_objects(to_load=to_load, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(train_loader,max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\"0\": {\"precision\": 0.0, \"recall\": 0.0, \"f1-score\": 0.0}, \"1\": {\"precision\": 0.5813953488372093, \"recall\": 0.5434782608695652, \"f1-score\": 0.5617977528089882}, \"2\": {\"precision\": 0.0, \"recall\": 0.0, \"f1-score\": 0.0}, \"3\": {\"precision\": 0.2714285714285714, \"recall\": 0.3392857142857143, \"f1-score\": 0.30158730158730107}, \"4\": {\"precision\": 0.4794520547945205, \"recall\": 0.42168674698795183, \"f1-score\": 0.44871794871794823}, \"5\": {\"precision\": 0.208955223880597, \"recall\": 0.2222222222222222, \"f1-score\": 0.21538461538461487}, \"6\": {\"precision\": 0.75, \"recall\": 0.16666666666666666, \"f1-score\": 0.27272727272727243}, \"7\": {\"precision\": 0.5263157894736842, \"recall\": 0.2857142857142857, \"f1-score\": 0.3703703703703699}, \"8\": {\"precision\": 0.3111111111111111, \"recall\": 0.3218390804597701, \"f1-score\": 0.3163841807909599}, \"9\": {\"precision\": 0.2975206611570248,\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \"recall\": 0.576, \"f1-score\": 0.392370572207084}, \"10\": {\"precision\": 0.47297297297297297, \"recall\": 0.5511811023622047, \"f1-score\": 0.5090909090909086}, \"11\": {\"precision\": 0.42105263157894735, \"recall\": 0.4, \"f1-score\": 0.41025641025640974}, \"12\": {\"precision\": 0.4117647058823529, \"recall\": 0.42857142857142855, \"f1-score\": 0.4199999999999995}, \"13\": {\"precision\": 0.4308510638297872, \"recall\": 0.574468085106383, \"f1-score\": 0.49240121580547064}, \"14\": {\"precision\": 0.6923076923076923, \"recall\": 0.28378378378378377, \"f1-score\": 0.40255591054313056}, \"15\": {\"precision\": 0.0, \"recall\": 0.0, \"f1-score\": 0.0}, \"16\": {\"precision\": 0.0, \"recall\": 0.0, \"f1-score\": 0.0}, \"17\": {\"precision\": 0.0, \"recall\": 0.0, \"f1-score\": 0.0}, \"macro avg\": {\"precision\": 0.3252848792919151, \"recall\": 0.28416096539055424, \"f1-score\": 0.2840913589050254}}\n",
    "print(type(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_trainer(model, optimizer, criterion, lr_scheduler, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "import torch\n",
    "from MileStone_3_Create_Neural_Network.WillyAlexNet import MyAlexNet\n",
    "#willyAlexNet = MyAlexNet()\n",
    "model = MyAlexNet()\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)\n",
    "print(model)\n",
    "args = {\"lr\": 0.01, \"momentum\":0.5, \"epochs\": 5, \"log_interval\": 3 }\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args[\"lr\"], momentum = args[\"momentum\"])\n",
    "# optimizer = optim.AdamW(model.parameters(), weight_decay=1e-6)\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single operation step when training\n",
    "def train_one_step(engine, batch):\n",
    "    \"\"\"\n",
    "    Single operation step when training\n",
    "\n",
    "    Args:\n",
    "        engine: ignite.engine.Engine\n",
    "        batch: tuple contains the training sample and their labels\n",
    "    \"\"\"\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "    data, target = batch\n",
    "    output = model(data.to(device))\n",
    "    loss = criterion(output, target.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_on_batch(engine, batch):\n",
    "    \"\"\"\n",
    "    Evaluation of the model on a single batch\n",
    "\n",
    "    Args:\n",
    "        engine: ignite.engine.Engine\n",
    "        batch: tuple contains the training sample and their labels\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data, target = batch\n",
    "        y_pred = model(data.to(device))\n",
    "        \n",
    "        return y_pred, target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Engine(train_one_step)\n",
    "train_evaluator = Engine(eval_model_on_batch)\n",
    "validation_evaluator = Engine(eval_model_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningAverage(output_transform=lambda x: x).attach(trainer,'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy(output_transform=lambda x: x).attach(train_evaluator,'accuracy')\n",
    "Loss(criterion).attach(train_evaluator, 'NLLLoss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy(output_transform=lambda x: x).attach(validation_evaluator,'accuracy')\n",
    "Loss(criterion).attach(validation_evaluator, 'NLLLoss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar(persist=True, bar_format=\"\")\n",
    "pbar.attach(trainer,['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(engine):\n",
    "    val_loss = engine.state.metrics['NLLLoss']\n",
    "    return -val_loss\n",
    "\n",
    "handler_earlyStopping = EarlyStopping(patience=5, score_function=score_function, trainer=trainer)\n",
    "validation_evaluator.add_event_handler(Events.COMPLETED,handler_earlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    train_evaluator.run(train_loader)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_nll_loos = metrics['NLLLoss']\n",
    "    pbar.log_message(\n",
    "        \"Training Results - Epoch: {} Avg accuracy: {:.2f} avg loss {:.2f}\".format(engine.state.epoch, avg_accuracy,\n",
    "       avg_nll_loos)\n",
    "       )\n",
    "\n",
    "def log_validation_results(engine):\n",
    "    validation_evaluator.run(valid_loader)\n",
    "    metrics = validation_evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_nll_loos = metrics['NLLLoss'] \n",
    "    pbar.log_message\n",
    "    (\n",
    "        \"Training Results - Epoch: {} Avg accuracy: {:.2f} avg loss {:.2f}\".format(engine.state.epoch, avg_accuracy, avg_nll_loos)\n",
    "    )\n",
    "    pbar.n = pbar.last_print_n = 0\n",
    "\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint('./tmp/models', 'willyAlexNet', n_saved=2, create_dir=True, save_as_state_dict=True)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'willyAlexNet': model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(train_loader, max_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def create_trainer(model, optimizer, criterion, lr_scheduler, config):\n",
    "    device = config['device']\n",
    "    # Define any training logic for iteration update\n",
    "\n",
    "    def train_step(engine, batch):\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        model.train()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    # Define trainer engine\n",
    "    trainer = Engine(train_step)\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED(every=3))\n",
    "    def save_checkpoint():\n",
    "        fp = Path(config.get(\"output_path\", \"output\")) / \"checkpoint.pt\"\n",
    "        torch.save(model.state_dict(), fp)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        train_evaluator.run(train_loader)\n",
    "        metrics = train_evaluator.state.metrics\n",
    "        avg_accuracy = metrics['accuracy']\n",
    "        avg_nll_loos = metrics['NLLLoss']\n",
    "        pbar.log_message(\n",
    "            \"Training Results - Epoch: {} Avg accuracy: {:.2f} avg loss {:.2f}\".format(engine.state.epoch, avg_accuracy,\n",
    "                                                                                       avg_nll_loos)\n",
    "        )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(engine):\n",
    "        validation_evaluator.run(valid_loader)\n",
    "        metrics = validation_evaluator.state.metrics\n",
    "        avg_accuracy = metrics['accuracy']\n",
    "        avg_nll_loos = metrics['NLLLoss']\n",
    "        pbar.log_message\n",
    "        (\n",
    "            \"Training Results - Epoch: {} Avg accuracy: {:.2f} avg loss {:.2f}\".format(\n",
    "                engine.state.epoch, avg_accuracy, avg_nll_loos)\n",
    "        )\n",
    "        pbar.n = pbar.last_print_n = 0\n",
    "\n",
    "    # Add progress bar showing batch loss value\n",
    "    ProgressBar().attach(trainer, output_transform=lambda x: {\"batch loss\": x})\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        config.get('checkpointPath', './tmp/models'), config.get('model',\n",
    "                                                                 'Unknown_you_lazy_bastart'),\n",
    "        n_saved=config.get('max_saved_checkout', 2), create_dir=True,\n",
    "        save_as_state_dict=True\n",
    "    )\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED,\n",
    "                              checkpointer, {config.get('model'): model}\n",
    "                              )\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MileStone_3_Create_Neural_Network.WillyAlexNet import MyAlexNet\n",
    "import torch\n",
    "\n",
    "\n",
    "def initalizeModel(config):\n",
    "    # ToDo return model from my private zoo\n",
    "    model = MyAlexNet()\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.get(\"lr\", 1e-2),\n",
    "        momentum=config.get(\"momentum\", 0.5),\n",
    "        weight_decay=config.get('weight_decay', 1e-6),\n",
    "        nesterov=True,\n",
    "    )\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    le = config['num_iter_per_epoch']\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer,\n",
    "                                                   step_size=le,\n",
    "                                                   gamma=config.get(\n",
    "                                                       'gamma', 0.9)\n",
    "                                                   )\n",
    "    return model, optimizer, criterion, lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "config = {\n",
    "    \"model\": \"MyAlexNet\",\n",
    "    \"dataset\": \"Willys\",\n",
    "    'batch_size': 400,\n",
    "    \"num_iter_per_epoch\": len(train_loader),\n",
    "    'data_path': \"../Dataset_Willys_2020/ORGINAL/\",\n",
    "    'output_path': './tmp/output/test',\n",
    "    'device': torch.device('cuda:0')\n",
    "}\n",
    "model, optimizer, criterion, lr_scheduler = initalizeModel(config=config)\n",
    "torchinfo.summary(model);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_trainer(model, optimizer, criterion, lr_scheduler, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(config.get('data_path', \"dd\")) / \"checkpointer.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "import torch\n",
    "from MileStone_3_Create_Neural_Network.WillyAlexNet import MyAlexNet\n",
    "#willyAlexNet = MyAlexNet()\n",
    "model = MyAlexNet()\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)\n",
    "print(model)\n",
    "args = {\"lr\": 0.01, \"momentum\":0.5, \"epochs\": 5, \"log_interval\": 3 }\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args[\"lr\"], momentum = args[\"momentum\"])\n",
    "# optimizer = optim.AdamW(model.parameters(), weight_decay=1e-6)\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "import torch\n",
    "from MileStone_3_Create_Neural_Network.WillyAlexNet import MyAlexNet\n",
    "#willyAlexNet = MyAlexNet()\n",
    "model = MyAlexNet()\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def train(model, device, train_loaderIn, args):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args[\"lr\"], momentum = args[\"momentum\"])\n",
    "    # optimizer = optim.AdamW(model.parameters(), weight_decay=1e-6)\n",
    "    for epoch in range(1, args[\"epochs\"]+1):\n",
    "        train_epoch(epoch=epoch, args=args, model=model,\n",
    "         device=device, data_loader=train_loaderIn, optimizer=optimizer\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "def train_epoch(epoch:int, args:dict, model, device, data_loader, optimizer):\n",
    "    model.train(True)\n",
    "    pid = os.getpid()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        # output_pred = F.log_softmax(output,dim=1)\n",
    "        loss = F.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args[\"log_interval\"] == 0:\n",
    "            print(\"{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                pid, epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.item()\n",
    "                ) \n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loaderIn):\n",
    "    test_epoch(model=model, device=device,data_loader=test_loaderIn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, device, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = model(data.to(device))\n",
    "            # output_pred = F.log_softmax(output,dim=1)\n",
    "            test_loss += F.nll_loss(output, target.to(device),reduction='sum').item()\n",
    "            pred = output.max(1)[1]\n",
    "            correct += pred.eq(target.to(device)).sum().item()\n",
    "\n",
    "    test_loss /=  len(data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the model\n",
    "from MileStone_3_Create_Neural_Network.WillyNet import WillyNet\n",
    "import torch\n",
    "assert torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "willy = WillyNet()\n",
    "from torchinfo import summary\n",
    "summary(willy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"lr\": 0.01, \"momentum\":0.5, \"epochs\": 5, \"log_interval\": 3 }\n",
    "device = torch.device('cuda:0')\n",
    "willy.to(device)\n",
    "willy.share_memory()\n",
    "train(model=willy, device=device, train_loader=train_loader,args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model=willy, device=device, test_loader=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(rank, args, model, device, dataset, dataloader_kwargs):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr,\n",
    "                          momentum=args.momentum)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_epoch(epoch, args, model, device, train_loader, optimizer)\n",
    "\n",
    "\n",
    "def test(args, model, device, dataset, dataloader_kwargs):\n",
    "    torch.manual_seed(args.seed)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)\n",
    "    test_epoch(model, device, test_loader)\n",
    "\n",
    "\n",
    "def train_epoch(epoch, args, model, device, data_loader, optimizer):\n",
    "    model.train()\n",
    "    pid = os.getpid()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        loss = F.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                pid, epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test_epoch(model, device, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = model(data.to(device))\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target.to(device),\n",
    "                                    reduction='sum').item()\n",
    "            pred = output.max(1)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device)).sum().item()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bf86de19e6908e90672858f1bf900381ae778a88ca47b5d35f6eb2214d9bffa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('umu-pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
