{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from MileStone_1_Training_Validation_data_Setup.load_data import load_data_from_ImageFolder\n",
    "\n",
    "SOURCE_DIR = \"./Dataset_Willys_2020/ORGINAL/\"\n",
    "dataset = load_data_from_ImageFolder(root=SOURCE_DIR)\n",
    "#criterion = nn.SmoothL1Loss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MileStone 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "valid_split = 0.15\n",
    "test_split = 0.15\n",
    "batch_size = 256\n",
    "n_jobs = 24\n",
    "n_epochs = 5\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(valid_split*num_train)\n",
    "\n",
    "\n",
    "valid_idx = np.random.choice(indices,size=split,replace=False)\n",
    "train_idx = list(set(indices)-set(valid_idx))\n",
    "#test_idx = \n",
    "\n",
    "train_sampler= SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size,\n",
    "num_workers=n_jobs,pin_memory=True, drop_last= True)\n",
    "valid_loader = DataLoader(dataset,sampler=valid_sampler, batch_size=batch_size\n",
    ", num_workers=n_jobs,pin_memory = True, drop_last= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp, label_temp = next(iter(train_loader))\n",
    "label_temp.shape, train_temp.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_temp.type(torch.long)\n",
    "type(label_temp), label_temp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = dataset.classes\n",
    "#len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    #mean = np.array([0.485, 0.456, 0.406])\n",
    "    #std = np.array([0.229, 0.224, 0.225])\n",
    "    #inp = std * inp + mean\n",
    "    #inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "#inputs, classes = sample\n",
    "#out = make_grid(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imshow(out, title=[labels[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MileStone 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.cuda.is_available()\n",
    "from torchinfo import summary\n",
    "from torchvision import models\n",
    "device = torch.device('cuda:0')\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "pretrained = models.mnasnet0_5(pretrained=True)\n",
    "#summary(model,input_size=((5,3,198,198)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi, For this, there are different approaches but personally, I would create a class on top of previously\n",
    "# defined model which you have weights for. Then add any other layers as another sequential to the new defined model.\n",
    "# Something like this:\n",
    "#pretrained = torchvision.models.alexnet(pretrained=True)\n",
    "#https://discuss.pytorch.org/t/load-only-a-part-of-the-network-with-pretrained-weights/88397\n",
    "\n",
    "import torch\n",
    "assert torch.cuda.is_available()\n",
    "from torchinfo import summary\n",
    "from torchvision import models\n",
    "device = torch.device('cuda:0')\n",
    "torch.backends.cudnn.benchmark=True\n",
    "from torch import nn\n",
    "pretrained = models.mnasnet0_5(pretrained=True)\n",
    "\n",
    "class MyAlexNet(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(MyAlexNet, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "        self.my_new_layers = nn.Linear(1000,out_features=18,bias=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.pretrained(x)\n",
    "        x = self.my_new_layers(x)\n",
    "        return x\n",
    "\n",
    " \n",
    "mm = MyAlexNet(pretrained)\n",
    "summary(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MileStone_3_Create_Neural_Network.WillyNet import WillyNet\n",
    "import torch\n",
    "assert torch.cuda.is_available()\n",
    "from torchinfo import summary\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "torch.backends.cudnn.benchmark=True\n",
    "from torch import nn\n",
    "\n",
    "model = WillyNet()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MyAlexNet(pretrained)\n",
    "#num_out = model.classifier[1].out_features\n",
    "summary(model,input_size=((5,3,198,198)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda(device=device)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4,weight_decay=1e-6)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),weight_decay=1e-6)\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = F.nll_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    classes = predictions#torch.argmax(predictions, dim=1)\n",
    "    #labell = torch.argmax(labels,1)\n",
    "    #print(classes.shape, labels.shape)\n",
    "    return torch.mean((classes == labels).float())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MileStone 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_score(logit, label, threshold=0.5, beta=2):\n",
    "    prob = torch.sigmoid(logit)\n",
    "    prob = prob > threshold\n",
    "    label = label > threshold\n",
    "    TP = (prob & label).sum().float()\n",
    "    TN = ((~prob) & (~label)).sum().float()\n",
    "    FP = (prob & (~label)).sum().float()\n",
    "    FN = ((~prob) & label).sum().float()\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    precision = torch.mean(TP / (TP + FP + 1e-12))\n",
    "    recall = torch.mean(TP / (TP + FN + 1e-12))\n",
    "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
    "    return accuracy, precision, recall, F2.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch import nn\n",
    "train_losses, valid_losses = [], []\n",
    "n_batches = len(train_loader)\n",
    "running_acc = 0\n",
    "running_val_acc = 0\n",
    "acc, prec, recall,F2 = [],[],[],[]\n",
    "val_acc, val_prec, val_recall,val_F2 = [],[],[],[]\n",
    "for t in range(1, n_epochs + 1):\n",
    "    # training\n",
    "    t_losses = []\n",
    "    model.train(True)\n",
    "    for i, (train_batch, train_label) in enumerate(train_loader):\n",
    "        # label need to be torch.long for nn.CrossEntropyLoss()\n",
    "        #train_label = train_label.type(torch.long)\n",
    "        # train_label = torch.nn.functional.one_hot(train_label,18).type(torch.FloatTensor).to(device)\n",
    "        train_batch, train_label = train_batch.to(device), train_label.type(torch.long).to(device)\n",
    "        # print(train_batch.shape) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_outputs = model(train_batch)\n",
    "\n",
    "        # for nn.CrossEntropyLoss() preds should be feeded as is after F.log_softmax()\n",
    "        preds = F.log_softmax(train_outputs,dim=1)\n",
    "        #_, preds = torch.max(train_outputs,1)\n",
    "        #outputs_ = torch.nn.functional.one_hot(torch.argmax(train_outputs, 1), 18).type(torch.FloatTensor).to(device)\n",
    "        #label = torch.nn.functional.one_hot(label,num_classes=len(labels))\n",
    "        #print(outputs_.shape, label.shape)\n",
    "        \n",
    "        # for nn.CrossEntropyLoss() you just add the output from F.log_softmax()\n",
    "        loss = F.nll_loss(preds, train_label)#criterion(preds, train_label)\n",
    "        #loss = criterion(train_outputs, train_label)\n",
    "        \n",
    "        \n",
    "        #running_acc += accuracy(outputs_,train_label)\n",
    "        #acc_train, precision_train, recall_train, F2_train = F_score(outputs_,train_label)\n",
    "        #acc.append(float(acc_train)); prec.append(float(precision_train)); recall.append(float(recall_train)); F2.append(float(F2_train))\n",
    "\n",
    "        t_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(t_losses)\n",
    "    #running_acc /=len(train_loader)\n",
    "\n",
    "    # validation\n",
    "    v_losses = []\n",
    "    model.train(False)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for val_batch, val_label in valid_loader:\n",
    "            # for nn.CrossEntropyLoss()\n",
    "            val_label = val_label.type(torch.long)\n",
    "            # val_label = torch.nn.functional.one_hot(val_label,18).type(torch.FloatTensor)\n",
    "            val_batch, val_label = val_batch.to(device), val_label.to(device)            \n",
    "            \n",
    "            # also for nn.CrossEntropyLoss()\n",
    "            val_outputs = model(val_batch)\n",
    "            preds = F.log_softmax(val_outputs,dim=1)\n",
    "            # _, val_preds = torch.max(val_outputs,1)            \n",
    "            # val_outputs_ = torch.nn.functional.one_hot(torch.argmax(val_outputs, 1), 18)\n",
    "            val_loss = F.nll_loss(preds,val_label)#criterion(preds, val_label) \n",
    "            # val_loss = criterion(val_outputs, val_label)\n",
    "            \n",
    "            \n",
    "            v_losses.append(val_loss.item())\n",
    "            #running_val_acc +=accuracy(val_outputs_,val_label)\n",
    "            #acc_, precision_, recall_, F2_ = F_score(val_outputs_,val_label)\n",
    "            #val_acc.append(float(acc_)); val_prec.append(float(precision_)); val_recall.append(float(recall_)); val_F2.append(float(F2_))\n",
    "        valid_losses.append(v_losses)\n",
    "        #running_val_acc/=len(valid_loader)\n",
    "\n",
    "    if not np.all(np.isfinite(t_losses)):\n",
    "        raise RuntimeError(\n",
    "            'NaN or Inf in training loss, cannot recover. Exiting.')\n",
    "    log = f'Epoch: {t} - Training Loss: {np.mean(t_losses):.2e}, Validation Loss: {np.mean(v_losses):.2e}'\n",
    "    print(log)\n",
    "    #log_acc = f'train_acc: {running_acc:.6f} - val_acc:   {running_val_acc:.6f}'\n",
    "    #print(log_acc)\n",
    "#    log_acc = f'F2_acc:    {float(acc[-1]):.6f} - F2_val: {float(val_acc[-1]):.6f}'\n",
    " #   print(log_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(recall)/len(train_loader),sum(F2)/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch: 1 - Training Loss: 1.26e-01, Validation Loss: 4.74e-02\n",
    "train_acc: 0.900206 - val_acc: 0.905265\n",
    "Epoch: 2 - Training Loss: 4.65e-02, Validation Loss: 4.20e-02\n",
    "train_acc: 0.937633 - val_acc: 1.066907\n",
    "Epoch: 3 - Training Loss: 3.40e-02, Validation Loss: 4.25e-02\n",
    "train_acc: 0.944765 - val_acc: 1.092722\n",
    "Epoch: 4 - Training Loss: 2.99e-02, Validation Loss: 4.10e-02\n",
    "train_acc: 0.948942 - val_acc: 1.100060\n",
    "Epoch: 5 - Training Loss: 2.80e-02, Validation Loss: 4.06e-02\n",
    "train_acc: 0.950584 - val_acc: 1.102230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"./MyModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MyAlexNet(pretrained)\n",
    "model.load_state_dict(torch.load(\"./MyModel\"))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bf86de19e6908e90672858f1bf900381ae778a88ca47b5d35f6eb2214d9bffa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('umu-pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
